{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "# import ipdb\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('residual_net_n', 7, '')\n",
    "tf.app.flags.DEFINE_string('train_tf_path', 'E:/dynamicquantization/data/train.tf', '')\n",
    "tf.app.flags.DEFINE_string('val_tf_path', 'E:/dynamicquantization/data/test.tf', '')\n",
    "tf.app.flags.DEFINE_integer('train_batch_size', 128, '')\n",
    "tf.app.flags.DEFINE_integer('val_batch_size', 100, '')\n",
    "tf.app.flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay')\n",
    "tf.app.flags.DEFINE_integer('summary_interval', 100, 'Interval for summary.')\n",
    "tf.app.flags.DEFINE_integer('val_interval', 1000, 'Interval for evaluation.')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000, 'Maximum number of iterations.')\n",
    "tf.app.flags.DEFINE_integer('save_interval', 5000, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  with tf.device('/cpu:0'):\n",
    "    params = tf.constant(embedding_params)\n",
    "    embedding = tf.gather(params, label)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_in, n_out, k, s, p='SAME', bias=False, scope='conv'):\n",
    "  with tf.variable_scope(scope):\n",
    "    kernel = tf.Variable(\n",
    "      tf.truncated_normal([k, k, n_in, n_out],\n",
    "        stddev=math.sqrt(2/(k*k*n_in))),\n",
    "      name='weight')\n",
    "    tf.add_to_collection('weights', kernel)\n",
    "    conv = tf.nn.conv2d(x, kernel, [1,s,s,1], padding=p)\n",
    "    if bias:\n",
    "      bias = tf.get_variable('bias', [n_out], initializer=tf.constant_initializer(0.0))\n",
    "      tf.add_to_collection('biases', bias)\n",
    "      conv = tf.nn.bias_add(conv, bias)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn', affine=True):\n",
    "  \"\"\"\n",
    "  Batch normalization on convolutional maps.\n",
    "  Args:\n",
    "    x: Tensor, 4D BHWD input maps\n",
    "    n_out: integer, depth of input maps\n",
    "    phase_train: boolean tf.Variable, true indicates training phase\n",
    "    scope: string, variable scope\n",
    "    affine: whether to affine-transform outputs\n",
    "  Return:\n",
    "    normed: batch-normalized maps\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "      name='beta', trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "      name='gamma', trainable=affine)\n",
    "    tf.add_to_collection('biases', beta)\n",
    "    tf.add_to_collection('weights', gamma)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "      ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "      with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train,\n",
    "      mean_var_with_update,\n",
    "      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \n",
    "      beta, gamma, 1e-3, affine)\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x, n_in, n_out, subsample, phase_train, scope='res_block'):\n",
    "  with tf.variable_scope(scope):\n",
    "    if subsample:\n",
    "      y = conv2d(x, n_in, n_out, 3, 2, 'SAME', False, scope='conv_1')\n",
    "      shortcut = conv2d(x, n_in, n_out, 3, 2, 'SAME',\n",
    "                False, scope='shortcut')\n",
    "    else:\n",
    "      y = conv2d(x, n_in, n_out, 3, 1, 'SAME', False, scope='conv_1')\n",
    "      shortcut = tf.identity(x, name='shortcut')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_1')\n",
    "    y = tf.nn.relu(y, name='relu_1')\n",
    "    y = conv2d(y, n_out, n_out, 3, 1, 'SAME', True, scope='conv_2')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_2')\n",
    "    y = y + shortcut\n",
    "    y = tf.nn.relu(y, name='relu_2')\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_group(x, n_in, n_out, n, first_subsample, phase_train, scope='res_group'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = residual_block(x, n_in, n_out, first_subsample, phase_train, scope='block_1')\n",
    "    for i in range(n - 1):\n",
    "      y = residual_block(y, n_out, n_out, False, phase_train, scope='block_%d' % (i + 2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_net(x, n, n_classes, phase_train, scope='res_net'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = conv2d(x, 3, 16, 3, 1, 'SAME', False, scope='conv_init')\n",
    "    y = batch_norm(y, 16, phase_train, scope='bn_init')\n",
    "    y = tf.nn.relu(y, name='relu_init')\n",
    "    y = residual_group(y, 16, 16, n, False, phase_train, scope='group_1')\n",
    "    y = residual_group(y, 16, 32, n, True, phase_train, scope='group_2')\n",
    "    y = residual_group(y, 32, 64, n, True, phase_train, scope='group_3')\n",
    "#     y = conv2d(y, 64, n_classes, 1, 1, 'SAME', True, scope='conv_last')\n",
    "    y = tf.nn.avg_pool(y, [1, 8, 8, 1], [1, 1, 1, 1], 'VALID', name='avg_pool')\n",
    "    y = tf.reshape(y, [-1, 64])\n",
    "    w = tf.get_variable(name='weight_fc', shape=[64, n_classes], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('weights', w)\n",
    "    b = tf.get_variable(name='weight_biase', shape=[n_classes], initializer=tf.constant_initializer(0))\n",
    "    tf.add_to_collection('last_biases', b)\n",
    "    y = tf.matmul(y, w) + b\n",
    "#     y = tf.squeeze(y, squeeze_dims=[1, 2])\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, labels, scope='loss'):\n",
    "  with tf.variable_scope(scope):\n",
    "    # entropy loss\n",
    "    targets = one_hot_embedding(labels, 10)\n",
    "    entropy_loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets),\n",
    "      name='entropy_loss')\n",
    "    tf.add_to_collection('losses', entropy_loss)\n",
    "    # weight l2 decay loss\n",
    "    weight_l2_losses = [tf.nn.l2_loss(o) for o in tf.get_collection('weights')]\n",
    "    weight_decay_loss = FLAGS.weight_decay*tf.add_n(weight_l2_losses)\n",
    "    tf.add_to_collection('losses', weight_decay_loss)\n",
    "  # for var in tf.get_collection('losses'):\n",
    "    # tf.scalar_summary('losses/' + var.op.name, var)\n",
    "  # total loss\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _accuracy(logits, gt_label, scope='accuracy'):\n",
    "  with tf.variable_scope(scope):\n",
    "    pred_label = tf.argmax(logits, 1)\n",
    "    acc = 1.0 - tf.nn.zero_fraction(\n",
    "      tf.cast(tf.equal(pred_label, gt_label), tf.int32))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_op(loss, global_step, learning_rate):\n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params, name='gradients')\n",
    "  optim = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "  update = optim.apply_gradients(zip(gradients, params))\n",
    "  with tf.control_dependencies([update]):\n",
    "    train_op = tf.no_op(name='train_op')\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "  # meanstd = joblib.load(FLAGS.mean_std_path)\n",
    "  # mean, std = meanstd['mean'], meanstd['std']\n",
    "  mean = [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(\n",
    "    image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_batch(train_records_path, batch_size):\n",
    "  with tf.variable_scope('train_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "      train_image = normalize_image(train_image)\n",
    "      train_image = random_distort_image(train_image)\n",
    "      train_image_batch, train_label_batch = tf.train.shuffle_batch(\n",
    "        [train_image, train_label], batch_size=batch_size, num_threads=4,\n",
    "        capacity=50000,\n",
    "        min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  with tf.variable_scope('evaluate_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "      test_image = normalize_image(test_image)\n",
    "      test_image_batch, test_label_batch = tf.train.batch(\n",
    "        [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "        capacity=10000)\n",
    "  return test_image_batch, test_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch = make_train_batch(FLAGS.train_tf_path, FLAGS.train_batch_size)\n",
    "val_image_batch, val_label_batch = make_validation_batch(FLAGS.val_tf_path, FLAGS.val_batch_size)\n",
    "\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "\n",
    "\n",
    "logits = residual_net(image_batch, FLAGS.residual_net_n, 10, phase_train)\n",
    "\n",
    "\n",
    "loss = _loss(logits, label_batch)\n",
    "accuracy = _accuracy(logits, label_batch)\n",
    "\n",
    "# train one step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "INFO:tensorflow:Restoring parameters from E:/dynamicquantization/full_precision/res44/model/res.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-6, started daemon 11284)>,\n",
       " <Thread(Thread-7, started daemon 11448)>,\n",
       " <Thread(Thread-8, started daemon 3408)>,\n",
       " <Thread(Thread-9, started daemon 6764)>,\n",
       " <Thread(Thread-10, started daemon 6556)>,\n",
       " <Thread(Thread-11, started daemon 6288)>,\n",
       " <Thread(Thread-12, started daemon 14812)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print('Initializing...')\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'E:/dynamicquantization/full_precision/res44/model/res.ckpt')\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test accuracy = 0.933200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# validation\n",
    "\n",
    "print('Evaluating...')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "  fetches = [logits, label_batch, loss]\n",
    "  session_outputs = sess.run(\n",
    "    fetches, {phase_train.name: False})\n",
    "  val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "  val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "  val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(\n",
    "  pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_prune_on_grads(grads_and_vars, dict_nzidx):\n",
    "    for key, nzidx in dict_nzidx.items():\n",
    "        count = 0\n",
    "        for grad, var in grads_and_vars:\n",
    "            if var.name == key:\n",
    "                nzidx_obj = tf.cast(tf.constant(sess.run(dict_nzidx[key])), tf.float32)\n",
    "                grads_and_vars[count] = (tf.multiply(nzidx_obj, grad), var)\n",
    "            count += 1\n",
    "    return grads_and_vars\n",
    "\n",
    "def apply_inq(weights, inq_dict, var_name, prune_rate):  \n",
    "    for target in var_name:\n",
    "        wl = target\n",
    "        bit = 32\n",
    "\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = sess.run(weight_obj)\n",
    "\n",
    "        weight_rest = np.reshape(weight_arr, [-1])\n",
    "        dic_tem = np.reshape(sess.run(inq_dict[wl]), [-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest, [-1]))), 0)\n",
    "        num_prune = int(len(weight_rest) * prune_rate)\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]]\n",
    "\n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        n2 = n1 + 1 - bit / 4\n",
    "        upper_bound = 2 ** (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        lower_bound = 2 ** (n1 + 1 - bit / 4)\n",
    "\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2 ** (np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0] * 4 / 3)))) * np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr = np.reshape(weight_rest, np.shape(weight_arr))\n",
    "        dic_tem[idx_rest[:num_prune]] = np.zeros_like(dic_tem[idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = tf.cast(np.reshape(dic_tem, np.shape(sess.run(inq_dict[wl]))), tf.float32)\n",
    "        sess.run(weights[wl].assign(weight_arr))\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮  量化\n",
    "prune_rate =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.933200\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.08215426 -0.05700303  0.04312498  0.1090302  -0.0417071  -0.07600322\n",
      "  0.00174217 -0.00667335  0.03555666 -0.09022148]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.5)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 10:35:33.058179] Iteration 100, train loss = 0.335733, train accuracy = 0.945312\n",
      "[2018-06-02 10:35:52.283179] Iteration 200, train loss = 0.276167, train accuracy = 0.968750\n",
      "[2018-06-02 10:36:11.450179] Iteration 300, train loss = 0.283944, train accuracy = 0.937500\n",
      "[2018-06-02 10:36:30.678179] Iteration 400, train loss = 0.197090, train accuracy = 0.976562\n",
      "[2018-06-02 10:36:49.903179] Iteration 500, train loss = 0.207580, train accuracy = 0.976562\n",
      "[2018-06-02 10:37:09.025179] Iteration 600, train loss = 0.177967, train accuracy = 0.992188\n",
      "[2018-06-02 10:37:28.143179] Iteration 700, train loss = 0.197727, train accuracy = 0.976562\n",
      "[2018-06-02 10:37:47.307179] Iteration 800, train loss = 0.193050, train accuracy = 0.976562\n",
      "[2018-06-02 10:38:06.611179] Iteration 900, train loss = 0.161707, train accuracy = 1.000000\n",
      "[2018-06-02 10:38:25.730179] Iteration 1000, train loss = 0.187207, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.916100\n",
      "[2018-06-02 10:38:49.051179] Iteration 1100, train loss = 0.146281, train accuracy = 1.000000\n",
      "[2018-06-02 10:39:08.195179] Iteration 1200, train loss = 0.190225, train accuracy = 0.960938\n",
      "[2018-06-02 10:39:27.390179] Iteration 1300, train loss = 0.175376, train accuracy = 0.984375\n",
      "[2018-06-02 10:39:46.563179] Iteration 1400, train loss = 0.203720, train accuracy = 0.976562\n",
      "[2018-06-02 10:40:05.728179] Iteration 1500, train loss = 0.157823, train accuracy = 1.000000\n",
      "[2018-06-02 10:40:24.876179] Iteration 1600, train loss = 0.235943, train accuracy = 0.968750\n",
      "[2018-06-02 10:40:44.026179] Iteration 1700, train loss = 0.173325, train accuracy = 0.984375\n",
      "[2018-06-02 10:41:03.207179] Iteration 1800, train loss = 0.187360, train accuracy = 0.984375\n",
      "[2018-06-02 10:41:22.371179] Iteration 1900, train loss = 0.160787, train accuracy = 0.992188\n",
      "[2018-06-02 10:41:41.509179] Iteration 2000, train loss = 0.186761, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.919700\n",
      "[2018-06-02 10:42:04.786179] Iteration 2100, train loss = 0.180409, train accuracy = 0.984375\n",
      "[2018-06-02 10:42:23.896179] Iteration 2200, train loss = 0.180097, train accuracy = 0.976562\n",
      "[2018-06-02 10:42:43.069179] Iteration 2300, train loss = 0.163735, train accuracy = 0.992188\n",
      "[2018-06-02 10:43:02.241179] Iteration 2400, train loss = 0.159261, train accuracy = 0.992188\n",
      "[2018-06-02 10:43:21.437179] Iteration 2500, train loss = 0.156272, train accuracy = 0.992188\n",
      "[2018-06-02 10:43:40.613179] Iteration 2600, train loss = 0.161286, train accuracy = 0.992188\n",
      "[2018-06-02 10:43:59.790179] Iteration 2700, train loss = 0.157853, train accuracy = 0.984375\n",
      "[2018-06-02 10:44:18.943179] Iteration 2800, train loss = 0.162933, train accuracy = 0.992188\n",
      "[2018-06-02 10:44:38.171179] Iteration 2900, train loss = 0.188416, train accuracy = 0.968750\n",
      "[2018-06-02 10:44:57.395179] Iteration 3000, train loss = 0.159087, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-02 10:45:20.689179] Iteration 3100, train loss = 0.195180, train accuracy = 0.976562\n",
      "[2018-06-02 10:45:39.804179] Iteration 3200, train loss = 0.183262, train accuracy = 0.984375\n",
      "[2018-06-02 10:45:59.036179] Iteration 3300, train loss = 0.148764, train accuracy = 1.000000\n",
      "[2018-06-02 10:46:18.190179] Iteration 3400, train loss = 0.143695, train accuracy = 1.000000\n",
      "[2018-06-02 10:46:37.412179] Iteration 3500, train loss = 0.148030, train accuracy = 0.992188\n",
      "[2018-06-02 10:46:56.764179] Iteration 3600, train loss = 0.167293, train accuracy = 0.992188\n",
      "[2018-06-02 10:47:16.031179] Iteration 3700, train loss = 0.141570, train accuracy = 1.000000\n",
      "[2018-06-02 10:47:35.219179] Iteration 3800, train loss = 0.152085, train accuracy = 0.992188\n",
      "[2018-06-02 10:47:54.408179] Iteration 3900, train loss = 0.146388, train accuracy = 1.000000\n",
      "[2018-06-02 10:48:13.569179] Iteration 4000, train loss = 0.179859, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.923300\n",
      "[2018-06-02 10:48:36.892179] Iteration 4100, train loss = 0.171143, train accuracy = 0.992188\n",
      "[2018-06-02 10:48:56.101179] Iteration 4200, train loss = 0.155256, train accuracy = 0.992188\n",
      "[2018-06-02 10:49:15.289179] Iteration 4300, train loss = 0.166078, train accuracy = 0.992188\n",
      "[2018-06-02 10:49:34.438179] Iteration 4400, train loss = 0.151922, train accuracy = 1.000000\n",
      "[2018-06-02 10:49:53.604179] Iteration 4500, train loss = 0.163599, train accuracy = 0.992188\n",
      "[2018-06-02 10:50:12.783179] Iteration 4600, train loss = 0.220356, train accuracy = 0.984375\n",
      "[2018-06-02 10:50:31.939179] Iteration 4700, train loss = 0.159737, train accuracy = 0.992188\n",
      "[2018-06-02 10:50:51.176179] Iteration 4800, train loss = 0.143813, train accuracy = 1.000000\n",
      "[2018-06-02 10:51:10.418179] Iteration 4900, train loss = 0.185354, train accuracy = 0.984375\n",
      "[2018-06-02 10:51:29.573179] Iteration 5000, train loss = 0.147122, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-02 10:51:52.921179] Iteration 5100, train loss = 0.157937, train accuracy = 0.992188\n",
      "[2018-06-02 10:52:12.118179] Iteration 5200, train loss = 0.146881, train accuracy = 1.000000\n",
      "[2018-06-02 10:52:31.325179] Iteration 5300, train loss = 0.143210, train accuracy = 1.000000\n",
      "[2018-06-02 10:52:50.475179] Iteration 5400, train loss = 0.165610, train accuracy = 0.992188\n",
      "[2018-06-02 10:53:09.699179] Iteration 5500, train loss = 0.149949, train accuracy = 1.000000\n",
      "[2018-06-02 10:53:28.912179] Iteration 5600, train loss = 0.150196, train accuracy = 1.000000\n",
      "[2018-06-02 10:53:48.130179] Iteration 5700, train loss = 0.145553, train accuracy = 1.000000\n",
      "[2018-06-02 10:54:07.236179] Iteration 5800, train loss = 0.166698, train accuracy = 0.984375\n",
      "[2018-06-02 10:54:26.534179] Iteration 5900, train loss = 0.141586, train accuracy = 1.000000\n",
      "[2018-06-02 10:54:45.776179] Iteration 6000, train loss = 0.145653, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 10:55:09.118179] Iteration 6100, train loss = 0.178671, train accuracy = 0.992188\n",
      "[2018-06-02 10:55:28.275179] Iteration 6200, train loss = 0.148455, train accuracy = 0.992188\n",
      "[2018-06-02 10:55:47.448179] Iteration 6300, train loss = 0.166179, train accuracy = 0.984375\n",
      "[2018-06-02 10:56:06.627179] Iteration 6400, train loss = 0.147845, train accuracy = 0.992188\n",
      "[2018-06-02 10:56:25.769179] Iteration 6500, train loss = 0.159166, train accuracy = 0.992188\n",
      "[2018-06-02 10:56:44.926179] Iteration 6600, train loss = 0.145474, train accuracy = 1.000000\n",
      "[2018-06-02 10:57:04.127179] Iteration 6700, train loss = 0.159424, train accuracy = 0.992188\n",
      "[2018-06-02 10:57:23.391179] Iteration 6800, train loss = 0.156652, train accuracy = 0.992188\n",
      "[2018-06-02 10:57:42.696179] Iteration 6900, train loss = 0.155975, train accuracy = 0.984375\n",
      "[2018-06-02 10:58:01.945179] Iteration 7000, train loss = 0.169670, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.925500\n",
      "[2018-06-02 10:58:25.229179] Iteration 7100, train loss = 0.141830, train accuracy = 1.000000\n",
      "[2018-06-02 10:58:44.473179] Iteration 7200, train loss = 0.159543, train accuracy = 0.992188\n",
      "[2018-06-02 10:59:03.663179] Iteration 7300, train loss = 0.139819, train accuracy = 1.000000\n",
      "[2018-06-02 10:59:22.876179] Iteration 7400, train loss = 0.162120, train accuracy = 0.992188\n",
      "[2018-06-02 10:59:42.137179] Iteration 7500, train loss = 0.159623, train accuracy = 0.992188\n",
      "[2018-06-02 11:00:01.338179] Iteration 7600, train loss = 0.166778, train accuracy = 0.976562\n",
      "[2018-06-02 11:00:20.493179] Iteration 7700, train loss = 0.163317, train accuracy = 0.992188\n",
      "[2018-06-02 11:00:39.597179] Iteration 7800, train loss = 0.155162, train accuracy = 0.992188\n",
      "[2018-06-02 11:00:58.764179] Iteration 7900, train loss = 0.139278, train accuracy = 1.000000\n",
      "[2018-06-02 11:01:18.039179] Iteration 8000, train loss = 0.157287, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925200\n",
      "[2018-06-02 11:01:41.311179] Iteration 8100, train loss = 0.160570, train accuracy = 0.992188\n",
      "[2018-06-02 11:02:00.437179] Iteration 8200, train loss = 0.145166, train accuracy = 1.000000\n",
      "[2018-06-02 11:02:19.603179] Iteration 8300, train loss = 0.153034, train accuracy = 0.992188\n",
      "[2018-06-02 11:02:38.916179] Iteration 8400, train loss = 0.150605, train accuracy = 1.000000\n",
      "[2018-06-02 11:02:58.053179] Iteration 8500, train loss = 0.163909, train accuracy = 0.992188\n",
      "[2018-06-02 11:03:17.218179] Iteration 8600, train loss = 0.181956, train accuracy = 0.984375\n",
      "[2018-06-02 11:03:36.310179] Iteration 8700, train loss = 0.144978, train accuracy = 1.000000\n",
      "[2018-06-02 11:03:55.574179] Iteration 8800, train loss = 0.156478, train accuracy = 0.992188\n",
      "[2018-06-02 11:04:14.707179] Iteration 8900, train loss = 0.144088, train accuracy = 1.000000\n",
      "[2018-06-02 11:04:33.832179] Iteration 9000, train loss = 0.162430, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925900\n",
      "[2018-06-02 11:04:57.236179] Iteration 9100, train loss = 0.148357, train accuracy = 0.992188\n",
      "[2018-06-02 11:05:16.503179] Iteration 9200, train loss = 0.157358, train accuracy = 0.984375\n",
      "[2018-06-02 11:05:35.671179] Iteration 9300, train loss = 0.154483, train accuracy = 0.992188\n",
      "[2018-06-02 11:05:54.947179] Iteration 9400, train loss = 0.178473, train accuracy = 0.984375\n",
      "[2018-06-02 11:06:14.080179] Iteration 9500, train loss = 0.147684, train accuracy = 1.000000\n",
      "[2018-06-02 11:06:33.259179] Iteration 9600, train loss = 0.145193, train accuracy = 1.000000\n",
      "[2018-06-02 11:06:52.446179] Iteration 9700, train loss = 0.152696, train accuracy = 0.992188\n",
      "[2018-06-02 11:07:11.669179] Iteration 9800, train loss = 0.145812, train accuracy = 1.000000\n",
      "[2018-06-02 11:07:30.772179] Iteration 9900, train loss = 0.161489, train accuracy = 0.984375\n",
      "[2018-06-02 11:07:50.023179] Iteration 10000, train loss = 0.142535, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926700\n",
      "[2018-06-02 11:08:13.337179] Iteration 10100, train loss = 0.152696, train accuracy = 0.992188\n",
      "[2018-06-02 11:08:32.473179] Iteration 10200, train loss = 0.148044, train accuracy = 1.000000\n",
      "[2018-06-02 11:08:51.686179] Iteration 10300, train loss = 0.149488, train accuracy = 1.000000\n",
      "[2018-06-02 11:09:10.978179] Iteration 10400, train loss = 0.145815, train accuracy = 1.000000\n",
      "[2018-06-02 11:09:30.054179] Iteration 10500, train loss = 0.153934, train accuracy = 1.000000\n",
      "[2018-06-02 11:09:49.260179] Iteration 10600, train loss = 0.149560, train accuracy = 1.000000\n",
      "[2018-06-02 11:10:08.485179] Iteration 10700, train loss = 0.137717, train accuracy = 1.000000\n",
      "[2018-06-02 11:10:27.760179] Iteration 10800, train loss = 0.143354, train accuracy = 1.000000\n",
      "[2018-06-02 11:10:46.899179] Iteration 10900, train loss = 0.145323, train accuracy = 1.000000\n",
      "[2018-06-02 11:11:06.164179] Iteration 11000, train loss = 0.151416, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926200\n",
      "[2018-06-02 11:11:29.446179] Iteration 11100, train loss = 0.143462, train accuracy = 1.000000\n",
      "[2018-06-02 11:11:48.578179] Iteration 11200, train loss = 0.142954, train accuracy = 1.000000\n",
      "[2018-06-02 11:12:07.797179] Iteration 11300, train loss = 0.162642, train accuracy = 0.992188\n",
      "[2018-06-02 11:12:26.940179] Iteration 11400, train loss = 0.161071, train accuracy = 0.992188\n",
      "[2018-06-02 11:12:46.151179] Iteration 11500, train loss = 0.151829, train accuracy = 0.984375\n",
      "[2018-06-02 11:13:05.346179] Iteration 11600, train loss = 0.144919, train accuracy = 1.000000\n",
      "[2018-06-02 11:13:24.477179] Iteration 11700, train loss = 0.166929, train accuracy = 0.984375\n",
      "[2018-06-02 11:13:43.693179] Iteration 11800, train loss = 0.138632, train accuracy = 1.000000\n",
      "[2018-06-02 11:14:02.958179] Iteration 11900, train loss = 0.151994, train accuracy = 0.992188\n",
      "[2018-06-02 11:14:22.138179] Iteration 12000, train loss = 0.158473, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926900\n",
      "[2018-06-02 11:14:45.479179] Iteration 12100, train loss = 0.168440, train accuracy = 0.992188\n",
      "[2018-06-02 11:15:04.639179] Iteration 12200, train loss = 0.139611, train accuracy = 1.000000\n",
      "[2018-06-02 11:15:23.820179] Iteration 12300, train loss = 0.151128, train accuracy = 1.000000\n",
      "[2018-06-02 11:15:43.043179] Iteration 12400, train loss = 0.190805, train accuracy = 0.976562\n",
      "[2018-06-02 11:16:02.243179] Iteration 12500, train loss = 0.150915, train accuracy = 1.000000\n",
      "[2018-06-02 11:16:21.494179] Iteration 12600, train loss = 0.176997, train accuracy = 0.992188\n",
      "[2018-06-02 11:16:40.749179] Iteration 12700, train loss = 0.149449, train accuracy = 1.000000\n",
      "[2018-06-02 11:16:59.845179] Iteration 12800, train loss = 0.151514, train accuracy = 0.992188\n",
      "[2018-06-02 11:17:18.948179] Iteration 12900, train loss = 0.147848, train accuracy = 1.000000\n",
      "[2018-06-02 11:17:38.102179] Iteration 13000, train loss = 0.144783, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927000\n",
      "[2018-06-02 11:18:01.420179] Iteration 13100, train loss = 0.143600, train accuracy = 1.000000\n",
      "[2018-06-02 11:18:20.599179] Iteration 13200, train loss = 0.145930, train accuracy = 0.992188\n",
      "[2018-06-02 11:18:39.746179] Iteration 13300, train loss = 0.153214, train accuracy = 1.000000\n",
      "[2018-06-02 11:18:58.862179] Iteration 13400, train loss = 0.147992, train accuracy = 1.000000\n",
      "[2018-06-02 11:19:18.018179] Iteration 13500, train loss = 0.160204, train accuracy = 0.992188\n",
      "[2018-06-02 11:19:37.183179] Iteration 13600, train loss = 0.149762, train accuracy = 1.000000\n",
      "[2018-06-02 11:19:56.307179] Iteration 13700, train loss = 0.140453, train accuracy = 1.000000\n",
      "[2018-06-02 11:20:15.482179] Iteration 13800, train loss = 0.159978, train accuracy = 0.992188\n",
      "[2018-06-02 11:20:34.650179] Iteration 13900, train loss = 0.142619, train accuracy = 1.000000\n",
      "[2018-06-02 11:20:53.933179] Iteration 14000, train loss = 0.151280, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926700\n",
      "[2018-06-02 11:21:17.301179] Iteration 14100, train loss = 0.137232, train accuracy = 1.000000\n",
      "[2018-06-02 11:21:36.488179] Iteration 14200, train loss = 0.171238, train accuracy = 0.992188\n",
      "[2018-06-02 11:21:55.650179] Iteration 14300, train loss = 0.142954, train accuracy = 1.000000\n",
      "[2018-06-02 11:22:14.823179] Iteration 14400, train loss = 0.143034, train accuracy = 1.000000\n",
      "[2018-06-02 11:22:33.997179] Iteration 14500, train loss = 0.141639, train accuracy = 1.000000\n",
      "[2018-06-02 11:22:53.139179] Iteration 14600, train loss = 0.190533, train accuracy = 0.984375\n",
      "[2018-06-02 11:23:12.263179] Iteration 14700, train loss = 0.159000, train accuracy = 0.984375\n",
      "[2018-06-02 11:23:31.356179] Iteration 14800, train loss = 0.141853, train accuracy = 1.000000\n",
      "[2018-06-02 11:23:50.556179] Iteration 14900, train loss = 0.145461, train accuracy = 1.000000\n",
      "[2018-06-02 11:24:09.733179] Iteration 15000, train loss = 0.157334, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927100\n",
      "[2018-06-02 11:24:33.156179] Iteration 15100, train loss = 0.136580, train accuracy = 1.000000\n",
      "[2018-06-02 11:24:52.366179] Iteration 15200, train loss = 0.150643, train accuracy = 0.992188\n",
      "[2018-06-02 11:25:11.513179] Iteration 15300, train loss = 0.175412, train accuracy = 0.984375\n",
      "[2018-06-02 11:25:30.637179] Iteration 15400, train loss = 0.141803, train accuracy = 1.000000\n",
      "[2018-06-02 11:25:49.887179] Iteration 15500, train loss = 0.139802, train accuracy = 1.000000\n",
      "[2018-06-02 11:26:09.131179] Iteration 15600, train loss = 0.158323, train accuracy = 0.984375\n",
      "[2018-06-02 11:26:28.282179] Iteration 15700, train loss = 0.139881, train accuracy = 1.000000\n",
      "[2018-06-02 11:26:47.422179] Iteration 15800, train loss = 0.146593, train accuracy = 1.000000\n",
      "[2018-06-02 11:27:06.513179] Iteration 15900, train loss = 0.139370, train accuracy = 1.000000\n",
      "[2018-06-02 11:27:25.812179] Iteration 16000, train loss = 0.138001, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 11:27:49.096179] Iteration 16100, train loss = 0.154818, train accuracy = 0.992188\n",
      "[2018-06-02 11:28:08.330179] Iteration 16200, train loss = 0.140179, train accuracy = 1.000000\n",
      "[2018-06-02 11:28:27.562179] Iteration 16300, train loss = 0.146036, train accuracy = 1.000000\n",
      "[2018-06-02 11:28:46.721179] Iteration 16400, train loss = 0.139977, train accuracy = 1.000000\n",
      "[2018-06-02 11:29:05.930179] Iteration 16500, train loss = 0.158406, train accuracy = 0.992188\n",
      "[2018-06-02 11:29:25.113179] Iteration 16600, train loss = 0.144189, train accuracy = 1.000000\n",
      "[2018-06-02 11:29:44.227179] Iteration 16700, train loss = 0.149068, train accuracy = 1.000000\n",
      "[2018-06-02 11:30:03.470179] Iteration 16800, train loss = 0.144207, train accuracy = 1.000000\n",
      "[2018-06-02 11:30:22.669179] Iteration 16900, train loss = 0.143940, train accuracy = 1.000000\n",
      "[2018-06-02 11:30:41.750179] Iteration 17000, train loss = 0.142904, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926800\n",
      "[2018-06-02 11:31:05.073179] Iteration 17100, train loss = 0.150191, train accuracy = 0.992188\n",
      "[2018-06-02 11:31:24.225179] Iteration 17200, train loss = 0.148669, train accuracy = 0.992188\n",
      "[2018-06-02 11:31:43.419179] Iteration 17300, train loss = 0.137293, train accuracy = 1.000000\n",
      "[2018-06-02 11:32:02.675179] Iteration 17400, train loss = 0.161935, train accuracy = 0.992188\n",
      "[2018-06-02 11:32:21.856179] Iteration 17500, train loss = 0.158314, train accuracy = 0.992188\n",
      "[2018-06-02 11:32:41.003179] Iteration 17600, train loss = 0.142832, train accuracy = 1.000000\n",
      "[2018-06-02 11:33:00.148179] Iteration 17700, train loss = 0.148531, train accuracy = 1.000000\n",
      "[2018-06-02 11:33:19.514179] Iteration 17800, train loss = 0.146513, train accuracy = 0.992188\n",
      "[2018-06-02 11:33:38.659179] Iteration 17900, train loss = 0.151720, train accuracy = 0.992188\n",
      "[2018-06-02 11:33:57.823179] Iteration 18000, train loss = 0.148611, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 11:34:21.124179] Iteration 18100, train loss = 0.150865, train accuracy = 0.992188\n",
      "[2018-06-02 11:34:40.294179] Iteration 18200, train loss = 0.142332, train accuracy = 1.000000\n",
      "[2018-06-02 11:34:59.574179] Iteration 18300, train loss = 0.151924, train accuracy = 0.992188\n",
      "[2018-06-02 11:35:18.720179] Iteration 18400, train loss = 0.152499, train accuracy = 0.992188\n",
      "[2018-06-02 11:35:37.978179] Iteration 18500, train loss = 0.140343, train accuracy = 1.000000\n",
      "[2018-06-02 11:35:57.218179] Iteration 18600, train loss = 0.137882, train accuracy = 1.000000\n",
      "[2018-06-02 11:36:16.370179] Iteration 18700, train loss = 0.141906, train accuracy = 1.000000\n",
      "[2018-06-02 11:36:35.515179] Iteration 18800, train loss = 0.143965, train accuracy = 1.000000\n",
      "[2018-06-02 11:36:54.658179] Iteration 18900, train loss = 0.150201, train accuracy = 0.992188\n",
      "[2018-06-02 11:37:13.914179] Iteration 19000, train loss = 0.144816, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "[2018-06-02 11:37:37.243179] Iteration 19100, train loss = 0.137708, train accuracy = 1.000000\n",
      "[2018-06-02 11:37:56.426179] Iteration 19200, train loss = 0.139387, train accuracy = 1.000000\n",
      "[2018-06-02 11:38:15.601179] Iteration 19300, train loss = 0.140488, train accuracy = 1.000000\n",
      "[2018-06-02 11:38:34.776179] Iteration 19400, train loss = 0.143210, train accuracy = 1.000000\n",
      "[2018-06-02 11:38:53.927179] Iteration 19500, train loss = 0.144224, train accuracy = 1.000000\n",
      "[2018-06-02 11:39:13.136179] Iteration 19600, train loss = 0.156444, train accuracy = 1.000000\n",
      "[2018-06-02 11:39:32.252179] Iteration 19700, train loss = 0.182029, train accuracy = 0.984375\n",
      "[2018-06-02 11:39:51.370179] Iteration 19800, train loss = 0.143498, train accuracy = 1.000000\n",
      "[2018-06-02 11:40:10.598179] Iteration 19900, train loss = 0.180962, train accuracy = 0.992188\n",
      "[2018-06-02 11:40:29.759179] Iteration 20000, train loss = 0.146189, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 11:40:53.024179] Iteration 20100, train loss = 0.139215, train accuracy = 1.000000\n",
      "[2018-06-02 11:41:12.203179] Iteration 20200, train loss = 0.139195, train accuracy = 1.000000\n",
      "[2018-06-02 11:41:31.449179] Iteration 20300, train loss = 0.148300, train accuracy = 0.992188\n",
      "[2018-06-02 11:41:50.665179] Iteration 20400, train loss = 0.145189, train accuracy = 1.000000\n",
      "[2018-06-02 11:42:09.798179] Iteration 20500, train loss = 0.148137, train accuracy = 0.992188\n",
      "[2018-06-02 11:42:28.954179] Iteration 20600, train loss = 0.137506, train accuracy = 1.000000\n",
      "[2018-06-02 11:42:48.158179] Iteration 20700, train loss = 0.164287, train accuracy = 0.984375\n",
      "[2018-06-02 11:43:07.295179] Iteration 20800, train loss = 0.142005, train accuracy = 1.000000\n",
      "[2018-06-02 11:43:26.473179] Iteration 20900, train loss = 0.140438, train accuracy = 1.000000\n",
      "[2018-06-02 11:43:45.644179] Iteration 21000, train loss = 0.154359, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928900\n",
      "[2018-06-02 11:44:09.018179] Iteration 21100, train loss = 0.149649, train accuracy = 0.992188\n",
      "[2018-06-02 11:44:28.165179] Iteration 21200, train loss = 0.150961, train accuracy = 1.000000\n",
      "[2018-06-02 11:44:47.270179] Iteration 21300, train loss = 0.138339, train accuracy = 1.000000\n",
      "[2018-06-02 11:45:06.519179] Iteration 21400, train loss = 0.144828, train accuracy = 1.000000\n",
      "[2018-06-02 11:45:25.662179] Iteration 21500, train loss = 0.161721, train accuracy = 0.992188\n",
      "[2018-06-02 11:45:44.757179] Iteration 21600, train loss = 0.159280, train accuracy = 0.992188\n",
      "[2018-06-02 11:46:03.989179] Iteration 21700, train loss = 0.174910, train accuracy = 0.984375\n",
      "[2018-06-02 11:46:23.317179] Iteration 21800, train loss = 0.139112, train accuracy = 1.000000\n",
      "[2018-06-02 11:46:42.558179] Iteration 21900, train loss = 0.171932, train accuracy = 0.984375\n",
      "[2018-06-02 11:47:01.687179] Iteration 22000, train loss = 0.179045, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 11:47:25.016179] Iteration 22100, train loss = 0.143511, train accuracy = 0.992188\n",
      "[2018-06-02 11:47:44.393179] Iteration 22200, train loss = 0.143551, train accuracy = 1.000000\n",
      "[2018-06-02 11:48:03.540179] Iteration 22300, train loss = 0.172415, train accuracy = 0.976562\n",
      "[2018-06-02 11:48:22.679179] Iteration 22400, train loss = 0.140415, train accuracy = 1.000000\n",
      "[2018-06-02 11:48:41.812179] Iteration 22500, train loss = 0.149572, train accuracy = 1.000000\n",
      "[2018-06-02 11:49:00.981179] Iteration 22600, train loss = 0.137598, train accuracy = 1.000000\n",
      "[2018-06-02 11:49:20.055179] Iteration 22700, train loss = 0.141410, train accuracy = 1.000000\n",
      "[2018-06-02 11:49:39.180179] Iteration 22800, train loss = 0.136389, train accuracy = 1.000000\n",
      "[2018-06-02 11:49:58.370179] Iteration 22900, train loss = 0.141941, train accuracy = 1.000000\n",
      "[2018-06-02 11:50:17.606179] Iteration 23000, train loss = 0.137251, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 11:50:40.974179] Iteration 23100, train loss = 0.153133, train accuracy = 0.992188\n",
      "[2018-06-02 11:51:00.186179] Iteration 23200, train loss = 0.141606, train accuracy = 1.000000\n",
      "[2018-06-02 11:51:19.414179] Iteration 23300, train loss = 0.144979, train accuracy = 1.000000\n",
      "[2018-06-02 11:51:38.570179] Iteration 23400, train loss = 0.154478, train accuracy = 0.992188\n",
      "[2018-06-02 11:51:57.856179] Iteration 23500, train loss = 0.137958, train accuracy = 1.000000\n",
      "[2018-06-02 11:52:17.023179] Iteration 23600, train loss = 0.147784, train accuracy = 1.000000\n",
      "[2018-06-02 11:52:36.138179] Iteration 23700, train loss = 0.141758, train accuracy = 1.000000\n",
      "[2018-06-02 11:52:55.313179] Iteration 23800, train loss = 0.138720, train accuracy = 1.000000\n",
      "[2018-06-02 11:53:14.462179] Iteration 23900, train loss = 0.143119, train accuracy = 1.000000\n",
      "[2018-06-02 11:53:33.599179] Iteration 24000, train loss = 0.159868, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 11:53:56.899179] Iteration 24100, train loss = 0.146996, train accuracy = 1.000000\n",
      "[2018-06-02 11:54:16.126179] Iteration 24200, train loss = 0.147745, train accuracy = 0.992188\n",
      "[2018-06-02 11:54:35.403179] Iteration 24300, train loss = 0.141570, train accuracy = 1.000000\n",
      "[2018-06-02 11:54:54.584179] Iteration 24400, train loss = 0.137373, train accuracy = 1.000000\n",
      "[2018-06-02 11:55:13.846179] Iteration 24500, train loss = 0.139785, train accuracy = 1.000000\n",
      "[2018-06-02 11:55:32.096179] Iteration 24600, train loss = 0.136323, train accuracy = 1.000000\n",
      "[2018-06-02 11:55:51.394179] Iteration 24700, train loss = 0.137629, train accuracy = 1.000000\n",
      "[2018-06-02 11:56:10.725179] Iteration 24800, train loss = 0.143565, train accuracy = 1.000000\n",
      "[2018-06-02 11:56:29.993179] Iteration 24900, train loss = 0.170373, train accuracy = 0.992188\n",
      "[2018-06-02 11:56:49.285179] Iteration 25000, train loss = 0.148303, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.929000\n",
      "[2018-06-02 11:57:12.780179] Iteration 25100, train loss = 0.139597, train accuracy = 1.000000\n",
      "[2018-06-02 11:57:32.030179] Iteration 25200, train loss = 0.139964, train accuracy = 1.000000\n",
      "[2018-06-02 11:57:51.362179] Iteration 25300, train loss = 0.140154, train accuracy = 1.000000\n",
      "[2018-06-02 11:58:10.743179] Iteration 25400, train loss = 0.143103, train accuracy = 1.000000\n",
      "[2018-06-02 11:58:30.117179] Iteration 25500, train loss = 0.145274, train accuracy = 1.000000\n",
      "[2018-06-02 11:58:49.499179] Iteration 25600, train loss = 0.141363, train accuracy = 1.000000\n",
      "[2018-06-02 11:59:08.842179] Iteration 25700, train loss = 0.145365, train accuracy = 1.000000\n",
      "[2018-06-02 11:59:28.284179] Iteration 25800, train loss = 0.143045, train accuracy = 1.000000\n",
      "[2018-06-02 11:59:47.622179] Iteration 25900, train loss = 0.138721, train accuracy = 1.000000\n",
      "[2018-06-02 12:00:07.010179] Iteration 26000, train loss = 0.140408, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 12:00:30.608179] Iteration 26100, train loss = 0.151240, train accuracy = 0.992188\n",
      "[2018-06-02 12:00:50.077179] Iteration 26200, train loss = 0.148456, train accuracy = 1.000000\n",
      "[2018-06-02 12:01:09.496179] Iteration 26300, train loss = 0.137969, train accuracy = 1.000000\n",
      "[2018-06-02 12:01:28.951179] Iteration 26400, train loss = 0.146465, train accuracy = 0.992188\n",
      "[2018-06-02 12:01:48.281179] Iteration 26500, train loss = 0.141668, train accuracy = 1.000000\n",
      "[2018-06-02 12:02:07.635179] Iteration 26600, train loss = 0.152839, train accuracy = 0.992188\n",
      "[2018-06-02 12:02:27.115179] Iteration 26700, train loss = 0.137041, train accuracy = 1.000000\n",
      "[2018-06-02 12:02:46.673179] Iteration 26800, train loss = 0.141361, train accuracy = 1.000000\n",
      "[2018-06-02 12:03:06.107179] Iteration 26900, train loss = 0.143450, train accuracy = 1.000000\n",
      "[2018-06-02 12:03:25.489179] Iteration 27000, train loss = 0.157475, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 12:03:49.075179] Iteration 27100, train loss = 0.138839, train accuracy = 1.000000\n",
      "[2018-06-02 12:04:08.532179] Iteration 27200, train loss = 0.150488, train accuracy = 1.000000\n",
      "[2018-06-02 12:04:27.925179] Iteration 27300, train loss = 0.138161, train accuracy = 1.000000\n",
      "[2018-06-02 12:04:47.307179] Iteration 27400, train loss = 0.150277, train accuracy = 1.000000\n",
      "[2018-06-02 12:05:06.794179] Iteration 27500, train loss = 0.142559, train accuracy = 1.000000\n",
      "[2018-06-02 12:05:26.325179] Iteration 27600, train loss = 0.138534, train accuracy = 1.000000\n",
      "[2018-06-02 12:05:45.802179] Iteration 27700, train loss = 0.158817, train accuracy = 1.000000\n",
      "[2018-06-02 12:06:05.307179] Iteration 27800, train loss = 0.142762, train accuracy = 1.000000\n",
      "[2018-06-02 12:06:24.816179] Iteration 27900, train loss = 0.137345, train accuracy = 1.000000\n",
      "[2018-06-02 12:06:44.281179] Iteration 28000, train loss = 0.141438, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 12:07:07.965179] Iteration 28100, train loss = 0.143342, train accuracy = 1.000000\n",
      "[2018-06-02 12:07:27.389179] Iteration 28200, train loss = 0.144960, train accuracy = 1.000000\n",
      "[2018-06-02 12:07:46.795179] Iteration 28300, train loss = 0.156929, train accuracy = 0.984375\n",
      "[2018-06-02 12:08:06.159179] Iteration 28400, train loss = 0.157392, train accuracy = 0.992188\n",
      "[2018-06-02 12:08:25.528179] Iteration 28500, train loss = 0.145030, train accuracy = 1.000000\n",
      "[2018-06-02 12:08:45.007179] Iteration 28600, train loss = 0.139788, train accuracy = 1.000000\n",
      "[2018-06-02 12:09:04.547179] Iteration 28700, train loss = 0.144853, train accuracy = 1.000000\n",
      "[2018-06-02 12:09:23.926179] Iteration 28800, train loss = 0.149788, train accuracy = 0.992188\n",
      "[2018-06-02 12:09:43.348179] Iteration 28900, train loss = 0.143315, train accuracy = 0.992188\n",
      "[2018-06-02 12:10:02.852179] Iteration 29000, train loss = 0.141147, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 12:10:26.460179] Iteration 29100, train loss = 0.141299, train accuracy = 1.000000\n",
      "[2018-06-02 12:10:45.796179] Iteration 29200, train loss = 0.171365, train accuracy = 0.992188\n",
      "[2018-06-02 12:11:05.222179] Iteration 29300, train loss = 0.157389, train accuracy = 0.992188\n",
      "[2018-06-02 12:11:24.542179] Iteration 29400, train loss = 0.136626, train accuracy = 1.000000\n",
      "[2018-06-02 12:11:43.783179] Iteration 29500, train loss = 0.161720, train accuracy = 0.984375\n",
      "[2018-06-02 12:12:03.037179] Iteration 29600, train loss = 0.153983, train accuracy = 1.000000\n",
      "[2018-06-02 12:12:22.422179] Iteration 29700, train loss = 0.139082, train accuracy = 1.000000\n",
      "[2018-06-02 12:12:41.794179] Iteration 29800, train loss = 0.141152, train accuracy = 1.000000\n",
      "[2018-06-02 12:13:01.150179] Iteration 29900, train loss = 0.139296, train accuracy = 1.000000\n",
      "[2018-06-02 12:13:20.513179] Iteration 30000, train loss = 0.146082, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 12:13:43.978179] Iteration 30100, train loss = 0.157728, train accuracy = 0.992188\n",
      "[2018-06-02 12:14:03.284179] Iteration 30200, train loss = 0.142037, train accuracy = 1.000000\n",
      "[2018-06-02 12:14:22.499179] Iteration 30300, train loss = 0.147383, train accuracy = 1.000000\n",
      "[2018-06-02 12:14:41.768179] Iteration 30400, train loss = 0.166735, train accuracy = 0.984375\n",
      "[2018-06-02 12:15:01.069179] Iteration 30500, train loss = 0.141087, train accuracy = 1.000000\n",
      "[2018-06-02 12:15:20.374179] Iteration 30600, train loss = 0.138891, train accuracy = 1.000000\n",
      "[2018-06-02 12:15:39.664179] Iteration 30700, train loss = 0.142240, train accuracy = 1.000000\n",
      "[2018-06-02 12:15:59.004179] Iteration 30800, train loss = 0.159829, train accuracy = 0.992188\n",
      "[2018-06-02 12:16:18.285179] Iteration 30900, train loss = 0.145420, train accuracy = 1.000000\n",
      "[2018-06-02 12:16:37.549179] Iteration 31000, train loss = 0.139178, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928800\n",
      "[2018-06-02 12:17:01.089179] Iteration 31100, train loss = 0.136530, train accuracy = 1.000000\n",
      "[2018-06-02 12:17:20.357179] Iteration 31200, train loss = 0.149560, train accuracy = 0.992188\n",
      "[2018-06-02 12:17:39.580179] Iteration 31300, train loss = 0.143349, train accuracy = 1.000000\n",
      "[2018-06-02 12:17:58.957179] Iteration 31400, train loss = 0.148376, train accuracy = 1.000000\n",
      "[2018-06-02 12:18:18.301179] Iteration 31500, train loss = 0.146276, train accuracy = 0.992188\n",
      "[2018-06-02 12:18:37.678179] Iteration 31600, train loss = 0.150109, train accuracy = 0.992188\n",
      "[2018-06-02 12:18:56.980179] Iteration 31700, train loss = 0.151581, train accuracy = 0.992188\n",
      "[2018-06-02 12:19:16.360179] Iteration 31800, train loss = 0.150783, train accuracy = 1.000000\n",
      "[2018-06-02 12:19:35.785179] Iteration 31900, train loss = 0.140218, train accuracy = 1.000000\n",
      "[2018-06-02 12:19:55.238179] Iteration 32000, train loss = 0.138555, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "[2018-06-02 12:20:18.771179] Iteration 32100, train loss = 0.161421, train accuracy = 0.984375\n",
      "[2018-06-02 12:20:38.109179] Iteration 32200, train loss = 0.151358, train accuracy = 0.992188\n",
      "[2018-06-02 12:20:57.372179] Iteration 32300, train loss = 0.150559, train accuracy = 0.992188\n",
      "[2018-06-02 12:21:16.706179] Iteration 32400, train loss = 0.138843, train accuracy = 1.000000\n",
      "[2018-06-02 12:21:36.082179] Iteration 32500, train loss = 0.143565, train accuracy = 1.000000\n",
      "[2018-06-02 12:21:55.423179] Iteration 32600, train loss = 0.153463, train accuracy = 0.992188\n",
      "[2018-06-02 12:22:14.710179] Iteration 32700, train loss = 0.140846, train accuracy = 1.000000\n",
      "[2018-06-02 12:22:34.014179] Iteration 32800, train loss = 0.176648, train accuracy = 0.984375\n",
      "[2018-06-02 12:22:53.316179] Iteration 32900, train loss = 0.137622, train accuracy = 1.000000\n",
      "[2018-06-02 12:23:12.655179] Iteration 33000, train loss = 0.140166, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 12:23:36.235179] Iteration 33100, train loss = 0.143245, train accuracy = 1.000000\n",
      "[2018-06-02 12:23:55.644179] Iteration 33200, train loss = 0.142910, train accuracy = 1.000000\n",
      "[2018-06-02 12:24:15.023179] Iteration 33300, train loss = 0.137691, train accuracy = 1.000000\n",
      "[2018-06-02 12:24:34.330179] Iteration 33400, train loss = 0.155663, train accuracy = 0.992188\n",
      "[2018-06-02 12:24:53.687179] Iteration 33500, train loss = 0.146262, train accuracy = 1.000000\n",
      "[2018-06-02 12:25:13.020179] Iteration 33600, train loss = 0.137532, train accuracy = 1.000000\n",
      "[2018-06-02 12:25:32.311179] Iteration 33700, train loss = 0.158748, train accuracy = 0.984375\n",
      "[2018-06-02 12:25:51.560179] Iteration 33800, train loss = 0.142049, train accuracy = 1.000000\n",
      "[2018-06-02 12:26:10.837179] Iteration 33900, train loss = 0.141364, train accuracy = 1.000000\n",
      "[2018-06-02 12:26:30.157179] Iteration 34000, train loss = 0.146465, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 12:26:53.710179] Iteration 34100, train loss = 0.146569, train accuracy = 0.992188\n",
      "[2018-06-02 12:27:13.060179] Iteration 34200, train loss = 0.145334, train accuracy = 1.000000\n",
      "[2018-06-02 12:27:32.336179] Iteration 34300, train loss = 0.148549, train accuracy = 0.992188\n",
      "[2018-06-02 12:27:51.688179] Iteration 34400, train loss = 0.143731, train accuracy = 1.000000\n",
      "[2018-06-02 12:28:10.914179] Iteration 34500, train loss = 0.142795, train accuracy = 1.000000\n",
      "[2018-06-02 12:28:30.118179] Iteration 34600, train loss = 0.137568, train accuracy = 1.000000\n",
      "[2018-06-02 12:28:49.417179] Iteration 34700, train loss = 0.151809, train accuracy = 0.992188\n",
      "[2018-06-02 12:29:08.752179] Iteration 34800, train loss = 0.143017, train accuracy = 1.000000\n",
      "[2018-06-02 12:29:28.043179] Iteration 34900, train loss = 0.145058, train accuracy = 1.000000\n",
      "[2018-06-02 12:29:47.374179] Iteration 35000, train loss = 0.138543, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 12:30:10.874179] Iteration 35100, train loss = 0.148224, train accuracy = 0.992188\n",
      "[2018-06-02 12:30:30.170179] Iteration 35200, train loss = 0.142503, train accuracy = 1.000000\n",
      "[2018-06-02 12:30:49.610179] Iteration 35300, train loss = 0.143819, train accuracy = 1.000000\n",
      "[2018-06-02 12:31:08.973179] Iteration 35400, train loss = 0.145374, train accuracy = 1.000000\n",
      "[2018-06-02 12:31:28.232179] Iteration 35500, train loss = 0.146036, train accuracy = 0.992188\n",
      "[2018-06-02 12:31:47.482179] Iteration 35600, train loss = 0.140437, train accuracy = 1.000000\n",
      "[2018-06-02 12:32:06.785179] Iteration 35700, train loss = 0.145330, train accuracy = 1.000000\n",
      "[2018-06-02 12:32:26.077179] Iteration 35800, train loss = 0.145468, train accuracy = 1.000000\n",
      "[2018-06-02 12:32:45.525179] Iteration 35900, train loss = 0.139948, train accuracy = 1.000000\n",
      "[2018-06-02 12:33:04.839179] Iteration 36000, train loss = 0.141988, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928100\n",
      "[2018-06-02 12:33:28.402179] Iteration 36100, train loss = 0.151760, train accuracy = 1.000000\n",
      "[2018-06-02 12:33:47.771179] Iteration 36200, train loss = 0.164045, train accuracy = 0.984375\n",
      "[2018-06-02 12:34:07.080179] Iteration 36300, train loss = 0.157127, train accuracy = 0.992188\n",
      "[2018-06-02 12:34:26.471179] Iteration 36400, train loss = 0.154606, train accuracy = 0.992188\n",
      "[2018-06-02 12:34:45.678179] Iteration 36500, train loss = 0.142648, train accuracy = 1.000000\n",
      "[2018-06-02 12:35:04.984179] Iteration 36600, train loss = 0.147252, train accuracy = 1.000000\n",
      "[2018-06-02 12:35:24.185179] Iteration 36700, train loss = 0.149262, train accuracy = 0.992188\n",
      "[2018-06-02 12:35:43.510179] Iteration 36800, train loss = 0.138313, train accuracy = 1.000000\n",
      "[2018-06-02 12:36:02.754179] Iteration 36900, train loss = 0.148173, train accuracy = 1.000000\n",
      "[2018-06-02 12:36:21.958179] Iteration 37000, train loss = 0.137557, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928100\n",
      "[2018-06-02 12:36:45.268179] Iteration 37100, train loss = 0.141526, train accuracy = 1.000000\n",
      "[2018-06-02 12:37:04.467179] Iteration 37200, train loss = 0.142806, train accuracy = 1.000000\n",
      "[2018-06-02 12:37:23.701179] Iteration 37300, train loss = 0.141582, train accuracy = 1.000000\n",
      "[2018-06-02 12:37:42.983179] Iteration 37400, train loss = 0.144580, train accuracy = 1.000000\n",
      "[2018-06-02 12:38:02.161179] Iteration 37500, train loss = 0.136324, train accuracy = 1.000000\n",
      "[2018-06-02 12:38:21.371179] Iteration 37600, train loss = 0.141839, train accuracy = 1.000000\n",
      "[2018-06-02 12:38:40.558179] Iteration 37700, train loss = 0.142550, train accuracy = 1.000000\n",
      "[2018-06-02 12:38:59.859179] Iteration 37800, train loss = 0.138511, train accuracy = 1.000000\n",
      "[2018-06-02 12:39:19.058179] Iteration 37900, train loss = 0.144562, train accuracy = 1.000000\n",
      "[2018-06-02 12:39:38.380179] Iteration 38000, train loss = 0.140781, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 12:40:01.673179] Iteration 38100, train loss = 0.139077, train accuracy = 1.000000\n",
      "[2018-06-02 12:40:20.933179] Iteration 38200, train loss = 0.141135, train accuracy = 1.000000\n",
      "[2018-06-02 12:40:40.210179] Iteration 38300, train loss = 0.144844, train accuracy = 1.000000\n",
      "[2018-06-02 12:40:59.477179] Iteration 38400, train loss = 0.148540, train accuracy = 1.000000\n",
      "[2018-06-02 12:41:18.694179] Iteration 38500, train loss = 0.150641, train accuracy = 0.992188\n",
      "[2018-06-02 12:41:38.010179] Iteration 38600, train loss = 0.144375, train accuracy = 1.000000\n",
      "[2018-06-02 12:41:57.281179] Iteration 38700, train loss = 0.161187, train accuracy = 0.984375\n",
      "[2018-06-02 12:42:16.464179] Iteration 38800, train loss = 0.168444, train accuracy = 0.992188\n",
      "[2018-06-02 12:42:35.714179] Iteration 38900, train loss = 0.142420, train accuracy = 1.000000\n",
      "[2018-06-02 12:42:54.874179] Iteration 39000, train loss = 0.152387, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 12:43:18.304179] Iteration 39100, train loss = 0.138414, train accuracy = 1.000000\n",
      "[2018-06-02 12:43:37.511179] Iteration 39200, train loss = 0.141537, train accuracy = 1.000000\n",
      "[2018-06-02 12:43:56.726179] Iteration 39300, train loss = 0.154394, train accuracy = 0.984375\n",
      "[2018-06-02 12:44:15.940179] Iteration 39400, train loss = 0.143864, train accuracy = 1.000000\n",
      "[2018-06-02 12:44:35.264179] Iteration 39500, train loss = 0.139650, train accuracy = 1.000000\n",
      "[2018-06-02 12:44:54.514179] Iteration 39600, train loss = 0.147524, train accuracy = 0.992188\n",
      "[2018-06-02 12:45:13.820179] Iteration 39700, train loss = 0.144129, train accuracy = 1.000000\n",
      "[2018-06-02 12:45:33.065179] Iteration 39800, train loss = 0.146099, train accuracy = 1.000000\n",
      "[2018-06-02 12:45:52.344179] Iteration 39900, train loss = 0.153957, train accuracy = 0.992188\n"
     ]
    }
   ],
   "source": [
    "curr_lr = 0\n",
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二轮 量化\n",
    "prune_rate = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.928300\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.03965073  0.125      -0.04613209 -0.0625\n",
      "  0.00102084 -0.00172143  0.04198297 -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.75)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 12:49:45.095179] Iteration 100, train loss = 0.174229, train accuracy = 0.984375\n",
      "[2018-06-02 12:50:04.320179] Iteration 200, train loss = 0.171929, train accuracy = 0.984375\n",
      "[2018-06-02 12:50:23.459179] Iteration 300, train loss = 0.203142, train accuracy = 0.976562\n",
      "[2018-06-02 12:50:42.595179] Iteration 400, train loss = 0.152575, train accuracy = 0.992188\n",
      "[2018-06-02 12:51:01.698179] Iteration 500, train loss = 0.165774, train accuracy = 0.984375\n",
      "[2018-06-02 12:51:20.841179] Iteration 600, train loss = 0.165610, train accuracy = 0.992188\n",
      "[2018-06-02 12:51:40.005179] Iteration 700, train loss = 0.144354, train accuracy = 1.000000\n",
      "[2018-06-02 12:51:59.145179] Iteration 800, train loss = 0.159273, train accuracy = 0.992188\n",
      "[2018-06-02 12:52:18.277179] Iteration 900, train loss = 0.182365, train accuracy = 0.976562\n",
      "[2018-06-02 12:52:37.508179] Iteration 1000, train loss = 0.178810, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.923000\n",
      "[2018-06-02 12:53:00.950179] Iteration 1100, train loss = 0.148592, train accuracy = 0.992188\n",
      "[2018-06-02 12:53:20.215179] Iteration 1200, train loss = 0.146108, train accuracy = 1.000000\n",
      "[2018-06-02 12:53:39.447179] Iteration 1300, train loss = 0.152579, train accuracy = 1.000000\n",
      "[2018-06-02 12:53:58.608179] Iteration 1400, train loss = 0.160001, train accuracy = 0.984375\n",
      "[2018-06-02 12:54:17.763179] Iteration 1500, train loss = 0.155893, train accuracy = 0.992188\n",
      "[2018-06-02 12:54:36.933179] Iteration 1600, train loss = 0.157397, train accuracy = 0.992188\n",
      "[2018-06-02 12:54:56.161179] Iteration 1700, train loss = 0.142435, train accuracy = 1.000000\n",
      "[2018-06-02 12:55:15.396179] Iteration 1800, train loss = 0.147982, train accuracy = 1.000000\n",
      "[2018-06-02 12:55:34.575179] Iteration 1900, train loss = 0.157121, train accuracy = 1.000000\n",
      "[2018-06-02 12:55:53.839179] Iteration 2000, train loss = 0.156864, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-02 12:56:17.232179] Iteration 2100, train loss = 0.157605, train accuracy = 0.992188\n",
      "[2018-06-02 12:56:36.400179] Iteration 2200, train loss = 0.158683, train accuracy = 0.992188\n",
      "[2018-06-02 12:56:55.610179] Iteration 2300, train loss = 0.152549, train accuracy = 0.992188\n",
      "[2018-06-02 12:57:14.842179] Iteration 2400, train loss = 0.145168, train accuracy = 1.000000\n",
      "[2018-06-02 12:57:33.975179] Iteration 2500, train loss = 0.147482, train accuracy = 1.000000\n",
      "[2018-06-02 12:57:53.225179] Iteration 2600, train loss = 0.148042, train accuracy = 1.000000\n",
      "[2018-06-02 12:58:12.340179] Iteration 2700, train loss = 0.141878, train accuracy = 1.000000\n",
      "[2018-06-02 12:58:31.537179] Iteration 2800, train loss = 0.154217, train accuracy = 0.992188\n",
      "[2018-06-02 12:58:50.719179] Iteration 2900, train loss = 0.152984, train accuracy = 0.992188\n",
      "[2018-06-02 12:59:09.800179] Iteration 3000, train loss = 0.155677, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 12:59:33.146179] Iteration 3100, train loss = 0.190052, train accuracy = 0.976562\n",
      "[2018-06-02 12:59:52.334179] Iteration 3200, train loss = 0.153030, train accuracy = 1.000000\n",
      "[2018-06-02 13:00:11.601179] Iteration 3300, train loss = 0.162828, train accuracy = 0.984375\n",
      "[2018-06-02 13:00:30.854179] Iteration 3400, train loss = 0.163492, train accuracy = 0.992188\n",
      "[2018-06-02 13:00:50.056179] Iteration 3500, train loss = 0.162444, train accuracy = 0.992188\n",
      "[2018-06-02 13:01:09.297179] Iteration 3600, train loss = 0.146262, train accuracy = 1.000000\n",
      "[2018-06-02 13:01:28.485179] Iteration 3700, train loss = 0.143861, train accuracy = 1.000000\n",
      "[2018-06-02 13:01:47.630179] Iteration 3800, train loss = 0.163535, train accuracy = 0.992188\n",
      "[2018-06-02 13:02:06.846179] Iteration 3900, train loss = 0.153262, train accuracy = 0.992188\n",
      "[2018-06-02 13:02:25.990179] Iteration 4000, train loss = 0.146997, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 13:02:49.461179] Iteration 4100, train loss = 0.140766, train accuracy = 1.000000\n",
      "[2018-06-02 13:03:08.603179] Iteration 4200, train loss = 0.161991, train accuracy = 0.992188\n",
      "[2018-06-02 13:03:27.851179] Iteration 4300, train loss = 0.143099, train accuracy = 1.000000\n",
      "[2018-06-02 13:03:46.939179] Iteration 4400, train loss = 0.143612, train accuracy = 1.000000\n",
      "[2018-06-02 13:04:06.167179] Iteration 4500, train loss = 0.189856, train accuracy = 0.984375\n",
      "[2018-06-02 13:04:25.389179] Iteration 4600, train loss = 0.146550, train accuracy = 0.992188\n",
      "[2018-06-02 13:04:44.564179] Iteration 4700, train loss = 0.136884, train accuracy = 1.000000\n",
      "[2018-06-02 13:05:03.723179] Iteration 4800, train loss = 0.154179, train accuracy = 0.992188\n",
      "[2018-06-02 13:05:22.980179] Iteration 4900, train loss = 0.158106, train accuracy = 0.992188\n",
      "[2018-06-02 13:05:42.216179] Iteration 5000, train loss = 0.154857, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926300\n",
      "[2018-06-02 13:06:05.474179] Iteration 5100, train loss = 0.146840, train accuracy = 1.000000\n",
      "[2018-06-02 13:06:24.680179] Iteration 5200, train loss = 0.159423, train accuracy = 0.992188\n",
      "[2018-06-02 13:06:43.912179] Iteration 5300, train loss = 0.183565, train accuracy = 0.976562\n",
      "[2018-06-02 13:07:03.111179] Iteration 5400, train loss = 0.152990, train accuracy = 0.992188\n",
      "[2018-06-02 13:07:22.396179] Iteration 5500, train loss = 0.163738, train accuracy = 0.992188\n",
      "[2018-06-02 13:07:41.543179] Iteration 5600, train loss = 0.174577, train accuracy = 0.984375\n",
      "[2018-06-02 13:08:00.717179] Iteration 5700, train loss = 0.169562, train accuracy = 0.992188\n",
      "[2018-06-02 13:08:19.892179] Iteration 5800, train loss = 0.147087, train accuracy = 1.000000\n",
      "[2018-06-02 13:08:39.099179] Iteration 5900, train loss = 0.188863, train accuracy = 0.984375\n",
      "[2018-06-02 13:08:58.226179] Iteration 6000, train loss = 0.160191, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926600\n",
      "[2018-06-02 13:09:21.606179] Iteration 6100, train loss = 0.138919, train accuracy = 1.000000\n",
      "[2018-06-02 13:09:40.821179] Iteration 6200, train loss = 0.142515, train accuracy = 1.000000\n",
      "[2018-06-02 13:10:00.071179] Iteration 6300, train loss = 0.144726, train accuracy = 1.000000\n",
      "[2018-06-02 13:10:19.288179] Iteration 6400, train loss = 0.157649, train accuracy = 0.992188\n",
      "[2018-06-02 13:10:38.415179] Iteration 6500, train loss = 0.167442, train accuracy = 0.984375\n",
      "[2018-06-02 13:10:57.650179] Iteration 6600, train loss = 0.140418, train accuracy = 1.000000\n",
      "[2018-06-02 13:11:16.863179] Iteration 6700, train loss = 0.149728, train accuracy = 0.992188\n",
      "[2018-06-02 13:11:36.089179] Iteration 6800, train loss = 0.148254, train accuracy = 0.992188\n",
      "[2018-06-02 13:11:55.270179] Iteration 6900, train loss = 0.144798, train accuracy = 1.000000\n",
      "[2018-06-02 13:12:14.446179] Iteration 7000, train loss = 0.149351, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926900\n",
      "[2018-06-02 13:12:37.710179] Iteration 7100, train loss = 0.160840, train accuracy = 0.984375\n",
      "[2018-06-02 13:12:57.005179] Iteration 7200, train loss = 0.150410, train accuracy = 1.000000\n",
      "[2018-06-02 13:13:16.131179] Iteration 7300, train loss = 0.145826, train accuracy = 1.000000\n",
      "[2018-06-02 13:13:35.338179] Iteration 7400, train loss = 0.145192, train accuracy = 1.000000\n",
      "[2018-06-02 13:13:54.491179] Iteration 7500, train loss = 0.161720, train accuracy = 0.992188\n",
      "[2018-06-02 13:14:13.709179] Iteration 7600, train loss = 0.150517, train accuracy = 0.992188\n",
      "[2018-06-02 13:14:32.912179] Iteration 7700, train loss = 0.155232, train accuracy = 0.992188\n",
      "[2018-06-02 13:14:52.029179] Iteration 7800, train loss = 0.160143, train accuracy = 0.992188\n",
      "[2018-06-02 13:15:11.246179] Iteration 7900, train loss = 0.161636, train accuracy = 0.992188\n",
      "[2018-06-02 13:15:30.439179] Iteration 8000, train loss = 0.141973, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926700\n",
      "[2018-06-02 13:15:53.756179] Iteration 8100, train loss = 0.153636, train accuracy = 0.992188\n",
      "[2018-06-02 13:16:12.949179] Iteration 8200, train loss = 0.145232, train accuracy = 1.000000\n",
      "[2018-06-02 13:16:32.164179] Iteration 8300, train loss = 0.142958, train accuracy = 1.000000\n",
      "[2018-06-02 13:16:51.440179] Iteration 8400, train loss = 0.153913, train accuracy = 1.000000\n",
      "[2018-06-02 13:17:10.616179] Iteration 8500, train loss = 0.158087, train accuracy = 1.000000\n",
      "[2018-06-02 13:17:29.880179] Iteration 8600, train loss = 0.159250, train accuracy = 0.992188\n",
      "[2018-06-02 13:17:49.029179] Iteration 8700, train loss = 0.159868, train accuracy = 0.992188\n",
      "[2018-06-02 13:18:08.248179] Iteration 8800, train loss = 0.161461, train accuracy = 0.992188\n",
      "[2018-06-02 13:18:27.493179] Iteration 8900, train loss = 0.156274, train accuracy = 0.992188\n",
      "[2018-06-02 13:18:46.720179] Iteration 9000, train loss = 0.155669, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 13:19:10.067179] Iteration 9100, train loss = 0.161558, train accuracy = 0.992188\n",
      "[2018-06-02 13:19:29.345179] Iteration 9200, train loss = 0.143197, train accuracy = 1.000000\n",
      "[2018-06-02 13:19:48.540179] Iteration 9300, train loss = 0.156146, train accuracy = 0.992188\n",
      "[2018-06-02 13:20:07.679179] Iteration 9400, train loss = 0.143557, train accuracy = 0.992188\n",
      "[2018-06-02 13:20:26.819179] Iteration 9500, train loss = 0.138526, train accuracy = 1.000000\n",
      "[2018-06-02 13:20:46.014179] Iteration 9600, train loss = 0.159298, train accuracy = 0.992188\n",
      "[2018-06-02 13:21:05.322179] Iteration 9700, train loss = 0.139835, train accuracy = 1.000000\n",
      "[2018-06-02 13:21:24.509179] Iteration 9800, train loss = 0.145982, train accuracy = 0.992188\n",
      "[2018-06-02 13:21:43.669179] Iteration 9900, train loss = 0.158638, train accuracy = 0.992188\n",
      "[2018-06-02 13:22:02.876179] Iteration 10000, train loss = 0.138468, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 13:22:26.234179] Iteration 10100, train loss = 0.143789, train accuracy = 1.000000\n",
      "[2018-06-02 13:22:45.456179] Iteration 10200, train loss = 0.155198, train accuracy = 0.992188\n",
      "[2018-06-02 13:23:04.654179] Iteration 10300, train loss = 0.156248, train accuracy = 0.992188\n",
      "[2018-06-02 13:23:23.841179] Iteration 10400, train loss = 0.154832, train accuracy = 0.992188\n",
      "[2018-06-02 13:23:43.048179] Iteration 10500, train loss = 0.163702, train accuracy = 0.992188\n",
      "[2018-06-02 13:24:02.266179] Iteration 10600, train loss = 0.166685, train accuracy = 0.992188\n",
      "[2018-06-02 13:24:21.461179] Iteration 10700, train loss = 0.149803, train accuracy = 1.000000\n",
      "[2018-06-02 13:24:40.709179] Iteration 10800, train loss = 0.147454, train accuracy = 1.000000\n",
      "[2018-06-02 13:24:59.754179] Iteration 10900, train loss = 0.166428, train accuracy = 0.992188\n",
      "[2018-06-02 13:25:18.975179] Iteration 11000, train loss = 0.145184, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927800\n",
      "[2018-06-02 13:25:42.261179] Iteration 11100, train loss = 0.150645, train accuracy = 1.000000\n",
      "[2018-06-02 13:26:01.443179] Iteration 11200, train loss = 0.149578, train accuracy = 1.000000\n",
      "[2018-06-02 13:26:20.595179] Iteration 11300, train loss = 0.161766, train accuracy = 0.992188\n",
      "[2018-06-02 13:26:39.830179] Iteration 11400, train loss = 0.147785, train accuracy = 0.992188\n",
      "[2018-06-02 13:26:59.035179] Iteration 11500, train loss = 0.156256, train accuracy = 0.992188\n",
      "[2018-06-02 13:27:18.268179] Iteration 11600, train loss = 0.135838, train accuracy = 1.000000\n",
      "[2018-06-02 13:27:37.424179] Iteration 11700, train loss = 0.150164, train accuracy = 1.000000\n",
      "[2018-06-02 13:27:56.635179] Iteration 11800, train loss = 0.145167, train accuracy = 1.000000\n",
      "[2018-06-02 13:28:15.828179] Iteration 11900, train loss = 0.149526, train accuracy = 1.000000\n",
      "[2018-06-02 13:28:34.984179] Iteration 12000, train loss = 0.147641, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927600\n",
      "[2018-06-02 13:28:58.283179] Iteration 12100, train loss = 0.160323, train accuracy = 0.992188\n",
      "[2018-06-02 13:29:17.533179] Iteration 12200, train loss = 0.143256, train accuracy = 1.000000\n",
      "[2018-06-02 13:29:36.738179] Iteration 12300, train loss = 0.150001, train accuracy = 1.000000\n",
      "[2018-06-02 13:29:55.843179] Iteration 12400, train loss = 0.141804, train accuracy = 1.000000\n",
      "[2018-06-02 13:30:15.034179] Iteration 12500, train loss = 0.147624, train accuracy = 1.000000\n",
      "[2018-06-02 13:30:34.219179] Iteration 12600, train loss = 0.152732, train accuracy = 1.000000\n",
      "[2018-06-02 13:30:53.433179] Iteration 12700, train loss = 0.178056, train accuracy = 0.984375\n",
      "[2018-06-02 13:31:12.615179] Iteration 12800, train loss = 0.175435, train accuracy = 0.976562\n",
      "[2018-06-02 13:31:31.792179] Iteration 12900, train loss = 0.143814, train accuracy = 1.000000\n",
      "[2018-06-02 13:31:51.007179] Iteration 13000, train loss = 0.141516, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 13:32:14.370179] Iteration 13100, train loss = 0.156542, train accuracy = 0.992188\n",
      "[2018-06-02 13:32:33.530179] Iteration 13200, train loss = 0.151021, train accuracy = 0.992188\n",
      "[2018-06-02 13:32:52.769179] Iteration 13300, train loss = 0.147580, train accuracy = 1.000000\n",
      "[2018-06-02 13:33:11.918179] Iteration 13400, train loss = 0.156195, train accuracy = 0.984375\n",
      "[2018-06-02 13:33:31.130179] Iteration 13500, train loss = 0.166575, train accuracy = 0.984375\n",
      "[2018-06-02 13:33:50.248179] Iteration 13600, train loss = 0.203761, train accuracy = 0.984375\n",
      "[2018-06-02 13:34:09.454179] Iteration 13700, train loss = 0.157867, train accuracy = 0.992188\n",
      "[2018-06-02 13:34:28.591179] Iteration 13800, train loss = 0.146981, train accuracy = 1.000000\n",
      "[2018-06-02 13:34:47.856179] Iteration 13900, train loss = 0.156151, train accuracy = 0.992188\n",
      "[2018-06-02 13:35:07.032179] Iteration 14000, train loss = 0.146078, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 13:35:30.283179] Iteration 14100, train loss = 0.156354, train accuracy = 0.992188\n",
      "[2018-06-02 13:35:49.556179] Iteration 14200, train loss = 0.154014, train accuracy = 1.000000\n",
      "[2018-06-02 13:36:08.717179] Iteration 14300, train loss = 0.176603, train accuracy = 0.976562\n",
      "[2018-06-02 13:36:27.975179] Iteration 14400, train loss = 0.150180, train accuracy = 0.992188\n",
      "[2018-06-02 13:36:47.116179] Iteration 14500, train loss = 0.142853, train accuracy = 1.000000\n",
      "[2018-06-02 13:37:06.307179] Iteration 14600, train loss = 0.142350, train accuracy = 1.000000\n",
      "[2018-06-02 13:37:25.472179] Iteration 14700, train loss = 0.145062, train accuracy = 1.000000\n",
      "[2018-06-02 13:37:44.701179] Iteration 14800, train loss = 0.141064, train accuracy = 1.000000\n",
      "[2018-06-02 13:38:03.943179] Iteration 14900, train loss = 0.144920, train accuracy = 1.000000\n",
      "[2018-06-02 13:38:23.072179] Iteration 15000, train loss = 0.152841, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 13:38:46.410179] Iteration 15100, train loss = 0.167698, train accuracy = 1.000000\n",
      "[2018-06-02 13:39:05.563179] Iteration 15200, train loss = 0.144600, train accuracy = 1.000000\n",
      "[2018-06-02 13:39:24.776179] Iteration 15300, train loss = 0.145609, train accuracy = 1.000000\n",
      "[2018-06-02 13:39:43.955179] Iteration 15400, train loss = 0.152925, train accuracy = 1.000000\n",
      "[2018-06-02 13:40:03.136179] Iteration 15500, train loss = 0.139335, train accuracy = 1.000000\n",
      "[2018-06-02 13:40:22.355179] Iteration 15600, train loss = 0.166802, train accuracy = 0.984375\n",
      "[2018-06-02 13:40:41.521179] Iteration 15700, train loss = 0.148285, train accuracy = 1.000000\n",
      "[2018-06-02 13:41:00.708179] Iteration 15800, train loss = 0.151441, train accuracy = 0.992188\n",
      "[2018-06-02 13:41:19.939179] Iteration 15900, train loss = 0.147569, train accuracy = 1.000000\n",
      "[2018-06-02 13:41:39.101179] Iteration 16000, train loss = 0.158214, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 13:42:02.509179] Iteration 16100, train loss = 0.147844, train accuracy = 0.992188\n",
      "[2018-06-02 13:42:21.737179] Iteration 16200, train loss = 0.170938, train accuracy = 0.992188\n",
      "[2018-06-02 13:42:41.004179] Iteration 16300, train loss = 0.147806, train accuracy = 1.000000\n",
      "[2018-06-02 13:43:00.205179] Iteration 16400, train loss = 0.147551, train accuracy = 1.000000\n",
      "[2018-06-02 13:43:19.396179] Iteration 16500, train loss = 0.143360, train accuracy = 1.000000\n",
      "[2018-06-02 13:43:38.618179] Iteration 16600, train loss = 0.148125, train accuracy = 1.000000\n",
      "[2018-06-02 13:43:57.758179] Iteration 16700, train loss = 0.138876, train accuracy = 1.000000\n",
      "[2018-06-02 13:44:16.924179] Iteration 16800, train loss = 0.153158, train accuracy = 0.992188\n",
      "[2018-06-02 13:44:36.177179] Iteration 16900, train loss = 0.150495, train accuracy = 0.992188\n",
      "[2018-06-02 13:44:55.345179] Iteration 17000, train loss = 0.143929, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.929100\n",
      "[2018-06-02 13:45:18.719179] Iteration 17100, train loss = 0.138688, train accuracy = 1.000000\n",
      "[2018-06-02 13:45:37.922179] Iteration 17200, train loss = 0.145293, train accuracy = 1.000000\n",
      "[2018-06-02 13:45:57.077179] Iteration 17300, train loss = 0.162305, train accuracy = 0.992188\n",
      "[2018-06-02 13:46:16.291179] Iteration 17400, train loss = 0.156415, train accuracy = 0.992188\n",
      "[2018-06-02 13:46:35.453179] Iteration 17500, train loss = 0.146062, train accuracy = 1.000000\n",
      "[2018-06-02 13:46:54.591179] Iteration 17600, train loss = 0.148996, train accuracy = 1.000000\n",
      "[2018-06-02 13:47:13.741179] Iteration 17700, train loss = 0.143255, train accuracy = 1.000000\n",
      "[2018-06-02 13:47:32.980179] Iteration 17800, train loss = 0.146207, train accuracy = 1.000000\n",
      "[2018-06-02 13:47:52.252179] Iteration 17900, train loss = 0.138482, train accuracy = 1.000000\n",
      "[2018-06-02 13:48:11.492179] Iteration 18000, train loss = 0.151940, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "[2018-06-02 13:48:34.836179] Iteration 18100, train loss = 0.143892, train accuracy = 1.000000\n",
      "[2018-06-02 13:48:54.032179] Iteration 18200, train loss = 0.148862, train accuracy = 1.000000\n",
      "[2018-06-02 13:49:13.189179] Iteration 18300, train loss = 0.154966, train accuracy = 0.992188\n",
      "[2018-06-02 13:49:32.524179] Iteration 18400, train loss = 0.139149, train accuracy = 1.000000\n",
      "[2018-06-02 13:49:51.763179] Iteration 18500, train loss = 0.145389, train accuracy = 1.000000\n",
      "[2018-06-02 13:50:11.006179] Iteration 18600, train loss = 0.145816, train accuracy = 1.000000\n",
      "[2018-06-02 13:50:30.173179] Iteration 18700, train loss = 0.147077, train accuracy = 1.000000\n",
      "[2018-06-02 13:50:49.366179] Iteration 18800, train loss = 0.145760, train accuracy = 0.992188\n",
      "[2018-06-02 13:51:08.606179] Iteration 18900, train loss = 0.150777, train accuracy = 0.992188\n",
      "[2018-06-02 13:51:27.792179] Iteration 19000, train loss = 0.141522, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 13:51:51.037179] Iteration 19100, train loss = 0.149117, train accuracy = 1.000000\n",
      "[2018-06-02 13:52:10.320179] Iteration 19200, train loss = 0.138932, train accuracy = 1.000000\n",
      "[2018-06-02 13:52:29.512179] Iteration 19300, train loss = 0.153576, train accuracy = 1.000000\n",
      "[2018-06-02 13:52:48.692179] Iteration 19400, train loss = 0.170459, train accuracy = 0.984375\n",
      "[2018-06-02 13:53:07.947179] Iteration 19500, train loss = 0.143504, train accuracy = 1.000000\n",
      "[2018-06-02 13:53:27.111179] Iteration 19600, train loss = 0.152532, train accuracy = 1.000000\n",
      "[2018-06-02 13:53:46.273179] Iteration 19700, train loss = 0.142116, train accuracy = 1.000000\n",
      "[2018-06-02 13:54:05.493179] Iteration 19800, train loss = 0.137743, train accuracy = 1.000000\n",
      "[2018-06-02 13:54:24.652179] Iteration 19900, train loss = 0.140614, train accuracy = 1.000000\n",
      "[2018-06-02 13:54:43.752179] Iteration 20000, train loss = 0.150664, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 13:55:07.045179] Iteration 20100, train loss = 0.152164, train accuracy = 1.000000\n",
      "[2018-06-02 13:55:26.188179] Iteration 20200, train loss = 0.159607, train accuracy = 0.992188\n",
      "[2018-06-02 13:55:45.378179] Iteration 20300, train loss = 0.144770, train accuracy = 1.000000\n",
      "[2018-06-02 13:56:04.489179] Iteration 20400, train loss = 0.152825, train accuracy = 0.992188\n",
      "[2018-06-02 13:56:23.596179] Iteration 20500, train loss = 0.159405, train accuracy = 0.992188\n",
      "[2018-06-02 13:56:42.834179] Iteration 20600, train loss = 0.140393, train accuracy = 1.000000\n",
      "[2018-06-02 13:57:02.016179] Iteration 20700, train loss = 0.141671, train accuracy = 1.000000\n",
      "[2018-06-02 13:57:21.150179] Iteration 20800, train loss = 0.144884, train accuracy = 1.000000\n",
      "[2018-06-02 13:57:40.301179] Iteration 20900, train loss = 0.145453, train accuracy = 1.000000\n",
      "[2018-06-02 13:57:59.536179] Iteration 21000, train loss = 0.159624, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 13:58:22.933179] Iteration 21100, train loss = 0.146714, train accuracy = 1.000000\n",
      "[2018-06-02 13:58:42.124179] Iteration 21200, train loss = 0.145865, train accuracy = 1.000000\n",
      "[2018-06-02 13:59:01.260179] Iteration 21300, train loss = 0.150687, train accuracy = 0.992188\n",
      "[2018-06-02 13:59:20.455179] Iteration 21400, train loss = 0.142623, train accuracy = 1.000000\n",
      "[2018-06-02 13:59:39.720179] Iteration 21500, train loss = 0.161068, train accuracy = 0.992188\n",
      "[2018-06-02 13:59:58.968179] Iteration 21600, train loss = 0.167665, train accuracy = 0.984375\n",
      "[2018-06-02 14:00:18.121179] Iteration 21700, train loss = 0.145729, train accuracy = 0.992188\n",
      "[2018-06-02 14:00:37.230179] Iteration 21800, train loss = 0.142667, train accuracy = 1.000000\n",
      "[2018-06-02 14:00:56.398179] Iteration 21900, train loss = 0.169196, train accuracy = 0.984375\n",
      "[2018-06-02 14:01:15.550179] Iteration 22000, train loss = 0.150348, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 14:01:38.894179] Iteration 22100, train loss = 0.175709, train accuracy = 0.984375\n",
      "[2018-06-02 14:01:58.059179] Iteration 22200, train loss = 0.142303, train accuracy = 1.000000\n",
      "[2018-06-02 14:02:17.269179] Iteration 22300, train loss = 0.141882, train accuracy = 1.000000\n",
      "[2018-06-02 14:02:36.391179] Iteration 22400, train loss = 0.143361, train accuracy = 1.000000\n",
      "[2018-06-02 14:02:55.693179] Iteration 22500, train loss = 0.137353, train accuracy = 1.000000\n",
      "[2018-06-02 14:03:14.837179] Iteration 22600, train loss = 0.139568, train accuracy = 1.000000\n",
      "[2018-06-02 14:03:33.996179] Iteration 22700, train loss = 0.137842, train accuracy = 1.000000\n",
      "[2018-06-02 14:03:53.207179] Iteration 22800, train loss = 0.141262, train accuracy = 1.000000\n",
      "[2018-06-02 14:04:12.359179] Iteration 22900, train loss = 0.148379, train accuracy = 0.992188\n",
      "[2018-06-02 14:04:31.538179] Iteration 23000, train loss = 0.183957, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.928700\n",
      "[2018-06-02 14:04:54.914179] Iteration 23100, train loss = 0.142472, train accuracy = 1.000000\n",
      "[2018-06-02 14:05:14.078179] Iteration 23200, train loss = 0.152917, train accuracy = 0.992188\n",
      "[2018-06-02 14:05:33.325179] Iteration 23300, train loss = 0.151444, train accuracy = 0.992188\n",
      "[2018-06-02 14:05:52.600179] Iteration 23400, train loss = 0.140560, train accuracy = 1.000000\n",
      "[2018-06-02 14:06:11.741179] Iteration 23500, train loss = 0.153976, train accuracy = 1.000000\n",
      "[2018-06-02 14:06:30.875179] Iteration 23600, train loss = 0.168561, train accuracy = 0.984375\n",
      "[2018-06-02 14:06:50.042179] Iteration 23700, train loss = 0.144832, train accuracy = 1.000000\n",
      "[2018-06-02 14:07:09.186179] Iteration 23800, train loss = 0.140131, train accuracy = 1.000000\n",
      "[2018-06-02 14:07:28.325179] Iteration 23900, train loss = 0.148004, train accuracy = 0.992188\n",
      "[2018-06-02 14:07:47.670179] Iteration 24000, train loss = 0.174705, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 14:08:11.223179] Iteration 24100, train loss = 0.147769, train accuracy = 1.000000\n",
      "[2018-06-02 14:08:30.437179] Iteration 24200, train loss = 0.150496, train accuracy = 1.000000\n",
      "[2018-06-02 14:08:49.896179] Iteration 24300, train loss = 0.154575, train accuracy = 1.000000\n",
      "[2018-06-02 14:09:09.314179] Iteration 24400, train loss = 0.140504, train accuracy = 1.000000\n",
      "[2018-06-02 14:09:28.462179] Iteration 24500, train loss = 0.164108, train accuracy = 0.992188\n",
      "[2018-06-02 14:09:47.633179] Iteration 24600, train loss = 0.158958, train accuracy = 0.992188\n",
      "[2018-06-02 14:10:06.919179] Iteration 24700, train loss = 0.143822, train accuracy = 1.000000\n",
      "[2018-06-02 14:10:26.097179] Iteration 24800, train loss = 0.143023, train accuracy = 1.000000\n",
      "[2018-06-02 14:10:45.280179] Iteration 24900, train loss = 0.142778, train accuracy = 1.000000\n",
      "[2018-06-02 14:11:04.504179] Iteration 25000, train loss = 0.144964, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "[2018-06-02 14:11:27.858179] Iteration 25100, train loss = 0.147527, train accuracy = 1.000000\n",
      "[2018-06-02 14:11:47.006179] Iteration 25200, train loss = 0.141060, train accuracy = 1.000000\n",
      "[2018-06-02 14:12:06.340179] Iteration 25300, train loss = 0.152098, train accuracy = 0.992188\n",
      "[2018-06-02 14:12:25.523179] Iteration 25400, train loss = 0.154677, train accuracy = 0.992188\n",
      "[2018-06-02 14:12:44.679179] Iteration 25500, train loss = 0.167914, train accuracy = 0.984375\n",
      "[2018-06-02 14:13:03.814179] Iteration 25600, train loss = 0.166007, train accuracy = 0.992188\n",
      "[2018-06-02 14:13:22.996179] Iteration 25700, train loss = 0.160836, train accuracy = 0.992188\n",
      "[2018-06-02 14:13:42.277179] Iteration 25800, train loss = 0.167988, train accuracy = 0.992188\n",
      "[2018-06-02 14:14:01.391179] Iteration 25900, train loss = 0.167834, train accuracy = 0.992188\n",
      "[2018-06-02 14:14:20.554179] Iteration 26000, train loss = 0.137453, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928900\n",
      "[2018-06-02 14:14:43.850179] Iteration 26100, train loss = 0.144390, train accuracy = 1.000000\n",
      "[2018-06-02 14:15:03.014179] Iteration 26200, train loss = 0.143540, train accuracy = 1.000000\n",
      "[2018-06-02 14:15:22.199179] Iteration 26300, train loss = 0.142489, train accuracy = 1.000000\n",
      "[2018-06-02 14:15:41.483179] Iteration 26400, train loss = 0.146726, train accuracy = 1.000000\n",
      "[2018-06-02 14:16:00.652179] Iteration 26500, train loss = 0.148756, train accuracy = 1.000000\n",
      "[2018-06-02 14:16:19.888179] Iteration 26600, train loss = 0.140141, train accuracy = 1.000000\n",
      "[2018-06-02 14:16:39.042179] Iteration 26700, train loss = 0.177987, train accuracy = 0.984375\n",
      "[2018-06-02 14:16:58.268179] Iteration 26800, train loss = 0.142958, train accuracy = 1.000000\n",
      "[2018-06-02 14:17:17.458179] Iteration 26900, train loss = 0.163086, train accuracy = 0.984375\n",
      "[2018-06-02 14:17:36.580179] Iteration 27000, train loss = 0.159081, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 14:17:59.904179] Iteration 27100, train loss = 0.153711, train accuracy = 0.992188\n",
      "[2018-06-02 14:18:19.111179] Iteration 27200, train loss = 0.145061, train accuracy = 1.000000\n",
      "[2018-06-02 14:18:38.449179] Iteration 27300, train loss = 0.150134, train accuracy = 0.992188\n",
      "[2018-06-02 14:18:57.679179] Iteration 27400, train loss = 0.146975, train accuracy = 0.992188\n",
      "[2018-06-02 14:19:16.869179] Iteration 27500, train loss = 0.141148, train accuracy = 1.000000\n",
      "[2018-06-02 14:19:36.061179] Iteration 27600, train loss = 0.140017, train accuracy = 1.000000\n",
      "[2018-06-02 14:19:55.266179] Iteration 27700, train loss = 0.141306, train accuracy = 1.000000\n",
      "[2018-06-02 14:20:14.460179] Iteration 27800, train loss = 0.145541, train accuracy = 1.000000\n",
      "[2018-06-02 14:20:33.628179] Iteration 27900, train loss = 0.152623, train accuracy = 1.000000\n",
      "[2018-06-02 14:20:52.767179] Iteration 28000, train loss = 0.160334, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 14:21:16.093179] Iteration 28100, train loss = 0.157927, train accuracy = 0.992188\n",
      "[2018-06-02 14:21:35.290179] Iteration 28200, train loss = 0.158775, train accuracy = 0.992188\n",
      "[2018-06-02 14:21:54.543179] Iteration 28300, train loss = 0.141933, train accuracy = 1.000000\n",
      "[2018-06-02 14:22:13.761179] Iteration 28400, train loss = 0.143163, train accuracy = 1.000000\n",
      "[2018-06-02 14:22:33.093179] Iteration 28500, train loss = 0.146577, train accuracy = 1.000000\n",
      "[2018-06-02 14:22:52.256179] Iteration 28600, train loss = 0.166131, train accuracy = 0.992188\n",
      "[2018-06-02 14:23:11.357179] Iteration 28700, train loss = 0.145728, train accuracy = 1.000000\n",
      "[2018-06-02 14:23:30.470179] Iteration 28800, train loss = 0.150470, train accuracy = 1.000000\n",
      "[2018-06-02 14:23:49.770179] Iteration 28900, train loss = 0.145986, train accuracy = 0.992188\n",
      "[2018-06-02 14:24:08.939179] Iteration 29000, train loss = 0.152013, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928900\n",
      "[2018-06-02 14:24:32.626179] Iteration 29100, train loss = 0.143220, train accuracy = 1.000000\n",
      "[2018-06-02 14:24:51.973179] Iteration 29200, train loss = 0.162504, train accuracy = 0.984375\n",
      "[2018-06-02 14:25:11.157179] Iteration 29300, train loss = 0.147373, train accuracy = 1.000000\n",
      "[2018-06-02 14:25:30.311179] Iteration 29400, train loss = 0.138737, train accuracy = 1.000000\n",
      "[2018-06-02 14:25:49.554179] Iteration 29500, train loss = 0.151029, train accuracy = 0.992188\n",
      "[2018-06-02 14:26:08.687179] Iteration 29600, train loss = 0.147360, train accuracy = 1.000000\n",
      "[2018-06-02 14:26:27.843179] Iteration 29700, train loss = 0.142931, train accuracy = 1.000000\n",
      "[2018-06-02 14:26:46.958179] Iteration 29800, train loss = 0.151712, train accuracy = 0.992188\n",
      "[2018-06-02 14:27:06.140179] Iteration 29900, train loss = 0.144335, train accuracy = 0.992188\n",
      "[2018-06-02 14:27:25.416179] Iteration 30000, train loss = 0.156478, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 14:27:48.784179] Iteration 30100, train loss = 0.145649, train accuracy = 1.000000\n",
      "[2018-06-02 14:28:07.915179] Iteration 30200, train loss = 0.141368, train accuracy = 1.000000\n",
      "[2018-06-02 14:28:27.150179] Iteration 30300, train loss = 0.151194, train accuracy = 1.000000\n",
      "[2018-06-02 14:28:46.276179] Iteration 30400, train loss = 0.159522, train accuracy = 0.992188\n",
      "[2018-06-02 14:29:05.611179] Iteration 30500, train loss = 0.150614, train accuracy = 0.992188\n",
      "[2018-06-02 14:29:24.800179] Iteration 30600, train loss = 0.154960, train accuracy = 0.992188\n",
      "[2018-06-02 14:29:43.526179] Iteration 30700, train loss = 0.150555, train accuracy = 1.000000\n",
      "[2018-06-02 14:30:02.927179] Iteration 30800, train loss = 0.142858, train accuracy = 1.000000\n",
      "[2018-06-02 14:30:22.284179] Iteration 30900, train loss = 0.178861, train accuracy = 0.992188\n",
      "[2018-06-02 14:30:41.672179] Iteration 31000, train loss = 0.162647, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 14:31:05.294179] Iteration 31100, train loss = 0.152975, train accuracy = 0.992188\n",
      "[2018-06-02 14:31:24.713179] Iteration 31200, train loss = 0.162230, train accuracy = 0.992188\n",
      "[2018-06-02 14:31:43.181179] Iteration 31300, train loss = 0.140347, train accuracy = 1.000000\n",
      "[2018-06-02 14:32:02.392179] Iteration 31400, train loss = 0.163579, train accuracy = 0.992188\n",
      "[2018-06-02 14:32:21.776179] Iteration 31500, train loss = 0.158814, train accuracy = 0.992188\n",
      "[2018-06-02 14:32:41.172179] Iteration 31600, train loss = 0.147681, train accuracy = 1.000000\n",
      "[2018-06-02 14:33:00.525179] Iteration 31700, train loss = 0.157368, train accuracy = 0.984375\n",
      "[2018-06-02 14:33:20.066179] Iteration 31800, train loss = 0.155637, train accuracy = 0.992188\n",
      "[2018-06-02 14:33:39.680179] Iteration 31900, train loss = 0.149133, train accuracy = 0.992188\n",
      "[2018-06-02 14:33:59.127179] Iteration 32000, train loss = 0.157610, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928100\n",
      "[2018-06-02 14:34:23.044179] Iteration 32100, train loss = 0.143678, train accuracy = 1.000000\n",
      "[2018-06-02 14:34:42.580179] Iteration 32200, train loss = 0.142195, train accuracy = 1.000000\n",
      "[2018-06-02 14:35:01.983179] Iteration 32300, train loss = 0.143015, train accuracy = 1.000000\n",
      "[2018-06-02 14:35:21.499179] Iteration 32400, train loss = 0.152247, train accuracy = 1.000000\n",
      "[2018-06-02 14:35:41.010179] Iteration 32500, train loss = 0.137701, train accuracy = 1.000000\n",
      "[2018-06-02 14:36:00.469179] Iteration 32600, train loss = 0.146539, train accuracy = 1.000000\n",
      "[2018-06-02 14:36:19.941179] Iteration 32700, train loss = 0.137092, train accuracy = 1.000000\n",
      "[2018-06-02 14:36:39.494179] Iteration 32800, train loss = 0.151176, train accuracy = 0.992188\n",
      "[2018-06-02 14:36:58.958179] Iteration 32900, train loss = 0.144323, train accuracy = 1.000000\n",
      "[2018-06-02 14:37:18.568179] Iteration 33000, train loss = 0.164758, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 14:37:42.373179] Iteration 33100, train loss = 0.164703, train accuracy = 0.976562\n",
      "[2018-06-02 14:38:01.838179] Iteration 33200, train loss = 0.168364, train accuracy = 0.992188\n",
      "[2018-06-02 14:38:21.416179] Iteration 33300, train loss = 0.168420, train accuracy = 0.992188\n",
      "[2018-06-02 14:38:40.951179] Iteration 33400, train loss = 0.145293, train accuracy = 1.000000\n",
      "[2018-06-02 14:39:00.490179] Iteration 33500, train loss = 0.151737, train accuracy = 0.992188\n",
      "[2018-06-02 14:39:19.980179] Iteration 33600, train loss = 0.179168, train accuracy = 0.984375\n",
      "[2018-06-02 14:39:39.440179] Iteration 33700, train loss = 0.142300, train accuracy = 1.000000\n",
      "[2018-06-02 14:39:58.994179] Iteration 33800, train loss = 0.160624, train accuracy = 1.000000\n",
      "[2018-06-02 14:40:18.578179] Iteration 33900, train loss = 0.145816, train accuracy = 1.000000\n",
      "[2018-06-02 14:40:38.227179] Iteration 34000, train loss = 0.141726, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 14:41:02.025179] Iteration 34100, train loss = 0.138380, train accuracy = 1.000000\n",
      "[2018-06-02 14:41:21.469179] Iteration 34200, train loss = 0.147403, train accuracy = 0.992188\n",
      "[2018-06-02 14:41:40.953179] Iteration 34300, train loss = 0.152174, train accuracy = 0.992188\n",
      "[2018-06-02 14:42:00.496179] Iteration 34400, train loss = 0.144329, train accuracy = 1.000000\n",
      "[2018-06-02 14:42:20.100179] Iteration 34500, train loss = 0.138343, train accuracy = 1.000000\n",
      "[2018-06-02 14:42:39.739179] Iteration 34600, train loss = 0.153415, train accuracy = 0.992188\n",
      "[2018-06-02 14:42:59.084179] Iteration 34700, train loss = 0.140872, train accuracy = 1.000000\n",
      "[2018-06-02 14:43:18.577179] Iteration 34800, train loss = 0.160864, train accuracy = 0.992188\n",
      "[2018-06-02 14:43:38.016179] Iteration 34900, train loss = 0.152559, train accuracy = 0.992188\n",
      "[2018-06-02 14:43:57.338179] Iteration 35000, train loss = 0.145250, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 14:44:20.887179] Iteration 35100, train loss = 0.173695, train accuracy = 0.984375\n",
      "[2018-06-02 14:44:40.258179] Iteration 35200, train loss = 0.137771, train accuracy = 1.000000\n",
      "[2018-06-02 14:44:59.573179] Iteration 35300, train loss = 0.150868, train accuracy = 1.000000\n",
      "[2018-06-02 14:45:19.055179] Iteration 35400, train loss = 0.142688, train accuracy = 1.000000\n",
      "[2018-06-02 14:45:38.557179] Iteration 35500, train loss = 0.139843, train accuracy = 1.000000\n",
      "[2018-06-02 14:45:57.921179] Iteration 35600, train loss = 0.147615, train accuracy = 1.000000\n",
      "[2018-06-02 14:46:17.378179] Iteration 35700, train loss = 0.168895, train accuracy = 0.992188\n",
      "[2018-06-02 14:46:36.737179] Iteration 35800, train loss = 0.169287, train accuracy = 0.984375\n",
      "[2018-06-02 14:46:56.222179] Iteration 35900, train loss = 0.143220, train accuracy = 1.000000\n",
      "[2018-06-02 14:47:15.644179] Iteration 36000, train loss = 0.141914, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 14:47:39.147179] Iteration 36100, train loss = 0.144199, train accuracy = 1.000000\n",
      "[2018-06-02 14:47:58.505179] Iteration 36200, train loss = 0.161014, train accuracy = 0.984375\n",
      "[2018-06-02 14:48:18.054179] Iteration 36300, train loss = 0.153014, train accuracy = 0.984375\n",
      "[2018-06-02 14:48:37.414179] Iteration 36400, train loss = 0.144039, train accuracy = 1.000000\n",
      "[2018-06-02 14:48:56.752179] Iteration 36500, train loss = 0.150881, train accuracy = 0.992188\n",
      "[2018-06-02 14:49:16.200179] Iteration 36600, train loss = 0.172113, train accuracy = 0.992188\n",
      "[2018-06-02 14:49:35.475179] Iteration 36700, train loss = 0.137896, train accuracy = 1.000000\n",
      "[2018-06-02 14:49:54.888179] Iteration 36800, train loss = 0.138000, train accuracy = 1.000000\n",
      "[2018-06-02 14:50:14.248179] Iteration 36900, train loss = 0.153757, train accuracy = 0.992188\n",
      "[2018-06-02 14:50:33.589179] Iteration 37000, train loss = 0.147344, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928500\n",
      "[2018-06-02 14:50:57.195179] Iteration 37100, train loss = 0.146603, train accuracy = 1.000000\n",
      "[2018-06-02 14:51:16.546179] Iteration 37200, train loss = 0.151807, train accuracy = 1.000000\n",
      "[2018-06-02 14:51:35.957179] Iteration 37300, train loss = 0.142600, train accuracy = 1.000000\n",
      "[2018-06-02 14:51:55.302179] Iteration 37400, train loss = 0.155455, train accuracy = 0.992188\n",
      "[2018-06-02 14:52:14.629179] Iteration 37500, train loss = 0.139485, train accuracy = 1.000000\n",
      "[2018-06-02 14:52:34.011179] Iteration 37600, train loss = 0.162379, train accuracy = 0.984375\n",
      "[2018-06-02 14:52:53.285179] Iteration 37700, train loss = 0.171311, train accuracy = 0.992188\n",
      "[2018-06-02 14:53:12.607179] Iteration 37800, train loss = 0.145124, train accuracy = 1.000000\n",
      "[2018-06-02 14:53:31.952179] Iteration 37900, train loss = 0.147630, train accuracy = 0.992188\n",
      "[2018-06-02 14:53:51.332179] Iteration 38000, train loss = 0.159979, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 14:54:14.861179] Iteration 38100, train loss = 0.153558, train accuracy = 0.992188\n",
      "[2018-06-02 14:54:34.158179] Iteration 38200, train loss = 0.141486, train accuracy = 1.000000\n",
      "[2018-06-02 14:54:53.499179] Iteration 38300, train loss = 0.160203, train accuracy = 0.984375\n",
      "[2018-06-02 14:55:12.890179] Iteration 38400, train loss = 0.171555, train accuracy = 0.992188\n",
      "[2018-06-02 14:55:32.246179] Iteration 38500, train loss = 0.144887, train accuracy = 1.000000\n",
      "[2018-06-02 14:55:51.687179] Iteration 38600, train loss = 0.150483, train accuracy = 0.992188\n",
      "[2018-06-02 14:56:10.965179] Iteration 38700, train loss = 0.175588, train accuracy = 0.984375\n",
      "[2018-06-02 14:56:30.412179] Iteration 38800, train loss = 0.150092, train accuracy = 1.000000\n",
      "[2018-06-02 14:56:49.747179] Iteration 38900, train loss = 0.144083, train accuracy = 1.000000\n",
      "[2018-06-02 14:57:09.045179] Iteration 39000, train loss = 0.145744, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 14:57:32.504179] Iteration 39100, train loss = 0.139119, train accuracy = 1.000000\n",
      "[2018-06-02 14:57:51.853179] Iteration 39200, train loss = 0.149815, train accuracy = 0.992188\n",
      "[2018-06-02 14:58:11.236179] Iteration 39300, train loss = 0.150816, train accuracy = 0.992188\n",
      "[2018-06-02 14:58:30.612179] Iteration 39400, train loss = 0.140719, train accuracy = 1.000000\n",
      "[2018-06-02 14:58:49.922179] Iteration 39500, train loss = 0.140489, train accuracy = 1.000000\n",
      "[2018-06-02 14:59:09.365179] Iteration 39600, train loss = 0.149323, train accuracy = 0.992188\n",
      "[2018-06-02 14:59:28.640179] Iteration 39700, train loss = 0.139152, train accuracy = 1.000000\n",
      "[2018-06-02 14:59:48.030179] Iteration 39800, train loss = 0.153351, train accuracy = 0.992188\n",
      "[2018-06-02 15:00:07.434179] Iteration 39900, train loss = 0.142511, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三轮  量化\n",
    "prune_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.927700\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.03419316  0.125      -0.03125    -0.0625\n",
      "  0.0039864   0.00145402  0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.85)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 15:05:24.276179] Iteration 100, train loss = 0.146051, train accuracy = 1.000000\n",
      "[2018-06-02 15:05:43.642179] Iteration 200, train loss = 0.139474, train accuracy = 1.000000\n",
      "[2018-06-02 15:06:02.993179] Iteration 300, train loss = 0.142894, train accuracy = 1.000000\n",
      "[2018-06-02 15:06:22.245179] Iteration 400, train loss = 0.166317, train accuracy = 0.984375\n",
      "[2018-06-02 15:06:41.521179] Iteration 500, train loss = 0.171874, train accuracy = 0.992188\n",
      "[2018-06-02 15:07:00.767179] Iteration 600, train loss = 0.154657, train accuracy = 1.000000\n",
      "[2018-06-02 15:07:20.035179] Iteration 700, train loss = 0.150754, train accuracy = 1.000000\n",
      "[2018-06-02 15:07:39.437179] Iteration 800, train loss = 0.144614, train accuracy = 1.000000\n",
      "[2018-06-02 15:07:58.654179] Iteration 900, train loss = 0.166569, train accuracy = 0.992188\n",
      "[2018-06-02 15:08:17.905179] Iteration 1000, train loss = 0.141870, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926500\n",
      "[2018-06-02 15:08:41.473179] Iteration 1100, train loss = 0.150321, train accuracy = 0.992188\n",
      "[2018-06-02 15:09:00.825179] Iteration 1200, train loss = 0.154214, train accuracy = 0.992188\n",
      "[2018-06-02 15:09:20.161179] Iteration 1300, train loss = 0.152672, train accuracy = 1.000000\n",
      "[2018-06-02 15:09:39.528179] Iteration 1400, train loss = 0.156190, train accuracy = 1.000000\n",
      "[2018-06-02 15:09:58.816179] Iteration 1500, train loss = 0.149992, train accuracy = 0.992188\n",
      "[2018-06-02 15:10:18.227179] Iteration 1600, train loss = 0.159697, train accuracy = 0.992188\n",
      "[2018-06-02 15:10:37.504179] Iteration 1700, train loss = 0.141062, train accuracy = 1.000000\n",
      "[2018-06-02 15:10:56.850179] Iteration 1800, train loss = 0.140618, train accuracy = 1.000000\n",
      "[2018-06-02 15:11:16.220179] Iteration 1900, train loss = 0.158708, train accuracy = 0.992188\n",
      "[2018-06-02 15:11:35.628179] Iteration 2000, train loss = 0.158328, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927300\n",
      "[2018-06-02 15:11:59.189179] Iteration 2100, train loss = 0.161913, train accuracy = 0.992188\n",
      "[2018-06-02 15:12:18.473179] Iteration 2200, train loss = 0.166241, train accuracy = 0.984375\n",
      "[2018-06-02 15:12:37.793179] Iteration 2300, train loss = 0.139176, train accuracy = 1.000000\n",
      "[2018-06-02 15:12:57.066179] Iteration 2400, train loss = 0.146208, train accuracy = 1.000000\n",
      "[2018-06-02 15:13:16.348179] Iteration 2500, train loss = 0.154327, train accuracy = 0.992188\n",
      "[2018-06-02 15:13:35.670179] Iteration 2600, train loss = 0.142122, train accuracy = 1.000000\n",
      "[2018-06-02 15:13:55.073179] Iteration 2700, train loss = 0.165431, train accuracy = 0.992188\n",
      "[2018-06-02 15:14:14.337179] Iteration 2800, train loss = 0.165997, train accuracy = 0.984375\n",
      "[2018-06-02 15:14:33.679179] Iteration 2900, train loss = 0.144134, train accuracy = 1.000000\n",
      "[2018-06-02 15:14:53.005179] Iteration 3000, train loss = 0.149057, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 15:15:16.596179] Iteration 3100, train loss = 0.147334, train accuracy = 1.000000\n",
      "[2018-06-02 15:15:35.914179] Iteration 3200, train loss = 0.160847, train accuracy = 0.984375\n",
      "[2018-06-02 15:15:55.211179] Iteration 3300, train loss = 0.152163, train accuracy = 1.000000\n",
      "[2018-06-02 15:16:14.555179] Iteration 3400, train loss = 0.151890, train accuracy = 0.992188\n",
      "[2018-06-02 15:16:33.947179] Iteration 3500, train loss = 0.142627, train accuracy = 1.000000\n",
      "[2018-06-02 15:16:53.315179] Iteration 3600, train loss = 0.146421, train accuracy = 1.000000\n",
      "[2018-06-02 15:17:12.704179] Iteration 3700, train loss = 0.150520, train accuracy = 1.000000\n",
      "[2018-06-02 15:17:32.154179] Iteration 3800, train loss = 0.152479, train accuracy = 0.992188\n",
      "[2018-06-02 15:17:51.438179] Iteration 3900, train loss = 0.142923, train accuracy = 1.000000\n",
      "[2018-06-02 15:18:10.829179] Iteration 4000, train loss = 0.144284, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 15:18:34.392179] Iteration 4100, train loss = 0.145342, train accuracy = 1.000000\n",
      "[2018-06-02 15:18:53.787179] Iteration 4200, train loss = 0.160157, train accuracy = 1.000000\n",
      "[2018-06-02 15:19:13.169179] Iteration 4300, train loss = 0.173041, train accuracy = 0.984375\n",
      "[2018-06-02 15:19:32.544179] Iteration 4400, train loss = 0.186239, train accuracy = 0.976562\n",
      "[2018-06-02 15:19:51.792179] Iteration 4500, train loss = 0.146683, train accuracy = 1.000000\n",
      "[2018-06-02 15:20:11.214179] Iteration 4600, train loss = 0.151268, train accuracy = 0.992188\n",
      "[2018-06-02 15:20:30.502179] Iteration 4700, train loss = 0.154958, train accuracy = 0.992188\n",
      "[2018-06-02 15:20:49.821179] Iteration 4800, train loss = 0.167330, train accuracy = 0.992188\n",
      "[2018-06-02 15:21:09.211179] Iteration 4900, train loss = 0.138881, train accuracy = 1.000000\n",
      "[2018-06-02 15:21:28.538179] Iteration 5000, train loss = 0.140563, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927400\n",
      "[2018-06-02 15:21:52.121179] Iteration 5100, train loss = 0.141428, train accuracy = 1.000000\n",
      "[2018-06-02 15:22:11.389179] Iteration 5200, train loss = 0.149144, train accuracy = 1.000000\n",
      "[2018-06-02 15:22:30.649179] Iteration 5300, train loss = 0.152449, train accuracy = 0.992188\n",
      "[2018-06-02 15:22:50.066179] Iteration 5400, train loss = 0.146549, train accuracy = 1.000000\n",
      "[2018-06-02 15:23:09.420179] Iteration 5500, train loss = 0.145889, train accuracy = 1.000000\n",
      "[2018-06-02 15:23:28.714179] Iteration 5600, train loss = 0.141950, train accuracy = 1.000000\n",
      "[2018-06-02 15:23:48.035179] Iteration 5700, train loss = 0.166849, train accuracy = 0.992188\n",
      "[2018-06-02 15:24:07.307179] Iteration 5800, train loss = 0.143228, train accuracy = 1.000000\n",
      "[2018-06-02 15:24:26.575179] Iteration 5900, train loss = 0.157876, train accuracy = 1.000000\n",
      "[2018-06-02 15:24:46.051179] Iteration 6000, train loss = 0.153957, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927000\n",
      "[2018-06-02 15:25:09.574179] Iteration 6100, train loss = 0.156800, train accuracy = 0.992188\n",
      "[2018-06-02 15:25:28.864179] Iteration 6200, train loss = 0.148670, train accuracy = 1.000000\n",
      "[2018-06-02 15:25:48.145179] Iteration 6300, train loss = 0.140210, train accuracy = 1.000000\n",
      "[2018-06-02 15:26:07.464179] Iteration 6400, train loss = 0.146457, train accuracy = 1.000000\n",
      "[2018-06-02 15:26:26.849179] Iteration 6500, train loss = 0.143322, train accuracy = 1.000000\n",
      "[2018-06-02 15:26:46.158179] Iteration 6600, train loss = 0.141136, train accuracy = 1.000000\n",
      "[2018-06-02 15:27:05.399179] Iteration 6700, train loss = 0.140783, train accuracy = 1.000000\n",
      "[2018-06-02 15:27:24.671179] Iteration 6800, train loss = 0.139365, train accuracy = 1.000000\n",
      "[2018-06-02 15:27:43.965179] Iteration 6900, train loss = 0.146647, train accuracy = 0.992188\n",
      "[2018-06-02 15:28:03.277179] Iteration 7000, train loss = 0.139709, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 15:28:26.770179] Iteration 7100, train loss = 0.156348, train accuracy = 0.992188\n",
      "[2018-06-02 15:28:46.058179] Iteration 7200, train loss = 0.158968, train accuracy = 0.984375\n",
      "[2018-06-02 15:29:05.431179] Iteration 7300, train loss = 0.161976, train accuracy = 0.992188\n",
      "[2018-06-02 15:29:24.722179] Iteration 7400, train loss = 0.146249, train accuracy = 1.000000\n",
      "[2018-06-02 15:29:43.986179] Iteration 7500, train loss = 0.136758, train accuracy = 1.000000\n",
      "[2018-06-02 15:30:03.289179] Iteration 7600, train loss = 0.142552, train accuracy = 1.000000\n",
      "[2018-06-02 15:30:22.661179] Iteration 7700, train loss = 0.147054, train accuracy = 1.000000\n",
      "[2018-06-02 15:30:41.896179] Iteration 7800, train loss = 0.150056, train accuracy = 1.000000\n",
      "[2018-06-02 15:31:01.138179] Iteration 7900, train loss = 0.142975, train accuracy = 1.000000\n",
      "[2018-06-02 15:31:20.403179] Iteration 8000, train loss = 0.144287, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 15:31:43.903179] Iteration 8100, train loss = 0.159017, train accuracy = 0.992188\n",
      "[2018-06-02 15:32:03.165179] Iteration 8200, train loss = 0.147243, train accuracy = 1.000000\n",
      "[2018-06-02 15:32:22.445179] Iteration 8300, train loss = 0.153152, train accuracy = 0.992188\n",
      "[2018-06-02 15:32:41.735179] Iteration 8400, train loss = 0.162518, train accuracy = 0.992188\n",
      "[2018-06-02 15:33:00.994179] Iteration 8500, train loss = 0.169418, train accuracy = 1.000000\n",
      "[2018-06-02 15:33:20.291179] Iteration 8600, train loss = 0.144097, train accuracy = 1.000000\n",
      "[2018-06-02 15:33:39.584179] Iteration 8700, train loss = 0.141008, train accuracy = 1.000000\n",
      "[2018-06-02 15:33:58.799179] Iteration 8800, train loss = 0.140414, train accuracy = 1.000000\n",
      "[2018-06-02 15:34:18.111179] Iteration 8900, train loss = 0.138981, train accuracy = 1.000000\n",
      "[2018-06-02 15:34:37.427179] Iteration 9000, train loss = 0.151252, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927300\n",
      "[2018-06-02 15:35:00.946179] Iteration 9100, train loss = 0.144137, train accuracy = 1.000000\n",
      "[2018-06-02 15:35:20.192179] Iteration 9200, train loss = 0.148492, train accuracy = 0.992188\n",
      "[2018-06-02 15:35:39.461179] Iteration 9300, train loss = 0.153237, train accuracy = 1.000000\n",
      "[2018-06-02 15:35:58.735179] Iteration 9400, train loss = 0.138549, train accuracy = 1.000000\n",
      "[2018-06-02 15:36:18.088179] Iteration 9500, train loss = 0.163316, train accuracy = 0.984375\n",
      "[2018-06-02 15:36:37.382179] Iteration 9600, train loss = 0.144879, train accuracy = 1.000000\n",
      "[2018-06-02 15:36:56.591179] Iteration 9700, train loss = 0.146000, train accuracy = 1.000000\n",
      "[2018-06-02 15:37:15.854179] Iteration 9800, train loss = 0.154900, train accuracy = 0.992188\n",
      "[2018-06-02 15:37:35.127179] Iteration 9900, train loss = 0.150975, train accuracy = 1.000000\n",
      "[2018-06-02 15:37:54.306179] Iteration 10000, train loss = 0.144127, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 15:38:17.788179] Iteration 10100, train loss = 0.137182, train accuracy = 1.000000\n",
      "[2018-06-02 15:38:37.072179] Iteration 10200, train loss = 0.142047, train accuracy = 1.000000\n",
      "[2018-06-02 15:38:56.346179] Iteration 10300, train loss = 0.155113, train accuracy = 1.000000\n",
      "[2018-06-02 15:39:15.676179] Iteration 10400, train loss = 0.155263, train accuracy = 0.992188\n",
      "[2018-06-02 15:39:35.001179] Iteration 10500, train loss = 0.142690, train accuracy = 1.000000\n",
      "[2018-06-02 15:39:54.233179] Iteration 10600, train loss = 0.159766, train accuracy = 0.984375\n",
      "[2018-06-02 15:40:13.512179] Iteration 10700, train loss = 0.150896, train accuracy = 1.000000\n",
      "[2018-06-02 15:40:32.798179] Iteration 10800, train loss = 0.173743, train accuracy = 0.984375\n",
      "[2018-06-02 15:40:51.999179] Iteration 10900, train loss = 0.149783, train accuracy = 0.992188\n",
      "[2018-06-02 15:41:11.222179] Iteration 11000, train loss = 0.150857, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927600\n",
      "[2018-06-02 15:41:34.682179] Iteration 11100, train loss = 0.144247, train accuracy = 1.000000\n",
      "[2018-06-02 15:41:53.906179] Iteration 11200, train loss = 0.170544, train accuracy = 0.984375\n",
      "[2018-06-02 15:42:13.309179] Iteration 11300, train loss = 0.140432, train accuracy = 1.000000\n",
      "[2018-06-02 15:42:32.808179] Iteration 11400, train loss = 0.146910, train accuracy = 0.992188\n",
      "[2018-06-02 15:42:52.145179] Iteration 11500, train loss = 0.146975, train accuracy = 1.000000\n",
      "[2018-06-02 15:43:11.506179] Iteration 11600, train loss = 0.146984, train accuracy = 1.000000\n",
      "[2018-06-02 15:43:30.748179] Iteration 11700, train loss = 0.154681, train accuracy = 0.992188\n",
      "[2018-06-02 15:43:50.128179] Iteration 11800, train loss = 0.154754, train accuracy = 1.000000\n",
      "[2018-06-02 15:44:09.561179] Iteration 11900, train loss = 0.147989, train accuracy = 1.000000\n",
      "[2018-06-02 15:44:28.911179] Iteration 12000, train loss = 0.155974, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 15:44:52.353179] Iteration 12100, train loss = 0.151821, train accuracy = 0.984375\n",
      "[2018-06-02 15:45:11.753179] Iteration 12200, train loss = 0.149991, train accuracy = 0.992188\n",
      "[2018-06-02 15:45:30.955179] Iteration 12300, train loss = 0.145571, train accuracy = 1.000000\n",
      "[2018-06-02 15:45:50.232179] Iteration 12400, train loss = 0.139678, train accuracy = 1.000000\n",
      "[2018-06-02 15:46:09.501179] Iteration 12500, train loss = 0.140060, train accuracy = 1.000000\n",
      "[2018-06-02 15:46:28.812179] Iteration 12600, train loss = 0.139604, train accuracy = 1.000000\n",
      "[2018-06-02 15:46:48.062179] Iteration 12700, train loss = 0.149879, train accuracy = 1.000000\n",
      "[2018-06-02 15:47:07.377179] Iteration 12800, train loss = 0.143831, train accuracy = 1.000000\n",
      "[2018-06-02 15:47:26.708179] Iteration 12900, train loss = 0.141475, train accuracy = 1.000000\n",
      "[2018-06-02 15:47:45.964179] Iteration 13000, train loss = 0.154606, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928100\n",
      "[2018-06-02 15:48:09.487179] Iteration 13100, train loss = 0.145442, train accuracy = 0.992188\n",
      "[2018-06-02 15:48:28.796179] Iteration 13200, train loss = 0.172268, train accuracy = 0.992188\n",
      "[2018-06-02 15:48:48.150179] Iteration 13300, train loss = 0.153410, train accuracy = 0.992188\n",
      "[2018-06-02 15:49:07.478179] Iteration 13400, train loss = 0.150286, train accuracy = 0.992188\n",
      "[2018-06-02 15:49:26.808179] Iteration 13500, train loss = 0.141850, train accuracy = 1.000000\n",
      "[2018-06-02 15:49:46.119179] Iteration 13600, train loss = 0.150856, train accuracy = 0.992188\n",
      "[2018-06-02 15:50:05.419179] Iteration 13700, train loss = 0.141094, train accuracy = 1.000000\n",
      "[2018-06-02 15:50:24.744179] Iteration 13800, train loss = 0.150693, train accuracy = 0.992188\n",
      "[2018-06-02 15:50:44.063179] Iteration 13900, train loss = 0.143824, train accuracy = 1.000000\n",
      "[2018-06-02 15:51:03.461179] Iteration 14000, train loss = 0.148994, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 15:51:27.090179] Iteration 14100, train loss = 0.138104, train accuracy = 1.000000\n",
      "[2018-06-02 15:51:46.440179] Iteration 14200, train loss = 0.138611, train accuracy = 1.000000\n",
      "[2018-06-02 15:52:05.710179] Iteration 14300, train loss = 0.175401, train accuracy = 0.984375\n",
      "[2018-06-02 15:52:25.032179] Iteration 14400, train loss = 0.164263, train accuracy = 0.984375\n",
      "[2018-06-02 15:52:44.386179] Iteration 14500, train loss = 0.165162, train accuracy = 0.976562\n",
      "[2018-06-02 15:53:03.773179] Iteration 14600, train loss = 0.157399, train accuracy = 0.992188\n",
      "[2018-06-02 15:53:23.141179] Iteration 14700, train loss = 0.152640, train accuracy = 1.000000\n",
      "[2018-06-02 15:53:42.506179] Iteration 14800, train loss = 0.152463, train accuracy = 0.992188\n",
      "[2018-06-02 15:54:02.035179] Iteration 14900, train loss = 0.140819, train accuracy = 1.000000\n",
      "[2018-06-02 15:54:21.314179] Iteration 15000, train loss = 0.174417, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.927800\n",
      "[2018-06-02 15:54:44.803179] Iteration 15100, train loss = 0.145664, train accuracy = 1.000000\n",
      "[2018-06-02 15:55:04.114179] Iteration 15200, train loss = 0.139045, train accuracy = 1.000000\n",
      "[2018-06-02 15:55:23.423179] Iteration 15300, train loss = 0.138975, train accuracy = 1.000000\n",
      "[2018-06-02 15:55:42.652179] Iteration 15400, train loss = 0.139413, train accuracy = 1.000000\n",
      "[2018-06-02 15:56:01.907179] Iteration 15500, train loss = 0.151133, train accuracy = 0.992188\n",
      "[2018-06-02 15:56:21.228179] Iteration 15600, train loss = 0.143555, train accuracy = 1.000000\n",
      "[2018-06-02 15:56:40.564179] Iteration 15700, train loss = 0.155174, train accuracy = 0.992188\n",
      "[2018-06-02 15:56:59.894179] Iteration 15800, train loss = 0.154359, train accuracy = 0.992188\n",
      "[2018-06-02 15:57:19.130179] Iteration 15900, train loss = 0.141090, train accuracy = 1.000000\n",
      "[2018-06-02 15:57:38.479179] Iteration 16000, train loss = 0.146780, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 15:58:02.047179] Iteration 16100, train loss = 0.148950, train accuracy = 1.000000\n",
      "[2018-06-02 15:58:21.310179] Iteration 16200, train loss = 0.143930, train accuracy = 1.000000\n",
      "[2018-06-02 15:58:40.698179] Iteration 16300, train loss = 0.156324, train accuracy = 0.992188\n",
      "[2018-06-02 15:58:59.951179] Iteration 16400, train loss = 0.141808, train accuracy = 1.000000\n",
      "[2018-06-02 15:59:19.212179] Iteration 16500, train loss = 0.143630, train accuracy = 1.000000\n",
      "[2018-06-02 15:59:38.495179] Iteration 16600, train loss = 0.140924, train accuracy = 1.000000\n",
      "[2018-06-02 15:59:57.774179] Iteration 16700, train loss = 0.148492, train accuracy = 1.000000\n",
      "[2018-06-02 16:00:17.021179] Iteration 16800, train loss = 0.156512, train accuracy = 0.992188\n",
      "[2018-06-02 16:00:36.318179] Iteration 16900, train loss = 0.158541, train accuracy = 0.992188\n",
      "[2018-06-02 16:00:55.668179] Iteration 17000, train loss = 0.154630, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 16:01:19.294179] Iteration 17100, train loss = 0.159929, train accuracy = 0.992188\n",
      "[2018-06-02 16:01:38.613179] Iteration 17200, train loss = 0.142230, train accuracy = 1.000000\n",
      "[2018-06-02 16:01:57.979179] Iteration 17300, train loss = 0.165715, train accuracy = 0.992188\n",
      "[2018-06-02 16:02:17.289179] Iteration 17400, train loss = 0.146147, train accuracy = 1.000000\n",
      "[2018-06-02 16:02:36.608179] Iteration 17500, train loss = 0.140973, train accuracy = 1.000000\n",
      "[2018-06-02 16:02:55.792179] Iteration 17600, train loss = 0.187782, train accuracy = 0.976562\n",
      "[2018-06-02 16:03:15.068179] Iteration 17700, train loss = 0.150366, train accuracy = 1.000000\n",
      "[2018-06-02 16:03:34.414179] Iteration 17800, train loss = 0.139267, train accuracy = 1.000000\n",
      "[2018-06-02 16:03:53.686179] Iteration 17900, train loss = 0.142168, train accuracy = 1.000000\n",
      "[2018-06-02 16:04:13.055179] Iteration 18000, train loss = 0.155485, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 16:04:36.485179] Iteration 18100, train loss = 0.146540, train accuracy = 0.992188\n",
      "[2018-06-02 16:04:55.699179] Iteration 18200, train loss = 0.142751, train accuracy = 1.000000\n",
      "[2018-06-02 16:05:15.037179] Iteration 18300, train loss = 0.142910, train accuracy = 0.992188\n",
      "[2018-06-02 16:05:34.393179] Iteration 18400, train loss = 0.144005, train accuracy = 1.000000\n",
      "[2018-06-02 16:05:53.706179] Iteration 18500, train loss = 0.145792, train accuracy = 0.992188\n",
      "[2018-06-02 16:06:12.933179] Iteration 18600, train loss = 0.139708, train accuracy = 1.000000\n",
      "[2018-06-02 16:06:32.243179] Iteration 18700, train loss = 0.160469, train accuracy = 0.992188\n",
      "[2018-06-02 16:06:51.543179] Iteration 18800, train loss = 0.140027, train accuracy = 1.000000\n",
      "[2018-06-02 16:07:10.766179] Iteration 18900, train loss = 0.146401, train accuracy = 1.000000\n",
      "[2018-06-02 16:07:30.084179] Iteration 19000, train loss = 0.143056, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 16:07:53.540179] Iteration 19100, train loss = 0.141849, train accuracy = 1.000000\n",
      "[2018-06-02 16:08:12.856179] Iteration 19200, train loss = 0.153107, train accuracy = 0.992188\n",
      "[2018-06-02 16:08:32.159179] Iteration 19300, train loss = 0.160019, train accuracy = 0.984375\n",
      "[2018-06-02 16:08:51.438179] Iteration 19400, train loss = 0.143520, train accuracy = 1.000000\n",
      "[2018-06-02 16:09:10.785179] Iteration 19500, train loss = 0.144316, train accuracy = 1.000000\n",
      "[2018-06-02 16:09:30.049179] Iteration 19600, train loss = 0.145355, train accuracy = 1.000000\n",
      "[2018-06-02 16:09:49.266179] Iteration 19700, train loss = 0.148235, train accuracy = 1.000000\n",
      "[2018-06-02 16:10:08.495179] Iteration 19800, train loss = 0.160454, train accuracy = 0.992188\n",
      "[2018-06-02 16:10:27.798179] Iteration 19900, train loss = 0.138333, train accuracy = 1.000000\n",
      "[2018-06-02 16:10:47.200179] Iteration 20000, train loss = 0.150389, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 16:11:10.799179] Iteration 20100, train loss = 0.165391, train accuracy = 0.992188\n",
      "[2018-06-02 16:11:30.166179] Iteration 20200, train loss = 0.143293, train accuracy = 1.000000\n",
      "[2018-06-02 16:11:49.573179] Iteration 20300, train loss = 0.149667, train accuracy = 0.992188\n",
      "[2018-06-02 16:12:08.927179] Iteration 20400, train loss = 0.154887, train accuracy = 1.000000\n",
      "[2018-06-02 16:12:28.233179] Iteration 20500, train loss = 0.149931, train accuracy = 1.000000\n",
      "[2018-06-02 16:12:47.623179] Iteration 20600, train loss = 0.152181, train accuracy = 0.992188\n",
      "[2018-06-02 16:13:07.019179] Iteration 20700, train loss = 0.145568, train accuracy = 1.000000\n",
      "[2018-06-02 16:13:26.373179] Iteration 20800, train loss = 0.158638, train accuracy = 0.992188\n",
      "[2018-06-02 16:13:45.719179] Iteration 20900, train loss = 0.144610, train accuracy = 1.000000\n",
      "[2018-06-02 16:14:05.133179] Iteration 21000, train loss = 0.139775, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927800\n",
      "[2018-06-02 16:14:28.911179] Iteration 21100, train loss = 0.144345, train accuracy = 1.000000\n",
      "[2018-06-02 16:14:48.250179] Iteration 21200, train loss = 0.169543, train accuracy = 0.992188\n",
      "[2018-06-02 16:15:07.408179] Iteration 21300, train loss = 0.138917, train accuracy = 1.000000\n",
      "[2018-06-02 16:15:26.574179] Iteration 21400, train loss = 0.147133, train accuracy = 0.992188\n",
      "[2018-06-02 16:15:45.802179] Iteration 21500, train loss = 0.155620, train accuracy = 1.000000\n",
      "[2018-06-02 16:16:05.021179] Iteration 21600, train loss = 0.169969, train accuracy = 0.992188\n",
      "[2018-06-02 16:16:24.282179] Iteration 21700, train loss = 0.139506, train accuracy = 1.000000\n",
      "[2018-06-02 16:16:43.507179] Iteration 21800, train loss = 0.154266, train accuracy = 1.000000\n",
      "[2018-06-02 16:17:02.696179] Iteration 21900, train loss = 0.148146, train accuracy = 1.000000\n",
      "[2018-06-02 16:17:21.892179] Iteration 22000, train loss = 0.149766, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 16:17:45.115179] Iteration 22100, train loss = 0.149743, train accuracy = 1.000000\n",
      "[2018-06-02 16:18:04.309179] Iteration 22200, train loss = 0.140636, train accuracy = 1.000000\n",
      "[2018-06-02 16:18:23.460179] Iteration 22300, train loss = 0.166425, train accuracy = 0.984375\n",
      "[2018-06-02 16:18:42.635179] Iteration 22400, train loss = 0.153738, train accuracy = 1.000000\n",
      "[2018-06-02 16:19:01.794179] Iteration 22500, train loss = 0.146732, train accuracy = 0.992188\n",
      "[2018-06-02 16:19:20.967179] Iteration 22600, train loss = 0.155864, train accuracy = 0.992188\n",
      "[2018-06-02 16:19:40.133179] Iteration 22700, train loss = 0.146753, train accuracy = 1.000000\n",
      "[2018-06-02 16:19:59.334179] Iteration 22800, train loss = 0.150625, train accuracy = 1.000000\n",
      "[2018-06-02 16:20:18.537179] Iteration 22900, train loss = 0.139266, train accuracy = 1.000000\n",
      "[2018-06-02 16:20:37.848179] Iteration 23000, train loss = 0.147537, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927600\n",
      "[2018-06-02 16:21:01.113179] Iteration 23100, train loss = 0.154246, train accuracy = 1.000000\n",
      "[2018-06-02 16:21:20.308179] Iteration 23200, train loss = 0.151477, train accuracy = 1.000000\n",
      "[2018-06-02 16:21:39.513179] Iteration 23300, train loss = 0.142641, train accuracy = 1.000000\n",
      "[2018-06-02 16:21:58.669179] Iteration 23400, train loss = 0.142090, train accuracy = 1.000000\n",
      "[2018-06-02 16:22:17.850179] Iteration 23500, train loss = 0.142979, train accuracy = 1.000000\n",
      "[2018-06-02 16:22:37.015179] Iteration 23600, train loss = 0.153856, train accuracy = 0.992188\n",
      "[2018-06-02 16:22:56.123179] Iteration 23700, train loss = 0.162695, train accuracy = 0.984375\n",
      "[2018-06-02 16:23:15.291179] Iteration 23800, train loss = 0.142579, train accuracy = 1.000000\n",
      "[2018-06-02 16:23:34.490179] Iteration 23900, train loss = 0.155782, train accuracy = 0.992188\n",
      "[2018-06-02 16:23:53.644179] Iteration 24000, train loss = 0.151335, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 16:24:16.927179] Iteration 24100, train loss = 0.141628, train accuracy = 1.000000\n",
      "[2018-06-02 16:24:36.037179] Iteration 24200, train loss = 0.158599, train accuracy = 0.992188\n",
      "[2018-06-02 16:24:55.378179] Iteration 24300, train loss = 0.141276, train accuracy = 1.000000\n",
      "[2018-06-02 16:25:14.615179] Iteration 24400, train loss = 0.147032, train accuracy = 1.000000\n",
      "[2018-06-02 16:25:33.789179] Iteration 24500, train loss = 0.145086, train accuracy = 1.000000\n",
      "[2018-06-02 16:25:53.028179] Iteration 24600, train loss = 0.149865, train accuracy = 0.992188\n",
      "[2018-06-02 16:26:12.221179] Iteration 24700, train loss = 0.147177, train accuracy = 1.000000\n",
      "[2018-06-02 16:26:31.432179] Iteration 24800, train loss = 0.138345, train accuracy = 1.000000\n",
      "[2018-06-02 16:26:50.670179] Iteration 24900, train loss = 0.143895, train accuracy = 1.000000\n",
      "[2018-06-02 16:27:09.925179] Iteration 25000, train loss = 0.145112, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928600\n",
      "[2018-06-02 16:27:33.223179] Iteration 25100, train loss = 0.149699, train accuracy = 0.992188\n",
      "[2018-06-02 16:27:52.434179] Iteration 25200, train loss = 0.137322, train accuracy = 1.000000\n",
      "[2018-06-02 16:28:11.585179] Iteration 25300, train loss = 0.139911, train accuracy = 1.000000\n",
      "[2018-06-02 16:28:30.882179] Iteration 25400, train loss = 0.145580, train accuracy = 1.000000\n",
      "[2018-06-02 16:28:50.023179] Iteration 25500, train loss = 0.177693, train accuracy = 0.984375\n",
      "[2018-06-02 16:29:09.202179] Iteration 25600, train loss = 0.148901, train accuracy = 0.992188\n",
      "[2018-06-02 16:29:28.397179] Iteration 25700, train loss = 0.175684, train accuracy = 0.984375\n",
      "[2018-06-02 16:29:47.530179] Iteration 25800, train loss = 0.139459, train accuracy = 1.000000\n",
      "[2018-06-02 16:30:06.673179] Iteration 25900, train loss = 0.140826, train accuracy = 1.000000\n",
      "[2018-06-02 16:30:25.796179] Iteration 26000, train loss = 0.139443, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927900\n",
      "[2018-06-02 16:30:49.156179] Iteration 26100, train loss = 0.143299, train accuracy = 1.000000\n",
      "[2018-06-02 16:31:08.368179] Iteration 26200, train loss = 0.144030, train accuracy = 1.000000\n",
      "[2018-06-02 16:31:27.677179] Iteration 26300, train loss = 0.144329, train accuracy = 1.000000\n",
      "[2018-06-02 16:31:46.866179] Iteration 26400, train loss = 0.150985, train accuracy = 0.992188\n",
      "[2018-06-02 16:32:06.051179] Iteration 26500, train loss = 0.177726, train accuracy = 0.984375\n",
      "[2018-06-02 16:32:25.209179] Iteration 26600, train loss = 0.147700, train accuracy = 0.992188\n",
      "[2018-06-02 16:32:44.340179] Iteration 26700, train loss = 0.147747, train accuracy = 1.000000\n",
      "[2018-06-02 16:33:03.470179] Iteration 26800, train loss = 0.155812, train accuracy = 1.000000\n",
      "[2018-06-02 16:33:22.670179] Iteration 26900, train loss = 0.161459, train accuracy = 1.000000\n",
      "[2018-06-02 16:33:41.844179] Iteration 27000, train loss = 0.151416, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928100\n",
      "[2018-06-02 16:34:05.185179] Iteration 27100, train loss = 0.146633, train accuracy = 1.000000\n",
      "[2018-06-02 16:34:24.377179] Iteration 27200, train loss = 0.162333, train accuracy = 0.992188\n",
      "[2018-06-02 16:34:43.549179] Iteration 27300, train loss = 0.143268, train accuracy = 1.000000\n",
      "[2018-06-02 16:35:02.767179] Iteration 27400, train loss = 0.149607, train accuracy = 1.000000\n",
      "[2018-06-02 16:35:21.960179] Iteration 27500, train loss = 0.151788, train accuracy = 0.992188\n",
      "[2018-06-02 16:35:41.176179] Iteration 27600, train loss = 0.147939, train accuracy = 0.992188\n",
      "[2018-06-02 16:36:00.321179] Iteration 27700, train loss = 0.146315, train accuracy = 1.000000\n",
      "[2018-06-02 16:36:19.539179] Iteration 27800, train loss = 0.165002, train accuracy = 0.992188\n",
      "[2018-06-02 16:36:38.705179] Iteration 27900, train loss = 0.151000, train accuracy = 0.992188\n",
      "[2018-06-02 16:36:57.908179] Iteration 28000, train loss = 0.166625, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 16:37:21.248179] Iteration 28100, train loss = 0.150460, train accuracy = 1.000000\n",
      "[2018-06-02 16:37:40.471179] Iteration 28200, train loss = 0.141953, train accuracy = 1.000000\n",
      "[2018-06-02 16:37:59.639179] Iteration 28300, train loss = 0.140341, train accuracy = 1.000000\n",
      "[2018-06-02 16:38:18.825179] Iteration 28400, train loss = 0.141640, train accuracy = 1.000000\n",
      "[2018-06-02 16:38:37.989179] Iteration 28500, train loss = 0.140748, train accuracy = 1.000000\n",
      "[2018-06-02 16:38:57.161179] Iteration 28600, train loss = 0.157666, train accuracy = 0.992188\n",
      "[2018-06-02 16:39:16.346179] Iteration 28700, train loss = 0.154306, train accuracy = 0.992188\n",
      "[2018-06-02 16:39:35.568179] Iteration 28800, train loss = 0.177190, train accuracy = 0.976562\n",
      "[2018-06-02 16:39:54.711179] Iteration 28900, train loss = 0.144647, train accuracy = 1.000000\n",
      "[2018-06-02 16:40:13.899179] Iteration 29000, train loss = 0.161504, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 16:40:37.166179] Iteration 29100, train loss = 0.172291, train accuracy = 0.976562\n",
      "[2018-06-02 16:40:56.435179] Iteration 29200, train loss = 0.150710, train accuracy = 0.992188\n",
      "[2018-06-02 16:41:15.667179] Iteration 29300, train loss = 0.166484, train accuracy = 0.992188\n",
      "[2018-06-02 16:41:34.863179] Iteration 29400, train loss = 0.157461, train accuracy = 0.992188\n",
      "[2018-06-02 16:41:54.066179] Iteration 29500, train loss = 0.149636, train accuracy = 1.000000\n",
      "[2018-06-02 16:42:13.266179] Iteration 29600, train loss = 0.150438, train accuracy = 0.992188\n",
      "[2018-06-02 16:42:32.462179] Iteration 29700, train loss = 0.139149, train accuracy = 1.000000\n",
      "[2018-06-02 16:42:51.638179] Iteration 29800, train loss = 0.156406, train accuracy = 0.992188\n",
      "[2018-06-02 16:43:10.887179] Iteration 29900, train loss = 0.166065, train accuracy = 0.992188\n",
      "[2018-06-02 16:43:30.172179] Iteration 30000, train loss = 0.147827, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928300\n",
      "[2018-06-02 16:43:53.501179] Iteration 30100, train loss = 0.146514, train accuracy = 1.000000\n",
      "[2018-06-02 16:44:12.728179] Iteration 30200, train loss = 0.145635, train accuracy = 1.000000\n",
      "[2018-06-02 16:44:31.883179] Iteration 30300, train loss = 0.143302, train accuracy = 1.000000\n",
      "[2018-06-02 16:44:51.152179] Iteration 30400, train loss = 0.140711, train accuracy = 1.000000\n",
      "[2018-06-02 16:45:10.290179] Iteration 30500, train loss = 0.178117, train accuracy = 0.984375\n",
      "[2018-06-02 16:45:29.464179] Iteration 30600, train loss = 0.160043, train accuracy = 0.984375\n",
      "[2018-06-02 16:45:48.654179] Iteration 30700, train loss = 0.141988, train accuracy = 1.000000\n",
      "[2018-06-02 16:46:07.872179] Iteration 30800, train loss = 0.154408, train accuracy = 0.992188\n",
      "[2018-06-02 16:46:27.050179] Iteration 30900, train loss = 0.155660, train accuracy = 0.992188\n",
      "[2018-06-02 16:46:46.283179] Iteration 31000, train loss = 0.160256, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 16:47:09.663179] Iteration 31100, train loss = 0.140016, train accuracy = 1.000000\n",
      "[2018-06-02 16:47:28.882179] Iteration 31200, train loss = 0.140716, train accuracy = 1.000000\n",
      "[2018-06-02 16:47:48.058179] Iteration 31300, train loss = 0.145072, train accuracy = 1.000000\n",
      "[2018-06-02 16:48:07.191179] Iteration 31400, train loss = 0.139699, train accuracy = 1.000000\n",
      "[2018-06-02 16:48:26.324179] Iteration 31500, train loss = 0.147604, train accuracy = 1.000000\n",
      "[2018-06-02 16:48:45.501179] Iteration 31600, train loss = 0.154185, train accuracy = 0.992188\n",
      "[2018-06-02 16:49:04.638179] Iteration 31700, train loss = 0.143664, train accuracy = 1.000000\n",
      "[2018-06-02 16:49:23.765179] Iteration 31800, train loss = 0.153316, train accuracy = 0.992188\n",
      "[2018-06-02 16:49:43.074179] Iteration 31900, train loss = 0.147099, train accuracy = 1.000000\n",
      "[2018-06-02 16:50:02.277179] Iteration 32000, train loss = 0.148589, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 16:50:25.606179] Iteration 32100, train loss = 0.141400, train accuracy = 1.000000\n",
      "[2018-06-02 16:50:44.829179] Iteration 32200, train loss = 0.141645, train accuracy = 1.000000\n",
      "[2018-06-02 16:51:03.985179] Iteration 32300, train loss = 0.146134, train accuracy = 1.000000\n",
      "[2018-06-02 16:51:23.226179] Iteration 32400, train loss = 0.158591, train accuracy = 0.992188\n",
      "[2018-06-02 16:51:42.428179] Iteration 32500, train loss = 0.150731, train accuracy = 1.000000\n",
      "[2018-06-02 16:52:01.714179] Iteration 32600, train loss = 0.147205, train accuracy = 1.000000\n",
      "[2018-06-02 16:52:20.850179] Iteration 32700, train loss = 0.138622, train accuracy = 1.000000\n",
      "[2018-06-02 16:52:40.085179] Iteration 32800, train loss = 0.143071, train accuracy = 1.000000\n",
      "[2018-06-02 16:52:59.238179] Iteration 32900, train loss = 0.147431, train accuracy = 1.000000\n",
      "[2018-06-02 16:53:18.460179] Iteration 33000, train loss = 0.140952, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927700\n",
      "[2018-06-02 16:53:41.748179] Iteration 33100, train loss = 0.149291, train accuracy = 0.992188\n",
      "[2018-06-02 16:54:00.857179] Iteration 33200, train loss = 0.152849, train accuracy = 1.000000\n",
      "[2018-06-02 16:54:20.061179] Iteration 33300, train loss = 0.148080, train accuracy = 1.000000\n",
      "[2018-06-02 16:54:39.253179] Iteration 33400, train loss = 0.152166, train accuracy = 1.000000\n",
      "[2018-06-02 16:54:58.322179] Iteration 33500, train loss = 0.153877, train accuracy = 0.992188\n",
      "[2018-06-02 16:55:17.588179] Iteration 33600, train loss = 0.153511, train accuracy = 0.992188\n",
      "[2018-06-02 16:55:36.848179] Iteration 33700, train loss = 0.154462, train accuracy = 0.992188\n",
      "[2018-06-02 16:55:56.244179] Iteration 33800, train loss = 0.149668, train accuracy = 1.000000\n",
      "[2018-06-02 16:56:15.477179] Iteration 33900, train loss = 0.163284, train accuracy = 0.992188\n",
      "[2018-06-02 16:56:34.614179] Iteration 34000, train loss = 0.142365, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.927600\n",
      "[2018-06-02 16:56:57.866179] Iteration 34100, train loss = 0.186389, train accuracy = 0.976562\n",
      "[2018-06-02 16:57:17.054179] Iteration 34200, train loss = 0.142685, train accuracy = 1.000000\n",
      "[2018-06-02 16:57:36.266179] Iteration 34300, train loss = 0.160388, train accuracy = 0.984375\n",
      "[2018-06-02 16:57:55.460179] Iteration 34400, train loss = 0.152811, train accuracy = 0.992188\n",
      "[2018-06-02 16:58:14.630179] Iteration 34500, train loss = 0.153789, train accuracy = 0.992188\n",
      "[2018-06-02 16:58:33.757179] Iteration 34600, train loss = 0.142457, train accuracy = 1.000000\n",
      "[2018-06-02 16:58:52.537179] Iteration 34700, train loss = 0.173070, train accuracy = 0.992188\n",
      "[2018-06-02 16:59:12.276179] Iteration 34800, train loss = 0.140810, train accuracy = 1.000000\n",
      "[2018-06-02 16:59:32.020179] Iteration 34900, train loss = 0.143334, train accuracy = 1.000000\n",
      "[2018-06-02 16:59:51.702179] Iteration 35000, train loss = 0.138700, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 17:00:15.193179] Iteration 35100, train loss = 0.156229, train accuracy = 0.992188\n",
      "[2018-06-02 17:00:34.389179] Iteration 35200, train loss = 0.143629, train accuracy = 1.000000\n",
      "[2018-06-02 17:00:54.094179] Iteration 35300, train loss = 0.140077, train accuracy = 1.000000\n",
      "[2018-06-02 17:01:13.836179] Iteration 35400, train loss = 0.137832, train accuracy = 1.000000\n",
      "[2018-06-02 17:01:33.501179] Iteration 35500, train loss = 0.171338, train accuracy = 0.992188\n",
      "[2018-06-02 17:01:53.223179] Iteration 35600, train loss = 0.147181, train accuracy = 1.000000\n",
      "[2018-06-02 17:02:12.941179] Iteration 35700, train loss = 0.140819, train accuracy = 1.000000\n",
      "[2018-06-02 17:02:32.668179] Iteration 35800, train loss = 0.151715, train accuracy = 0.992188\n",
      "[2018-06-02 17:02:52.413179] Iteration 35900, train loss = 0.140918, train accuracy = 1.000000\n",
      "[2018-06-02 17:03:12.158179] Iteration 36000, train loss = 0.151867, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.927800\n",
      "[2018-06-02 17:03:36.165179] Iteration 36100, train loss = 0.144373, train accuracy = 0.992188\n",
      "[2018-06-02 17:03:55.852179] Iteration 36200, train loss = 0.139785, train accuracy = 1.000000\n",
      "[2018-06-02 17:04:15.523179] Iteration 36300, train loss = 0.150755, train accuracy = 0.992188\n",
      "[2018-06-02 17:04:35.266179] Iteration 36400, train loss = 0.141842, train accuracy = 1.000000\n",
      "[2018-06-02 17:04:54.981179] Iteration 36500, train loss = 0.169972, train accuracy = 0.992188\n",
      "[2018-06-02 17:05:14.771179] Iteration 36600, train loss = 0.146435, train accuracy = 1.000000\n",
      "[2018-06-02 17:05:34.426179] Iteration 36700, train loss = 0.178511, train accuracy = 0.984375\n",
      "[2018-06-02 17:05:54.178179] Iteration 36800, train loss = 0.170908, train accuracy = 0.992188\n",
      "[2018-06-02 17:06:13.913179] Iteration 36900, train loss = 0.139462, train accuracy = 1.000000\n",
      "[2018-06-02 17:06:33.510179] Iteration 37000, train loss = 0.141471, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928000\n",
      "[2018-06-02 17:06:55.909179] Iteration 37100, train loss = 0.141007, train accuracy = 1.000000\n",
      "[2018-06-02 17:07:15.615179] Iteration 37200, train loss = 0.159733, train accuracy = 0.992188\n",
      "[2018-06-02 17:07:35.382179] Iteration 37300, train loss = 0.144255, train accuracy = 1.000000\n",
      "[2018-06-02 17:07:55.141179] Iteration 37400, train loss = 0.136652, train accuracy = 1.000000\n",
      "[2018-06-02 17:08:14.951179] Iteration 37500, train loss = 0.143857, train accuracy = 1.000000\n",
      "[2018-06-02 17:08:34.691179] Iteration 37600, train loss = 0.159260, train accuracy = 0.992188\n",
      "[2018-06-02 17:08:54.356179] Iteration 37700, train loss = 0.149245, train accuracy = 1.000000\n",
      "[2018-06-02 17:09:14.050179] Iteration 37800, train loss = 0.141413, train accuracy = 1.000000\n",
      "[2018-06-02 17:09:33.719179] Iteration 37900, train loss = 0.157403, train accuracy = 0.992188\n",
      "[2018-06-02 17:09:53.236179] Iteration 38000, train loss = 0.139958, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928400\n",
      "[2018-06-02 17:10:16.880179] Iteration 38100, train loss = 0.148960, train accuracy = 0.992188\n",
      "[2018-06-02 17:10:36.608179] Iteration 38200, train loss = 0.145090, train accuracy = 1.000000\n",
      "[2018-06-02 17:10:56.375179] Iteration 38300, train loss = 0.144348, train accuracy = 1.000000\n",
      "[2018-06-02 17:11:16.029179] Iteration 38400, train loss = 0.151735, train accuracy = 0.992188\n",
      "[2018-06-02 17:11:35.817179] Iteration 38500, train loss = 0.177563, train accuracy = 0.984375\n",
      "[2018-06-02 17:11:55.507179] Iteration 38600, train loss = 0.152819, train accuracy = 1.000000\n",
      "[2018-06-02 17:12:15.201179] Iteration 38700, train loss = 0.145339, train accuracy = 1.000000\n",
      "[2018-06-02 17:12:34.905179] Iteration 38800, train loss = 0.149450, train accuracy = 1.000000\n",
      "[2018-06-02 17:12:54.581179] Iteration 38900, train loss = 0.163926, train accuracy = 0.992188\n",
      "[2018-06-02 17:13:13.182179] Iteration 39000, train loss = 0.146280, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.928200\n",
      "[2018-06-02 17:13:37.232179] Iteration 39100, train loss = 0.147235, train accuracy = 1.000000\n",
      "[2018-06-02 17:13:56.965179] Iteration 39200, train loss = 0.158519, train accuracy = 0.984375\n",
      "[2018-06-02 17:14:16.666179] Iteration 39300, train loss = 0.168984, train accuracy = 0.992188\n",
      "[2018-06-02 17:14:36.359179] Iteration 39400, train loss = 0.144996, train accuracy = 1.000000\n",
      "[2018-06-02 17:14:56.030179] Iteration 39500, train loss = 0.151100, train accuracy = 0.992188\n",
      "[2018-06-02 17:15:15.758179] Iteration 39600, train loss = 0.148466, train accuracy = 1.000000\n",
      "[2018-06-02 17:15:35.601179] Iteration 39700, train loss = 0.139357, train accuracy = 1.000000\n",
      "[2018-06-02 17:15:55.315179] Iteration 39800, train loss = 0.157088, train accuracy = 0.992188\n",
      "[2018-06-02 17:16:15.021179] Iteration 39900, train loss = 0.139379, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四轮  量化  \n",
    "prune_rate = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.928300\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.03125     0.125      -0.03125    -0.0625\n",
      "  0.0058713   0.00379915  0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,1)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 17:23:09.412179] Iteration 100, train loss = 0.144742, train accuracy = 1.000000\n",
      "[2018-06-02 17:23:28.539179] Iteration 200, train loss = 0.142270, train accuracy = 1.000000\n",
      "[2018-06-02 17:23:47.704179] Iteration 300, train loss = 0.150725, train accuracy = 1.000000\n",
      "[2018-06-02 17:24:06.894179] Iteration 400, train loss = 0.158055, train accuracy = 0.992188\n",
      "[2018-06-02 17:24:25.987179] Iteration 500, train loss = 0.154361, train accuracy = 0.992188\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 17:24:45.058179] Iteration 600, train loss = 0.167586, train accuracy = 0.984375\n",
      "[2018-06-02 17:25:04.143179] Iteration 700, train loss = 0.179928, train accuracy = 0.976562\n",
      "[2018-06-02 17:25:23.253179] Iteration 800, train loss = 0.145214, train accuracy = 1.000000\n",
      "[2018-06-02 17:25:42.464179] Iteration 900, train loss = 0.145526, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "  if step <= 500:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune后的准确率\n",
      "Test accuracy = 0.923000\n"
     ]
    }
   ],
   "source": [
    "print('prune后的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.03125     0.125      -0.03125    -0.0625\n",
      "  0.0078125   0.00390625  0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
