{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "# import ipdb\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('residual_net_n', 5, '')\n",
    "tf.app.flags.DEFINE_string('train_tf_path', 'E:/dynamicquantization/data/train.tf', '')\n",
    "tf.app.flags.DEFINE_string('val_tf_path', 'E:/dynamicquantization/data/test.tf', '')\n",
    "tf.app.flags.DEFINE_integer('train_batch_size', 128, '')\n",
    "tf.app.flags.DEFINE_integer('val_batch_size', 100, '')\n",
    "tf.app.flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay')\n",
    "tf.app.flags.DEFINE_integer('summary_interval', 100, 'Interval for summary.')\n",
    "tf.app.flags.DEFINE_integer('val_interval', 1000, 'Interval for evaluation.')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000, 'Maximum number of iterations.')\n",
    "tf.app.flags.DEFINE_integer('save_interval', 5000, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  with tf.device('/cpu:0'):\n",
    "    params = tf.constant(embedding_params)\n",
    "    embedding = tf.gather(params, label)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_in, n_out, k, s, p='SAME', bias=False, scope='conv'):\n",
    "  with tf.variable_scope(scope):\n",
    "    kernel = tf.Variable(\n",
    "      tf.truncated_normal([k, k, n_in, n_out],\n",
    "        stddev=math.sqrt(2/(k*k*n_in))),\n",
    "      name='weight')\n",
    "    tf.add_to_collection('weights', kernel)\n",
    "    conv = tf.nn.conv2d(x, kernel, [1,s,s,1], padding=p)\n",
    "    if bias:\n",
    "      bias = tf.get_variable('bias', [n_out], initializer=tf.constant_initializer(0.0))\n",
    "      tf.add_to_collection('biases', bias)\n",
    "      conv = tf.nn.bias_add(conv, bias)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn', affine=True):\n",
    "  \"\"\"\n",
    "  Batch normalization on convolutional maps.\n",
    "  Args:\n",
    "    x: Tensor, 4D BHWD input maps\n",
    "    n_out: integer, depth of input maps\n",
    "    phase_train: boolean tf.Variable, true indicates training phase\n",
    "    scope: string, variable scope\n",
    "    affine: whether to affine-transform outputs\n",
    "  Return:\n",
    "    normed: batch-normalized maps\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "      name='beta', trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "      name='gamma', trainable=affine)\n",
    "    tf.add_to_collection('biases', beta)\n",
    "    tf.add_to_collection('weights', gamma)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "      ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "      with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train,\n",
    "      mean_var_with_update,\n",
    "      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \n",
    "      beta, gamma, 1e-3, affine)\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x, n_in, n_out, subsample, phase_train, scope='res_block'):\n",
    "  with tf.variable_scope(scope):\n",
    "    if subsample:\n",
    "      y = conv2d(x, n_in, n_out, 3, 2, 'SAME', False, scope='conv_1')\n",
    "      shortcut = conv2d(x, n_in, n_out, 3, 2, 'SAME',\n",
    "                False, scope='shortcut')\n",
    "    else:\n",
    "      y = conv2d(x, n_in, n_out, 3, 1, 'SAME', False, scope='conv_1')\n",
    "      shortcut = tf.identity(x, name='shortcut')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_1')\n",
    "    y = tf.nn.relu(y, name='relu_1')\n",
    "    y = conv2d(y, n_out, n_out, 3, 1, 'SAME', True, scope='conv_2')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_2')\n",
    "    y = y + shortcut\n",
    "    y = tf.nn.relu(y, name='relu_2')\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_group(x, n_in, n_out, n, first_subsample, phase_train, scope='res_group'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = residual_block(x, n_in, n_out, first_subsample, phase_train, scope='block_1')\n",
    "    for i in range(n - 1):\n",
    "      y = residual_block(y, n_out, n_out, False, phase_train, scope='block_%d' % (i + 2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_net(x, n, n_classes, phase_train, scope='res_net'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = conv2d(x, 3, 16, 3, 1, 'SAME', False, scope='conv_init')\n",
    "    y = batch_norm(y, 16, phase_train, scope='bn_init')\n",
    "    y = tf.nn.relu(y, name='relu_init')\n",
    "    y = residual_group(y, 16, 16, n, False, phase_train, scope='group_1')\n",
    "    y = residual_group(y, 16, 32, n, True, phase_train, scope='group_2')\n",
    "    y = residual_group(y, 32, 64, n, True, phase_train, scope='group_3')\n",
    "#     y = conv2d(y, 64, n_classes, 1, 1, 'SAME', True, scope='conv_last')\n",
    "    y = tf.nn.avg_pool(y, [1, 8, 8, 1], [1, 1, 1, 1], 'VALID', name='avg_pool')\n",
    "    y = tf.reshape(y, [-1, 64])\n",
    "    w = tf.get_variable(name='weight_fc', shape=[64, n_classes], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('weights', w)\n",
    "    b = tf.get_variable(name='weight_biase', shape=[n_classes], initializer=tf.constant_initializer(0))\n",
    "    tf.add_to_collection('last_biases', b)\n",
    "    y = tf.matmul(y, w) + b\n",
    "#     y = tf.squeeze(y, squeeze_dims=[1, 2])\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, labels, scope='loss'):\n",
    "  with tf.variable_scope(scope):\n",
    "    # entropy loss\n",
    "    targets = one_hot_embedding(labels, 10)\n",
    "    entropy_loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets),\n",
    "      name='entropy_loss')\n",
    "    tf.add_to_collection('losses', entropy_loss)\n",
    "    # weight l2 decay loss\n",
    "    weight_l2_losses = [tf.nn.l2_loss(o) for o in tf.get_collection('weights')]\n",
    "    weight_decay_loss = FLAGS.weight_decay*tf.add_n(weight_l2_losses)\n",
    "    tf.add_to_collection('losses', weight_decay_loss)\n",
    "  # for var in tf.get_collection('losses'):\n",
    "    # tf.scalar_summary('losses/' + var.op.name, var)\n",
    "  # total loss\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _accuracy(logits, gt_label, scope='accuracy'):\n",
    "  with tf.variable_scope(scope):\n",
    "    pred_label = tf.argmax(logits, 1)\n",
    "    acc = 1.0 - tf.nn.zero_fraction(\n",
    "      tf.cast(tf.equal(pred_label, gt_label), tf.int32))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_op(loss, global_step, learning_rate):\n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params, name='gradients')\n",
    "  optim = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "  update = optim.apply_gradients(zip(gradients, params))\n",
    "  with tf.control_dependencies([update]):\n",
    "    train_op = tf.no_op(name='train_op')\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "  # meanstd = joblib.load(FLAGS.mean_std_path)\n",
    "  # mean, std = meanstd['mean'], meanstd['std']\n",
    "  mean = [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(\n",
    "    image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_batch(train_records_path, batch_size):\n",
    "  with tf.variable_scope('train_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "      train_image = normalize_image(train_image)\n",
    "      train_image = random_distort_image(train_image)\n",
    "      train_image_batch, train_label_batch = tf.train.shuffle_batch(\n",
    "        [train_image, train_label], batch_size=batch_size, num_threads=4,\n",
    "        capacity=50000,\n",
    "        min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  with tf.variable_scope('evaluate_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "      test_image = normalize_image(test_image)\n",
    "      test_image_batch, test_label_batch = tf.train.batch(\n",
    "        [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "        capacity=10000)\n",
    "  return test_image_batch, test_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch = make_train_batch(FLAGS.train_tf_path, FLAGS.train_batch_size)\n",
    "val_image_batch, val_label_batch = make_validation_batch(FLAGS.val_tf_path, FLAGS.val_batch_size)\n",
    "\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "\n",
    "\n",
    "logits = residual_net(image_batch, FLAGS.residual_net_n, 10, phase_train)\n",
    "\n",
    "\n",
    "loss = _loss(logits, label_batch)\n",
    "accuracy = _accuracy(logits, label_batch)\n",
    "\n",
    "# train one step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "INFO:tensorflow:Restoring parameters from E:/dynamicquantization/full_precision/res32/model/res.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-6, started daemon 7908)>,\n",
       " <Thread(Thread-7, started daemon 8216)>,\n",
       " <Thread(Thread-8, started daemon 8024)>,\n",
       " <Thread(Thread-9, started daemon 9272)>,\n",
       " <Thread(Thread-10, started daemon 2596)>,\n",
       " <Thread(Thread-11, started daemon 6600)>,\n",
       " <Thread(Thread-12, started daemon 2612)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print('Initializing...')\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'E:/dynamicquantization/full_precision/res32/model/res.ckpt')\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test accuracy = 0.924600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# validation\n",
    "\n",
    "print('Evaluating...')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "  fetches = [logits, label_batch, loss]\n",
    "  session_outputs = sess.run(\n",
    "    fetches, {phase_train.name: False})\n",
    "  val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "  val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "  val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(\n",
    "  pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_prune_on_grads(grads_and_vars, dict_nzidx):\n",
    "    for key, nzidx in dict_nzidx.items():\n",
    "        count = 0\n",
    "        for grad, var in grads_and_vars:\n",
    "            if var.name == key:\n",
    "                nzidx_obj = tf.cast(tf.constant(sess.run(dict_nzidx[key])), tf.float32)\n",
    "                grads_and_vars[count] = (tf.multiply(nzidx_obj, grad), var)\n",
    "            count += 1\n",
    "    return grads_and_vars\n",
    "\n",
    "def apply_inq(weights, inq_dict, var_name, prune_rate):  \n",
    "    for target in var_name:\n",
    "        wl = target\n",
    "        bit = 32\n",
    "\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = sess.run(weight_obj)\n",
    "\n",
    "        weight_rest = np.reshape(weight_arr, [-1])\n",
    "        dic_tem = np.reshape(sess.run(inq_dict[wl]), [-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest, [-1]))), 0)\n",
    "        num_prune = int(len(weight_rest) * prune_rate)\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]]\n",
    "\n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        n2 = n1 + 1 - bit / 4\n",
    "        upper_bound = 2 ** (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        lower_bound = 2 ** (n1 + 1 - bit / 4)\n",
    "\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2 ** (np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0] * 4 / 3)))) * np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr = np.reshape(weight_rest, np.shape(weight_arr))\n",
    "        dic_tem[idx_rest[:num_prune]] = np.zeros_like(dic_tem[idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = tf.cast(np.reshape(dic_tem, np.shape(sess.run(inq_dict[wl]))), tf.float32)\n",
    "        sess.run(weights[wl].assign(weight_arr))\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮  量化\n",
    "prune_rate =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.924600\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.02709051 -0.08279994  0.01822233  0.11905745 -0.00915685 -0.03560071\n",
      "  0.01750009  0.03826123  0.00807592 -0.10065438]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.5)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-01 22:50:56.643179] Iteration 100, train loss = 0.164635, train accuracy = 0.992188\n",
      "[2018-06-01 22:51:10.680179] Iteration 200, train loss = 0.165644, train accuracy = 0.992188\n",
      "[2018-06-01 22:51:24.692179] Iteration 300, train loss = 0.185943, train accuracy = 0.992188\n",
      "[2018-06-01 22:51:38.763179] Iteration 400, train loss = 0.194825, train accuracy = 0.976562\n",
      "[2018-06-01 22:51:52.804179] Iteration 500, train loss = 0.193645, train accuracy = 0.984375\n",
      "[2018-06-01 22:52:06.846179] Iteration 600, train loss = 0.244811, train accuracy = 0.960938\n",
      "[2018-06-01 22:52:20.905179] Iteration 700, train loss = 0.164212, train accuracy = 0.992188\n",
      "[2018-06-01 22:52:35.005179] Iteration 800, train loss = 0.210110, train accuracy = 0.960938\n",
      "[2018-06-01 22:52:48.989179] Iteration 900, train loss = 0.166393, train accuracy = 0.992188\n",
      "[2018-06-01 22:53:03.007179] Iteration 1000, train loss = 0.144119, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.909400\n",
      "[2018-06-01 22:53:20.077179] Iteration 1100, train loss = 0.158135, train accuracy = 0.992188\n",
      "[2018-06-01 22:53:34.082179] Iteration 1200, train loss = 0.138826, train accuracy = 1.000000\n",
      "[2018-06-01 22:53:48.142179] Iteration 1300, train loss = 0.191853, train accuracy = 0.976562\n",
      "[2018-06-01 22:54:02.210179] Iteration 1400, train loss = 0.147043, train accuracy = 1.000000\n",
      "[2018-06-01 22:54:16.242179] Iteration 1500, train loss = 0.157700, train accuracy = 1.000000\n",
      "[2018-06-01 22:54:30.334179] Iteration 1600, train loss = 0.166722, train accuracy = 0.984375\n",
      "[2018-06-01 22:54:44.377179] Iteration 1700, train loss = 0.156471, train accuracy = 0.992188\n",
      "[2018-06-01 22:54:58.452179] Iteration 1800, train loss = 0.170902, train accuracy = 0.984375\n",
      "[2018-06-01 22:55:12.434179] Iteration 1900, train loss = 0.167193, train accuracy = 0.992188\n",
      "[2018-06-01 22:55:26.416179] Iteration 2000, train loss = 0.161699, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.912400\n",
      "[2018-06-01 22:55:43.574179] Iteration 2100, train loss = 0.150282, train accuracy = 0.992188\n",
      "[2018-06-01 22:55:57.677179] Iteration 2200, train loss = 0.157153, train accuracy = 1.000000\n",
      "[2018-06-01 22:56:11.757179] Iteration 2300, train loss = 0.157829, train accuracy = 0.984375\n",
      "[2018-06-01 22:56:25.798179] Iteration 2400, train loss = 0.201596, train accuracy = 0.984375\n",
      "[2018-06-01 22:56:39.822179] Iteration 2500, train loss = 0.155171, train accuracy = 1.000000\n",
      "[2018-06-01 22:56:53.823179] Iteration 2600, train loss = 0.165719, train accuracy = 0.992188\n",
      "[2018-06-01 22:57:07.873179] Iteration 2700, train loss = 0.161777, train accuracy = 0.984375\n",
      "[2018-06-01 22:57:21.825179] Iteration 2800, train loss = 0.174495, train accuracy = 0.984375\n",
      "[2018-06-01 22:57:35.896179] Iteration 2900, train loss = 0.151010, train accuracy = 1.000000\n",
      "[2018-06-01 22:57:50.077179] Iteration 3000, train loss = 0.152123, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.913800\n",
      "[2018-06-01 22:58:07.230179] Iteration 3100, train loss = 0.155578, train accuracy = 0.992188\n",
      "[2018-06-01 22:58:21.304179] Iteration 3200, train loss = 0.145593, train accuracy = 1.000000\n",
      "[2018-06-01 22:58:35.297179] Iteration 3300, train loss = 0.149493, train accuracy = 0.992188\n",
      "[2018-06-01 22:58:49.352179] Iteration 3400, train loss = 0.187638, train accuracy = 0.976562\n",
      "[2018-06-01 22:59:03.353179] Iteration 3500, train loss = 0.156078, train accuracy = 1.000000\n",
      "[2018-06-01 22:59:17.422179] Iteration 3600, train loss = 0.162679, train accuracy = 0.984375\n",
      "[2018-06-01 22:59:31.477179] Iteration 3700, train loss = 0.162795, train accuracy = 1.000000\n",
      "[2018-06-01 22:59:45.480179] Iteration 3800, train loss = 0.185121, train accuracy = 0.984375\n",
      "[2018-06-01 22:59:59.551179] Iteration 3900, train loss = 0.175485, train accuracy = 0.976562\n",
      "[2018-06-01 23:00:13.659179] Iteration 4000, train loss = 0.146995, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.915700\n",
      "[2018-06-01 23:00:30.777179] Iteration 4100, train loss = 0.151819, train accuracy = 1.000000\n",
      "[2018-06-01 23:00:44.894179] Iteration 4200, train loss = 0.154381, train accuracy = 1.000000\n",
      "[2018-06-01 23:00:58.932179] Iteration 4300, train loss = 0.158610, train accuracy = 0.992188\n",
      "[2018-06-01 23:01:12.982179] Iteration 4400, train loss = 0.163641, train accuracy = 0.992188\n",
      "[2018-06-01 23:01:27.087179] Iteration 4500, train loss = 0.142127, train accuracy = 1.000000\n",
      "[2018-06-01 23:01:41.247179] Iteration 4600, train loss = 0.184797, train accuracy = 0.976562\n",
      "[2018-06-01 23:01:55.293179] Iteration 4700, train loss = 0.151000, train accuracy = 1.000000\n",
      "[2018-06-01 23:02:09.357179] Iteration 4800, train loss = 0.155254, train accuracy = 0.992188\n",
      "[2018-06-01 23:02:23.356179] Iteration 4900, train loss = 0.151974, train accuracy = 0.992188\n",
      "[2018-06-01 23:02:37.409179] Iteration 5000, train loss = 0.142685, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.915300\n",
      "[2018-06-01 23:02:54.489179] Iteration 5100, train loss = 0.140793, train accuracy = 1.000000\n",
      "[2018-06-01 23:03:08.613179] Iteration 5200, train loss = 0.194625, train accuracy = 0.976562\n",
      "[2018-06-01 23:03:22.645179] Iteration 5300, train loss = 0.147349, train accuracy = 1.000000\n",
      "[2018-06-01 23:03:36.661179] Iteration 5400, train loss = 0.154721, train accuracy = 0.992188\n",
      "[2018-06-01 23:03:50.722179] Iteration 5500, train loss = 0.155980, train accuracy = 0.992188\n",
      "[2018-06-01 23:04:04.738179] Iteration 5600, train loss = 0.143515, train accuracy = 1.000000\n",
      "[2018-06-01 23:04:18.803179] Iteration 5700, train loss = 0.141595, train accuracy = 1.000000\n",
      "[2018-06-01 23:04:32.862179] Iteration 5800, train loss = 0.161257, train accuracy = 0.992188\n",
      "[2018-06-01 23:04:46.870179] Iteration 5900, train loss = 0.148849, train accuracy = 1.000000\n",
      "[2018-06-01 23:05:00.835179] Iteration 6000, train loss = 0.157199, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.916200\n",
      "[2018-06-01 23:05:17.976179] Iteration 6100, train loss = 0.184916, train accuracy = 0.976562\n",
      "[2018-06-01 23:05:32.088179] Iteration 6200, train loss = 0.179582, train accuracy = 0.984375\n",
      "[2018-06-01 23:05:46.109179] Iteration 6300, train loss = 0.141659, train accuracy = 1.000000\n",
      "[2018-06-01 23:06:00.276179] Iteration 6400, train loss = 0.150702, train accuracy = 0.992188\n",
      "[2018-06-01 23:06:14.269179] Iteration 6500, train loss = 0.143386, train accuracy = 1.000000\n",
      "[2018-06-01 23:06:28.308179] Iteration 6600, train loss = 0.162571, train accuracy = 0.992188\n",
      "[2018-06-01 23:06:42.422179] Iteration 6700, train loss = 0.135548, train accuracy = 1.000000\n",
      "[2018-06-01 23:06:56.522179] Iteration 6800, train loss = 0.140952, train accuracy = 1.000000\n",
      "[2018-06-01 23:07:10.566179] Iteration 6900, train loss = 0.234733, train accuracy = 0.968750\n",
      "[2018-06-01 23:07:24.632179] Iteration 7000, train loss = 0.159486, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.917800\n",
      "[2018-06-01 23:07:41.731179] Iteration 7100, train loss = 0.161408, train accuracy = 0.992188\n",
      "[2018-06-01 23:07:55.755179] Iteration 7200, train loss = 0.173072, train accuracy = 0.984375\n",
      "[2018-06-01 23:08:09.816179] Iteration 7300, train loss = 0.146784, train accuracy = 1.000000\n",
      "[2018-06-01 23:08:23.897179] Iteration 7400, train loss = 0.165234, train accuracy = 0.992188\n",
      "[2018-06-01 23:08:37.944179] Iteration 7500, train loss = 0.149071, train accuracy = 0.992188\n",
      "[2018-06-01 23:08:51.954179] Iteration 7600, train loss = 0.151148, train accuracy = 1.000000\n",
      "[2018-06-01 23:09:05.987179] Iteration 7700, train loss = 0.149765, train accuracy = 1.000000\n",
      "[2018-06-01 23:09:20.034179] Iteration 7800, train loss = 0.172406, train accuracy = 0.992188\n",
      "[2018-06-01 23:09:34.106179] Iteration 7900, train loss = 0.168254, train accuracy = 0.992188\n",
      "[2018-06-01 23:09:48.159179] Iteration 8000, train loss = 0.156468, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.917700\n",
      "[2018-06-01 23:10:05.389179] Iteration 8100, train loss = 0.136036, train accuracy = 1.000000\n",
      "[2018-06-01 23:10:19.412179] Iteration 8200, train loss = 0.150966, train accuracy = 0.992188\n",
      "[2018-06-01 23:10:33.427179] Iteration 8300, train loss = 0.159175, train accuracy = 0.984375\n",
      "[2018-06-01 23:10:47.426179] Iteration 8400, train loss = 0.157938, train accuracy = 0.984375\n",
      "[2018-06-01 23:11:01.565179] Iteration 8500, train loss = 0.144996, train accuracy = 1.000000\n",
      "[2018-06-01 23:11:15.577179] Iteration 8600, train loss = 0.157623, train accuracy = 0.984375\n",
      "[2018-06-01 23:11:29.643179] Iteration 8700, train loss = 0.146078, train accuracy = 1.000000\n",
      "[2018-06-01 23:11:43.747179] Iteration 8800, train loss = 0.145847, train accuracy = 1.000000\n",
      "[2018-06-01 23:11:57.775179] Iteration 8900, train loss = 0.163377, train accuracy = 0.984375\n",
      "[2018-06-01 23:12:11.812179] Iteration 9000, train loss = 0.145176, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919200\n",
      "[2018-06-01 23:12:28.832179] Iteration 9100, train loss = 0.149882, train accuracy = 1.000000\n",
      "[2018-06-01 23:12:42.909179] Iteration 9200, train loss = 0.148824, train accuracy = 1.000000\n",
      "[2018-06-01 23:12:56.898179] Iteration 9300, train loss = 0.159574, train accuracy = 0.984375\n",
      "[2018-06-01 23:13:10.895179] Iteration 9400, train loss = 0.154808, train accuracy = 0.992188\n",
      "[2018-06-01 23:13:25.011179] Iteration 9500, train loss = 0.137162, train accuracy = 1.000000\n",
      "[2018-06-01 23:13:39.096179] Iteration 9600, train loss = 0.146164, train accuracy = 0.992188\n",
      "[2018-06-01 23:13:53.156179] Iteration 9700, train loss = 0.139077, train accuracy = 1.000000\n",
      "[2018-06-01 23:14:07.194179] Iteration 9800, train loss = 0.155305, train accuracy = 0.992188\n",
      "[2018-06-01 23:14:21.221179] Iteration 9900, train loss = 0.144614, train accuracy = 1.000000\n",
      "[2018-06-01 23:14:35.202179] Iteration 10000, train loss = 0.134436, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919000\n",
      "[2018-06-01 23:14:52.227179] Iteration 10100, train loss = 0.153141, train accuracy = 0.992188\n",
      "[2018-06-01 23:15:06.289179] Iteration 10200, train loss = 0.144548, train accuracy = 1.000000\n",
      "[2018-06-01 23:15:20.389179] Iteration 10300, train loss = 0.152536, train accuracy = 1.000000\n",
      "[2018-06-01 23:15:34.424179] Iteration 10400, train loss = 0.147615, train accuracy = 1.000000\n",
      "[2018-06-01 23:15:48.412179] Iteration 10500, train loss = 0.139335, train accuracy = 1.000000\n",
      "[2018-06-01 23:16:02.563179] Iteration 10600, train loss = 0.154686, train accuracy = 0.992188\n",
      "[2018-06-01 23:16:16.599179] Iteration 10700, train loss = 0.152212, train accuracy = 0.992188\n",
      "[2018-06-01 23:16:30.685179] Iteration 10800, train loss = 0.140692, train accuracy = 1.000000\n",
      "[2018-06-01 23:16:44.797179] Iteration 10900, train loss = 0.160222, train accuracy = 0.992188\n",
      "[2018-06-01 23:16:58.877179] Iteration 11000, train loss = 0.168056, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.919800\n",
      "[2018-06-01 23:17:16.000179] Iteration 11100, train loss = 0.185864, train accuracy = 0.976562\n",
      "[2018-06-01 23:17:30.071179] Iteration 11200, train loss = 0.147726, train accuracy = 1.000000\n",
      "[2018-06-01 23:17:44.120179] Iteration 11300, train loss = 0.147957, train accuracy = 1.000000\n",
      "[2018-06-01 23:17:58.152179] Iteration 11400, train loss = 0.156329, train accuracy = 0.992188\n",
      "[2018-06-01 23:18:12.185179] Iteration 11500, train loss = 0.142035, train accuracy = 1.000000\n",
      "[2018-06-01 23:18:26.235179] Iteration 11600, train loss = 0.144536, train accuracy = 0.992188\n",
      "[2018-06-01 23:18:40.232179] Iteration 11700, train loss = 0.143402, train accuracy = 1.000000\n",
      "[2018-06-01 23:18:54.270179] Iteration 11800, train loss = 0.161590, train accuracy = 0.984375\n",
      "[2018-06-01 23:19:08.221179] Iteration 11900, train loss = 0.141801, train accuracy = 1.000000\n",
      "[2018-06-01 23:19:22.331179] Iteration 12000, train loss = 0.142902, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919600\n",
      "[2018-06-01 23:19:39.430179] Iteration 12100, train loss = 0.156731, train accuracy = 1.000000\n",
      "[2018-06-01 23:19:53.506179] Iteration 12200, train loss = 0.178178, train accuracy = 0.992188\n",
      "[2018-06-01 23:20:07.541179] Iteration 12300, train loss = 0.149636, train accuracy = 0.992188\n",
      "[2018-06-01 23:20:21.541179] Iteration 12400, train loss = 0.156069, train accuracy = 0.992188\n",
      "[2018-06-01 23:20:35.619179] Iteration 12500, train loss = 0.137692, train accuracy = 1.000000\n",
      "[2018-06-01 23:20:49.697179] Iteration 12600, train loss = 0.141669, train accuracy = 1.000000\n",
      "[2018-06-01 23:21:03.689179] Iteration 12700, train loss = 0.144753, train accuracy = 1.000000\n",
      "[2018-06-01 23:21:17.765179] Iteration 12800, train loss = 0.164233, train accuracy = 0.992188\n",
      "[2018-06-01 23:21:31.810179] Iteration 12900, train loss = 0.152036, train accuracy = 0.992188\n",
      "[2018-06-01 23:21:45.814179] Iteration 13000, train loss = 0.153869, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921100\n",
      "[2018-06-01 23:22:02.930179] Iteration 13100, train loss = 0.140156, train accuracy = 1.000000\n",
      "[2018-06-01 23:22:17.017179] Iteration 13200, train loss = 0.141079, train accuracy = 1.000000\n",
      "[2018-06-01 23:22:31.051179] Iteration 13300, train loss = 0.145295, train accuracy = 1.000000\n",
      "[2018-06-01 23:22:45.090179] Iteration 13400, train loss = 0.139772, train accuracy = 1.000000\n",
      "[2018-06-01 23:22:59.110179] Iteration 13500, train loss = 0.139131, train accuracy = 1.000000\n",
      "[2018-06-01 23:23:13.229179] Iteration 13600, train loss = 0.153483, train accuracy = 0.984375\n",
      "[2018-06-01 23:23:27.266179] Iteration 13700, train loss = 0.156579, train accuracy = 0.992188\n",
      "[2018-06-01 23:23:41.320179] Iteration 13800, train loss = 0.154438, train accuracy = 0.992188\n",
      "[2018-06-01 23:23:55.366179] Iteration 13900, train loss = 0.145911, train accuracy = 1.000000\n",
      "[2018-06-01 23:24:09.340179] Iteration 14000, train loss = 0.172981, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.920300\n",
      "[2018-06-01 23:24:26.378179] Iteration 14100, train loss = 0.162076, train accuracy = 0.992188\n",
      "[2018-06-01 23:24:40.567179] Iteration 14200, train loss = 0.155964, train accuracy = 0.992188\n",
      "[2018-06-01 23:24:54.571179] Iteration 14300, train loss = 0.144288, train accuracy = 0.992188\n",
      "[2018-06-01 23:25:08.576179] Iteration 14400, train loss = 0.141278, train accuracy = 1.000000\n",
      "[2018-06-01 23:25:22.649179] Iteration 14500, train loss = 0.213259, train accuracy = 0.976562\n",
      "[2018-06-01 23:25:36.811179] Iteration 14600, train loss = 0.154777, train accuracy = 1.000000\n",
      "[2018-06-01 23:25:50.916179] Iteration 14700, train loss = 0.142338, train accuracy = 1.000000\n",
      "[2018-06-01 23:26:05.036179] Iteration 14800, train loss = 0.143374, train accuracy = 1.000000\n",
      "[2018-06-01 23:26:19.077179] Iteration 14900, train loss = 0.147210, train accuracy = 0.992188\n",
      "[2018-06-01 23:26:33.135179] Iteration 15000, train loss = 0.139777, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920800\n",
      "[2018-06-01 23:26:50.191179] Iteration 15100, train loss = 0.156582, train accuracy = 0.992188\n",
      "[2018-06-01 23:27:04.271179] Iteration 15200, train loss = 0.149082, train accuracy = 1.000000\n",
      "[2018-06-01 23:27:18.297179] Iteration 15300, train loss = 0.175629, train accuracy = 0.984375\n",
      "[2018-06-01 23:27:32.415179] Iteration 15400, train loss = 0.137911, train accuracy = 1.000000\n",
      "[2018-06-01 23:27:46.572179] Iteration 15500, train loss = 0.150707, train accuracy = 1.000000\n",
      "[2018-06-01 23:28:00.619179] Iteration 15600, train loss = 0.145767, train accuracy = 0.992188\n",
      "[2018-06-01 23:28:14.718179] Iteration 15700, train loss = 0.142359, train accuracy = 1.000000\n",
      "[2018-06-01 23:28:28.697179] Iteration 15800, train loss = 0.145336, train accuracy = 1.000000\n",
      "[2018-06-01 23:28:42.741179] Iteration 15900, train loss = 0.156111, train accuracy = 0.992188\n",
      "[2018-06-01 23:28:56.756179] Iteration 16000, train loss = 0.149502, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-01 23:29:13.826179] Iteration 16100, train loss = 0.141789, train accuracy = 1.000000\n",
      "[2018-06-01 23:29:27.889179] Iteration 16200, train loss = 0.165120, train accuracy = 0.984375\n",
      "[2018-06-01 23:29:41.876179] Iteration 16300, train loss = 0.151989, train accuracy = 1.000000\n",
      "[2018-06-01 23:29:55.873179] Iteration 16400, train loss = 0.143367, train accuracy = 1.000000\n",
      "[2018-06-01 23:30:09.895179] Iteration 16500, train loss = 0.152829, train accuracy = 1.000000\n",
      "[2018-06-01 23:30:23.966179] Iteration 16600, train loss = 0.167136, train accuracy = 0.984375\n",
      "[2018-06-01 23:30:38.041179] Iteration 16700, train loss = 0.143553, train accuracy = 0.992188\n",
      "[2018-06-01 23:30:52.046179] Iteration 16800, train loss = 0.141937, train accuracy = 1.000000\n",
      "[2018-06-01 23:31:06.075179] Iteration 16900, train loss = 0.145544, train accuracy = 1.000000\n",
      "[2018-06-01 23:31:20.168179] Iteration 17000, train loss = 0.167985, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-01 23:31:37.212179] Iteration 17100, train loss = 0.154964, train accuracy = 0.992188\n",
      "[2018-06-01 23:31:51.267179] Iteration 17200, train loss = 0.146213, train accuracy = 0.992188\n",
      "[2018-06-01 23:32:05.318179] Iteration 17300, train loss = 0.143530, train accuracy = 1.000000\n",
      "[2018-06-01 23:32:19.353179] Iteration 17400, train loss = 0.155422, train accuracy = 0.992188\n",
      "[2018-06-01 23:32:33.372179] Iteration 17500, train loss = 0.144914, train accuracy = 1.000000\n",
      "[2018-06-01 23:32:47.399179] Iteration 17600, train loss = 0.145901, train accuracy = 1.000000\n",
      "[2018-06-01 23:33:01.427179] Iteration 17700, train loss = 0.140366, train accuracy = 1.000000\n",
      "[2018-06-01 23:33:15.397179] Iteration 17800, train loss = 0.191697, train accuracy = 0.984375\n",
      "[2018-06-01 23:33:29.452179] Iteration 17900, train loss = 0.156276, train accuracy = 0.992188\n",
      "[2018-06-01 23:33:43.443179] Iteration 18000, train loss = 0.140989, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920800\n",
      "[2018-06-01 23:34:00.462179] Iteration 18100, train loss = 0.144130, train accuracy = 0.992188\n",
      "[2018-06-01 23:34:14.484179] Iteration 18200, train loss = 0.139529, train accuracy = 1.000000\n",
      "[2018-06-01 23:34:28.553179] Iteration 18300, train loss = 0.147442, train accuracy = 1.000000\n",
      "[2018-06-01 23:34:42.652179] Iteration 18400, train loss = 0.152718, train accuracy = 0.992188\n",
      "[2018-06-01 23:34:56.739179] Iteration 18500, train loss = 0.139864, train accuracy = 1.000000\n",
      "[2018-06-01 23:35:10.765179] Iteration 18600, train loss = 0.150056, train accuracy = 1.000000\n",
      "[2018-06-01 23:35:24.744179] Iteration 18700, train loss = 0.142301, train accuracy = 1.000000\n",
      "[2018-06-01 23:35:38.829179] Iteration 18800, train loss = 0.168983, train accuracy = 0.976562\n",
      "[2018-06-01 23:35:52.977179] Iteration 18900, train loss = 0.150374, train accuracy = 1.000000\n",
      "[2018-06-01 23:36:06.989179] Iteration 19000, train loss = 0.191107, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921400\n",
      "[2018-06-01 23:36:24.149179] Iteration 19100, train loss = 0.154873, train accuracy = 1.000000\n",
      "[2018-06-01 23:36:38.241179] Iteration 19200, train loss = 0.138745, train accuracy = 1.000000\n",
      "[2018-06-01 23:36:52.334179] Iteration 19300, train loss = 0.145472, train accuracy = 1.000000\n",
      "[2018-06-01 23:37:06.345179] Iteration 19400, train loss = 0.171275, train accuracy = 0.984375\n",
      "[2018-06-01 23:37:20.385179] Iteration 19500, train loss = 0.146880, train accuracy = 1.000000\n",
      "[2018-06-01 23:37:34.458179] Iteration 19600, train loss = 0.136113, train accuracy = 1.000000\n",
      "[2018-06-01 23:37:48.497179] Iteration 19700, train loss = 0.138261, train accuracy = 1.000000\n",
      "[2018-06-01 23:38:02.643179] Iteration 19800, train loss = 0.147398, train accuracy = 0.992188\n",
      "[2018-06-01 23:38:16.671179] Iteration 19900, train loss = 0.137858, train accuracy = 1.000000\n",
      "[2018-06-01 23:38:30.696179] Iteration 20000, train loss = 0.140120, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921100\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-01 23:38:47.759179] Iteration 20100, train loss = 0.150074, train accuracy = 0.992188\n",
      "[2018-06-01 23:39:01.762179] Iteration 20200, train loss = 0.137101, train accuracy = 1.000000\n",
      "[2018-06-01 23:39:15.760179] Iteration 20300, train loss = 0.137203, train accuracy = 1.000000\n",
      "[2018-06-01 23:39:29.786179] Iteration 20400, train loss = 0.134302, train accuracy = 1.000000\n",
      "[2018-06-01 23:39:43.842179] Iteration 20500, train loss = 0.152352, train accuracy = 0.992188\n",
      "[2018-06-01 23:39:57.924179] Iteration 20600, train loss = 0.151459, train accuracy = 0.992188\n",
      "[2018-06-01 23:40:11.949179] Iteration 20700, train loss = 0.137942, train accuracy = 1.000000\n",
      "[2018-06-01 23:40:25.929179] Iteration 20800, train loss = 0.155215, train accuracy = 0.992188\n",
      "[2018-06-01 23:40:39.958179] Iteration 20900, train loss = 0.151010, train accuracy = 0.992188\n",
      "[2018-06-01 23:40:54.009179] Iteration 21000, train loss = 0.146947, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922000\n",
      "[2018-06-01 23:41:11.065179] Iteration 21100, train loss = 0.144204, train accuracy = 0.992188\n",
      "[2018-06-01 23:41:25.122179] Iteration 21200, train loss = 0.167735, train accuracy = 0.992188\n",
      "[2018-06-01 23:41:39.147179] Iteration 21300, train loss = 0.150206, train accuracy = 0.992188\n",
      "[2018-06-01 23:41:53.107179] Iteration 21400, train loss = 0.148569, train accuracy = 1.000000\n",
      "[2018-06-01 23:42:07.097179] Iteration 21500, train loss = 0.136183, train accuracy = 1.000000\n",
      "[2018-06-01 23:42:21.133179] Iteration 21600, train loss = 0.151538, train accuracy = 0.992188\n",
      "[2018-06-01 23:42:35.205179] Iteration 21700, train loss = 0.145935, train accuracy = 1.000000\n",
      "[2018-06-01 23:42:49.239179] Iteration 21800, train loss = 0.137225, train accuracy = 1.000000\n",
      "[2018-06-01 23:43:03.295179] Iteration 21900, train loss = 0.148253, train accuracy = 0.992188\n",
      "[2018-06-01 23:43:17.314179] Iteration 22000, train loss = 0.139879, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921700\n",
      "[2018-06-01 23:43:34.465179] Iteration 22100, train loss = 0.165381, train accuracy = 0.984375\n",
      "[2018-06-01 23:43:48.467179] Iteration 22200, train loss = 0.143088, train accuracy = 1.000000\n",
      "[2018-06-01 23:44:02.503179] Iteration 22300, train loss = 0.137241, train accuracy = 1.000000\n",
      "[2018-06-01 23:44:16.554179] Iteration 22400, train loss = 0.158083, train accuracy = 0.992188\n",
      "[2018-06-01 23:44:30.625179] Iteration 22500, train loss = 0.135079, train accuracy = 1.000000\n",
      "[2018-06-01 23:44:44.668179] Iteration 22600, train loss = 0.152046, train accuracy = 1.000000\n",
      "[2018-06-01 23:44:58.825179] Iteration 22700, train loss = 0.169638, train accuracy = 0.992188\n",
      "[2018-06-01 23:45:12.909179] Iteration 22800, train loss = 0.156638, train accuracy = 0.992188\n",
      "[2018-06-01 23:45:26.971179] Iteration 22900, train loss = 0.149785, train accuracy = 0.992188\n",
      "[2018-06-01 23:45:41.021179] Iteration 23000, train loss = 0.147079, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921600\n",
      "[2018-06-01 23:45:58.092179] Iteration 23100, train loss = 0.156160, train accuracy = 0.992188\n",
      "[2018-06-01 23:46:12.191179] Iteration 23200, train loss = 0.143940, train accuracy = 1.000000\n",
      "[2018-06-01 23:46:26.301179] Iteration 23300, train loss = 0.147161, train accuracy = 1.000000\n",
      "[2018-06-01 23:46:40.346179] Iteration 23400, train loss = 0.152737, train accuracy = 0.984375\n",
      "[2018-06-01 23:46:54.442179] Iteration 23500, train loss = 0.146101, train accuracy = 0.992188\n",
      "[2018-06-01 23:47:08.576179] Iteration 23600, train loss = 0.144373, train accuracy = 1.000000\n",
      "[2018-06-01 23:47:22.632179] Iteration 23700, train loss = 0.139796, train accuracy = 1.000000\n",
      "[2018-06-01 23:47:36.690179] Iteration 23800, train loss = 0.168245, train accuracy = 0.992188\n",
      "[2018-06-01 23:47:50.651179] Iteration 23900, train loss = 0.203306, train accuracy = 0.984375\n",
      "[2018-06-01 23:48:04.657179] Iteration 24000, train loss = 0.191323, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921500\n",
      "[2018-06-01 23:48:21.780179] Iteration 24100, train loss = 0.150905, train accuracy = 0.992188\n",
      "[2018-06-01 23:48:35.975179] Iteration 24200, train loss = 0.147809, train accuracy = 0.992188\n",
      "[2018-06-01 23:48:50.063179] Iteration 24300, train loss = 0.142961, train accuracy = 1.000000\n",
      "[2018-06-01 23:49:04.124179] Iteration 24400, train loss = 0.136417, train accuracy = 1.000000\n",
      "[2018-06-01 23:49:18.175179] Iteration 24500, train loss = 0.143942, train accuracy = 1.000000\n",
      "[2018-06-01 23:49:32.187179] Iteration 24600, train loss = 0.189707, train accuracy = 0.984375\n",
      "[2018-06-01 23:49:46.292179] Iteration 24700, train loss = 0.138745, train accuracy = 1.000000\n",
      "[2018-06-01 23:50:00.291179] Iteration 24800, train loss = 0.177653, train accuracy = 0.976562\n",
      "[2018-06-01 23:50:14.351179] Iteration 24900, train loss = 0.148994, train accuracy = 0.992188\n",
      "[2018-06-01 23:50:28.400179] Iteration 25000, train loss = 0.149948, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921700\n",
      "[2018-06-01 23:50:45.545179] Iteration 25100, train loss = 0.158200, train accuracy = 0.992188\n",
      "[2018-06-01 23:50:59.554179] Iteration 25200, train loss = 0.142642, train accuracy = 1.000000\n",
      "[2018-06-01 23:51:13.587179] Iteration 25300, train loss = 0.150327, train accuracy = 0.992188\n",
      "[2018-06-01 23:51:27.721179] Iteration 25400, train loss = 0.149880, train accuracy = 1.000000\n",
      "[2018-06-01 23:51:41.739179] Iteration 25500, train loss = 0.143580, train accuracy = 1.000000\n",
      "[2018-06-01 23:51:55.877179] Iteration 25600, train loss = 0.153539, train accuracy = 1.000000\n",
      "[2018-06-01 23:52:09.899179] Iteration 25700, train loss = 0.141926, train accuracy = 1.000000\n",
      "[2018-06-01 23:52:23.959179] Iteration 25800, train loss = 0.169366, train accuracy = 0.976562\n",
      "[2018-06-01 23:52:38.087179] Iteration 25900, train loss = 0.137720, train accuracy = 1.000000\n",
      "[2018-06-01 23:52:52.151179] Iteration 26000, train loss = 0.139353, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921800\n",
      "[2018-06-01 23:53:09.261179] Iteration 26100, train loss = 0.143738, train accuracy = 1.000000\n",
      "[2018-06-01 23:53:23.277179] Iteration 26200, train loss = 0.181921, train accuracy = 0.976562\n",
      "[2018-06-01 23:53:37.263179] Iteration 26300, train loss = 0.149863, train accuracy = 0.992188\n",
      "[2018-06-01 23:53:51.347179] Iteration 26400, train loss = 0.165552, train accuracy = 0.984375\n",
      "[2018-06-01 23:54:05.374179] Iteration 26500, train loss = 0.144465, train accuracy = 1.000000\n",
      "[2018-06-01 23:54:19.436179] Iteration 26600, train loss = 0.163686, train accuracy = 0.992188\n",
      "[2018-06-01 23:54:33.521179] Iteration 26700, train loss = 0.140835, train accuracy = 1.000000\n",
      "[2018-06-01 23:54:47.563179] Iteration 26800, train loss = 0.154719, train accuracy = 0.992188\n",
      "[2018-06-01 23:55:01.705179] Iteration 26900, train loss = 0.140357, train accuracy = 1.000000\n",
      "[2018-06-01 23:55:15.732179] Iteration 27000, train loss = 0.145494, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921900\n",
      "[2018-06-01 23:55:32.764179] Iteration 27100, train loss = 0.140301, train accuracy = 1.000000\n",
      "[2018-06-01 23:55:46.931179] Iteration 27200, train loss = 0.150791, train accuracy = 1.000000\n",
      "[2018-06-01 23:56:00.919179] Iteration 27300, train loss = 0.152227, train accuracy = 1.000000\n",
      "[2018-06-01 23:56:15.018179] Iteration 27400, train loss = 0.138665, train accuracy = 1.000000\n",
      "[2018-06-01 23:56:29.090179] Iteration 27500, train loss = 0.138909, train accuracy = 1.000000\n",
      "[2018-06-01 23:56:43.098179] Iteration 27600, train loss = 0.156761, train accuracy = 0.992188\n",
      "[2018-06-01 23:56:57.073179] Iteration 27700, train loss = 0.139997, train accuracy = 1.000000\n",
      "[2018-06-01 23:57:11.170179] Iteration 27800, train loss = 0.161313, train accuracy = 0.992188\n",
      "[2018-06-01 23:57:25.201179] Iteration 27900, train loss = 0.156346, train accuracy = 0.992188\n",
      "[2018-06-01 23:57:39.186179] Iteration 28000, train loss = 0.160254, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921800\n",
      "[2018-06-01 23:57:56.259179] Iteration 28100, train loss = 0.140920, train accuracy = 1.000000\n",
      "[2018-06-01 23:58:10.283179] Iteration 28200, train loss = 0.146582, train accuracy = 0.992188\n",
      "[2018-06-01 23:58:24.271179] Iteration 28300, train loss = 0.141785, train accuracy = 1.000000\n",
      "[2018-06-01 23:58:38.302179] Iteration 28400, train loss = 0.143450, train accuracy = 1.000000\n",
      "[2018-06-01 23:58:52.356179] Iteration 28500, train loss = 0.155574, train accuracy = 1.000000\n",
      "[2018-06-01 23:59:06.379179] Iteration 28600, train loss = 0.135941, train accuracy = 1.000000\n",
      "[2018-06-01 23:59:20.462179] Iteration 28700, train loss = 0.168914, train accuracy = 0.976562\n",
      "[2018-06-01 23:59:34.426179] Iteration 28800, train loss = 0.139843, train accuracy = 1.000000\n",
      "[2018-06-01 23:59:48.450179] Iteration 28900, train loss = 0.158571, train accuracy = 0.992188\n",
      "[2018-06-02 00:00:02.486179] Iteration 29000, train loss = 0.140298, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921600\n",
      "[2018-06-02 00:00:19.564179] Iteration 29100, train loss = 0.152100, train accuracy = 0.992188\n",
      "[2018-06-02 00:00:33.639179] Iteration 29200, train loss = 0.146666, train accuracy = 1.000000\n",
      "[2018-06-02 00:00:47.681179] Iteration 29300, train loss = 0.143998, train accuracy = 1.000000\n",
      "[2018-06-02 00:01:01.767179] Iteration 29400, train loss = 0.141537, train accuracy = 1.000000\n",
      "[2018-06-02 00:01:15.795179] Iteration 29500, train loss = 0.143406, train accuracy = 1.000000\n",
      "[2018-06-02 00:01:29.881179] Iteration 29600, train loss = 0.135848, train accuracy = 1.000000\n",
      "[2018-06-02 00:01:43.882179] Iteration 29700, train loss = 0.143193, train accuracy = 1.000000\n",
      "[2018-06-02 00:01:57.906179] Iteration 29800, train loss = 0.172477, train accuracy = 0.992188\n",
      "[2018-06-02 00:02:11.974179] Iteration 29900, train loss = 0.137061, train accuracy = 1.000000\n",
      "[2018-06-02 00:02:26.102179] Iteration 30000, train loss = 0.159445, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921600\n",
      "[2018-06-02 00:02:43.258179] Iteration 30100, train loss = 0.138125, train accuracy = 1.000000\n",
      "[2018-06-02 00:02:57.289179] Iteration 30200, train loss = 0.157207, train accuracy = 0.992188\n",
      "[2018-06-02 00:03:11.362179] Iteration 30300, train loss = 0.171545, train accuracy = 0.984375\n",
      "[2018-06-02 00:03:25.507179] Iteration 30400, train loss = 0.147747, train accuracy = 0.992188\n",
      "[2018-06-02 00:03:39.514179] Iteration 30500, train loss = 0.162781, train accuracy = 0.984375\n",
      "[2018-06-02 00:03:53.590179] Iteration 30600, train loss = 0.152933, train accuracy = 0.992188\n",
      "[2018-06-02 00:04:07.654179] Iteration 30700, train loss = 0.147284, train accuracy = 0.992188\n",
      "[2018-06-02 00:04:21.749179] Iteration 30800, train loss = 0.162373, train accuracy = 0.984375\n",
      "[2018-06-02 00:04:35.789179] Iteration 30900, train loss = 0.143833, train accuracy = 1.000000\n",
      "[2018-06-02 00:04:49.836179] Iteration 31000, train loss = 0.156719, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922100\n",
      "[2018-06-02 00:05:06.911179] Iteration 31100, train loss = 0.135084, train accuracy = 1.000000\n",
      "[2018-06-02 00:05:20.901179] Iteration 31200, train loss = 0.145288, train accuracy = 1.000000\n",
      "[2018-06-02 00:05:34.914179] Iteration 31300, train loss = 0.148674, train accuracy = 1.000000\n",
      "[2018-06-02 00:05:49.004179] Iteration 31400, train loss = 0.148844, train accuracy = 0.992188\n",
      "[2018-06-02 00:06:03.100179] Iteration 31500, train loss = 0.146752, train accuracy = 1.000000\n",
      "[2018-06-02 00:06:17.109179] Iteration 31600, train loss = 0.147765, train accuracy = 0.992188\n",
      "[2018-06-02 00:06:31.125179] Iteration 31700, train loss = 0.145758, train accuracy = 1.000000\n",
      "[2018-06-02 00:06:45.151179] Iteration 31800, train loss = 0.149296, train accuracy = 1.000000\n",
      "[2018-06-02 00:06:59.185179] Iteration 31900, train loss = 0.148576, train accuracy = 1.000000\n",
      "[2018-06-02 00:07:13.230179] Iteration 32000, train loss = 0.138229, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921800\n",
      "[2018-06-02 00:07:30.301179] Iteration 32100, train loss = 0.150211, train accuracy = 0.992188\n",
      "[2018-06-02 00:07:44.382179] Iteration 32200, train loss = 0.151164, train accuracy = 1.000000\n",
      "[2018-06-02 00:07:58.367179] Iteration 32300, train loss = 0.139773, train accuracy = 1.000000\n",
      "[2018-06-02 00:08:12.402179] Iteration 32400, train loss = 0.137814, train accuracy = 1.000000\n",
      "[2018-06-02 00:08:26.435179] Iteration 32500, train loss = 0.142237, train accuracy = 1.000000\n",
      "[2018-06-02 00:08:40.537179] Iteration 32600, train loss = 0.151592, train accuracy = 0.992188\n",
      "[2018-06-02 00:08:54.575179] Iteration 32700, train loss = 0.150238, train accuracy = 1.000000\n",
      "[2018-06-02 00:09:08.676179] Iteration 32800, train loss = 0.151271, train accuracy = 0.992188\n",
      "[2018-06-02 00:09:22.768179] Iteration 32900, train loss = 0.150737, train accuracy = 1.000000\n",
      "[2018-06-02 00:09:36.856179] Iteration 33000, train loss = 0.150704, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922300\n",
      "[2018-06-02 00:09:53.983179] Iteration 33100, train loss = 0.173382, train accuracy = 0.984375\n",
      "[2018-06-02 00:10:08.084179] Iteration 33200, train loss = 0.140530, train accuracy = 1.000000\n",
      "[2018-06-02 00:10:22.234179] Iteration 33300, train loss = 0.150910, train accuracy = 0.992188\n",
      "[2018-06-02 00:10:36.265179] Iteration 33400, train loss = 0.147413, train accuracy = 0.992188\n",
      "[2018-06-02 00:10:50.383179] Iteration 33500, train loss = 0.152713, train accuracy = 0.992188\n",
      "[2018-06-02 00:11:04.482179] Iteration 33600, train loss = 0.136809, train accuracy = 1.000000\n",
      "[2018-06-02 00:11:18.489179] Iteration 33700, train loss = 0.182946, train accuracy = 0.976562\n",
      "[2018-06-02 00:11:32.496179] Iteration 33800, train loss = 0.142279, train accuracy = 1.000000\n",
      "[2018-06-02 00:11:46.531179] Iteration 33900, train loss = 0.137906, train accuracy = 1.000000\n",
      "[2018-06-02 00:12:00.554179] Iteration 34000, train loss = 0.153583, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922100\n",
      "[2018-06-02 00:12:17.773179] Iteration 34100, train loss = 0.141266, train accuracy = 1.000000\n",
      "[2018-06-02 00:12:31.855179] Iteration 34200, train loss = 0.146805, train accuracy = 0.992188\n",
      "[2018-06-02 00:12:45.921179] Iteration 34300, train loss = 0.147014, train accuracy = 0.992188\n",
      "[2018-06-02 00:12:59.972179] Iteration 34400, train loss = 0.143143, train accuracy = 0.992188\n",
      "[2018-06-02 00:13:14.062179] Iteration 34500, train loss = 0.142101, train accuracy = 1.000000\n",
      "[2018-06-02 00:13:28.099179] Iteration 34600, train loss = 0.141137, train accuracy = 1.000000\n",
      "[2018-06-02 00:13:42.158179] Iteration 34700, train loss = 0.149997, train accuracy = 1.000000\n",
      "[2018-06-02 00:13:56.206179] Iteration 34800, train loss = 0.154927, train accuracy = 0.992188\n",
      "[2018-06-02 00:14:10.223179] Iteration 34900, train loss = 0.148864, train accuracy = 1.000000\n",
      "[2018-06-02 00:14:24.304179] Iteration 35000, train loss = 0.146143, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921400\n",
      "[2018-06-02 00:14:41.372179] Iteration 35100, train loss = 0.145415, train accuracy = 1.000000\n",
      "[2018-06-02 00:14:55.442179] Iteration 35200, train loss = 0.146689, train accuracy = 1.000000\n",
      "[2018-06-02 00:15:09.566179] Iteration 35300, train loss = 0.139383, train accuracy = 1.000000\n",
      "[2018-06-02 00:15:23.611179] Iteration 35400, train loss = 0.144686, train accuracy = 0.992188\n",
      "[2018-06-02 00:15:37.671179] Iteration 35500, train loss = 0.147045, train accuracy = 1.000000\n",
      "[2018-06-02 00:15:51.738179] Iteration 35600, train loss = 0.157916, train accuracy = 0.992188\n",
      "[2018-06-02 00:16:05.790179] Iteration 35700, train loss = 0.139555, train accuracy = 1.000000\n",
      "[2018-06-02 00:16:19.935179] Iteration 35800, train loss = 0.147497, train accuracy = 0.992188\n",
      "[2018-06-02 00:16:34.015179] Iteration 35900, train loss = 0.146082, train accuracy = 1.000000\n",
      "[2018-06-02 00:16:48.063179] Iteration 36000, train loss = 0.153966, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922600\n",
      "[2018-06-02 00:17:05.206179] Iteration 36100, train loss = 0.152007, train accuracy = 0.984375\n",
      "[2018-06-02 00:17:19.197179] Iteration 36200, train loss = 0.148440, train accuracy = 0.992188\n",
      "[2018-06-02 00:17:33.281179] Iteration 36300, train loss = 0.145786, train accuracy = 1.000000\n",
      "[2018-06-02 00:17:47.350179] Iteration 36400, train loss = 0.143383, train accuracy = 1.000000\n",
      "[2018-06-02 00:18:01.356179] Iteration 36500, train loss = 0.164681, train accuracy = 0.992188\n",
      "[2018-06-02 00:18:15.413179] Iteration 36600, train loss = 0.145710, train accuracy = 1.000000\n",
      "[2018-06-02 00:18:29.441179] Iteration 36700, train loss = 0.150623, train accuracy = 1.000000\n",
      "[2018-06-02 00:18:43.477179] Iteration 36800, train loss = 0.147189, train accuracy = 1.000000\n",
      "[2018-06-02 00:18:57.573179] Iteration 36900, train loss = 0.146395, train accuracy = 1.000000\n",
      "[2018-06-02 00:19:11.648179] Iteration 37000, train loss = 0.146943, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921900\n",
      "[2018-06-02 00:19:28.707179] Iteration 37100, train loss = 0.138020, train accuracy = 1.000000\n",
      "[2018-06-02 00:19:42.714179] Iteration 37200, train loss = 0.150087, train accuracy = 1.000000\n",
      "[2018-06-02 00:19:56.735179] Iteration 37300, train loss = 0.145671, train accuracy = 0.992188\n",
      "[2018-06-02 00:20:10.804179] Iteration 37400, train loss = 0.144806, train accuracy = 1.000000\n",
      "[2018-06-02 00:20:24.846179] Iteration 37500, train loss = 0.152908, train accuracy = 0.992188\n",
      "[2018-06-02 00:20:39.011179] Iteration 37600, train loss = 0.142305, train accuracy = 1.000000\n",
      "[2018-06-02 00:20:53.040179] Iteration 37700, train loss = 0.139067, train accuracy = 1.000000\n",
      "[2018-06-02 00:21:07.097179] Iteration 37800, train loss = 0.153113, train accuracy = 0.992188\n",
      "[2018-06-02 00:21:21.107179] Iteration 37900, train loss = 0.144495, train accuracy = 1.000000\n",
      "[2018-06-02 00:21:35.257179] Iteration 38000, train loss = 0.159638, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921800\n",
      "[2018-06-02 00:21:52.325179] Iteration 38100, train loss = 0.153226, train accuracy = 0.992188\n",
      "[2018-06-02 00:22:06.539179] Iteration 38200, train loss = 0.171133, train accuracy = 0.984375\n",
      "[2018-06-02 00:22:20.598179] Iteration 38300, train loss = 0.189772, train accuracy = 0.984375\n",
      "[2018-06-02 00:22:34.586179] Iteration 38400, train loss = 0.151942, train accuracy = 0.992188\n",
      "[2018-06-02 00:22:48.686179] Iteration 38500, train loss = 0.139658, train accuracy = 1.000000\n",
      "[2018-06-02 00:23:02.686179] Iteration 38600, train loss = 0.136616, train accuracy = 1.000000\n",
      "[2018-06-02 00:23:16.768179] Iteration 38700, train loss = 0.172025, train accuracy = 0.976562\n",
      "[2018-06-02 00:23:30.858179] Iteration 38800, train loss = 0.142067, train accuracy = 1.000000\n",
      "[2018-06-02 00:23:44.889179] Iteration 38900, train loss = 0.138807, train accuracy = 1.000000\n",
      "[2018-06-02 00:23:58.941179] Iteration 39000, train loss = 0.149898, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 00:24:16.010179] Iteration 39100, train loss = 0.153093, train accuracy = 1.000000\n",
      "[2018-06-02 00:24:30.078179] Iteration 39200, train loss = 0.146845, train accuracy = 1.000000\n",
      "[2018-06-02 00:24:44.135179] Iteration 39300, train loss = 0.176214, train accuracy = 0.984375\n",
      "[2018-06-02 00:24:58.165179] Iteration 39400, train loss = 0.162595, train accuracy = 0.992188\n",
      "[2018-06-02 00:25:12.188179] Iteration 39500, train loss = 0.137385, train accuracy = 1.000000\n",
      "[2018-06-02 00:25:26.229179] Iteration 39600, train loss = 0.157527, train accuracy = 0.992188\n",
      "[2018-06-02 00:25:40.262179] Iteration 39700, train loss = 0.173998, train accuracy = 0.984375\n",
      "[2018-06-02 00:25:54.371179] Iteration 39800, train loss = 0.145648, train accuracy = 1.000000\n",
      "[2018-06-02 00:26:08.442179] Iteration 39900, train loss = 0.139503, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "curr_lr = 0\n",
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二轮 量化\n",
    "prune_rate = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.921900\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.02971029 -0.0625      0.00994306  0.125      -0.0036665  -0.03125\n",
      "  0.01778514  0.03125     0.00959576 -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.75)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 00:28:18.548179] Iteration 100, train loss = 0.171240, train accuracy = 0.992188\n",
      "[2018-06-02 00:28:32.609179] Iteration 200, train loss = 0.147985, train accuracy = 1.000000\n",
      "[2018-06-02 00:28:46.641179] Iteration 300, train loss = 0.213451, train accuracy = 0.976562\n",
      "[2018-06-02 00:29:00.677179] Iteration 400, train loss = 0.208149, train accuracy = 0.960938\n",
      "[2018-06-02 00:29:14.780179] Iteration 500, train loss = 0.167801, train accuracy = 0.984375\n",
      "[2018-06-02 00:29:28.871179] Iteration 600, train loss = 0.176029, train accuracy = 0.992188\n",
      "[2018-06-02 00:29:42.856179] Iteration 700, train loss = 0.149790, train accuracy = 1.000000\n",
      "[2018-06-02 00:29:56.891179] Iteration 800, train loss = 0.174636, train accuracy = 0.984375\n",
      "[2018-06-02 00:30:10.993179] Iteration 900, train loss = 0.150018, train accuracy = 0.992188\n",
      "[2018-06-02 00:30:25.056179] Iteration 1000, train loss = 0.144637, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919200\n",
      "[2018-06-02 00:30:42.056179] Iteration 1100, train loss = 0.185859, train accuracy = 0.984375\n",
      "[2018-06-02 00:30:56.025179] Iteration 1200, train loss = 0.211411, train accuracy = 0.984375\n",
      "[2018-06-02 00:31:10.111179] Iteration 1300, train loss = 0.150395, train accuracy = 0.992188\n",
      "[2018-06-02 00:31:24.145179] Iteration 1400, train loss = 0.147300, train accuracy = 1.000000\n",
      "[2018-06-02 00:31:38.090179] Iteration 1500, train loss = 0.154662, train accuracy = 0.992188\n",
      "[2018-06-02 00:31:52.079179] Iteration 1600, train loss = 0.146154, train accuracy = 1.000000\n",
      "[2018-06-02 00:32:06.130179] Iteration 1700, train loss = 0.149589, train accuracy = 1.000000\n",
      "[2018-06-02 00:32:20.133179] Iteration 1800, train loss = 0.184967, train accuracy = 0.984375\n",
      "[2018-06-02 00:32:34.180179] Iteration 1900, train loss = 0.151820, train accuracy = 0.992188\n",
      "[2018-06-02 00:32:48.182179] Iteration 2000, train loss = 0.138822, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919500\n",
      "[2018-06-02 00:33:05.304179] Iteration 2100, train loss = 0.159519, train accuracy = 0.992188\n",
      "[2018-06-02 00:33:19.305179] Iteration 2200, train loss = 0.159149, train accuracy = 0.992188\n",
      "[2018-06-02 00:33:33.392179] Iteration 2300, train loss = 0.152476, train accuracy = 0.984375\n",
      "[2018-06-02 00:33:47.447179] Iteration 2400, train loss = 0.149286, train accuracy = 1.000000\n",
      "[2018-06-02 00:34:01.439179] Iteration 2500, train loss = 0.155111, train accuracy = 1.000000\n",
      "[2018-06-02 00:34:15.433179] Iteration 2600, train loss = 0.174886, train accuracy = 0.984375\n",
      "[2018-06-02 00:34:29.521179] Iteration 2700, train loss = 0.169411, train accuracy = 0.992188\n",
      "[2018-06-02 00:34:43.470179] Iteration 2800, train loss = 0.161303, train accuracy = 1.000000\n",
      "[2018-06-02 00:34:57.481179] Iteration 2900, train loss = 0.154653, train accuracy = 0.992188\n",
      "[2018-06-02 00:35:11.553179] Iteration 3000, train loss = 0.157718, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.919200\n",
      "[2018-06-02 00:35:28.598179] Iteration 3100, train loss = 0.154050, train accuracy = 0.984375\n",
      "[2018-06-02 00:35:42.631179] Iteration 3200, train loss = 0.160626, train accuracy = 0.984375\n",
      "[2018-06-02 00:35:56.765179] Iteration 3300, train loss = 0.163485, train accuracy = 0.976562\n",
      "[2018-06-02 00:36:10.795179] Iteration 3400, train loss = 0.166560, train accuracy = 0.984375\n",
      "[2018-06-02 00:36:24.810179] Iteration 3500, train loss = 0.143142, train accuracy = 1.000000\n",
      "[2018-06-02 00:36:38.787179] Iteration 3600, train loss = 0.157771, train accuracy = 1.000000\n",
      "[2018-06-02 00:36:52.918179] Iteration 3700, train loss = 0.140699, train accuracy = 1.000000\n",
      "[2018-06-02 00:37:07.014179] Iteration 3800, train loss = 0.182918, train accuracy = 0.992188\n",
      "[2018-06-02 00:37:21.000179] Iteration 3900, train loss = 0.143170, train accuracy = 1.000000\n",
      "[2018-06-02 00:37:35.001179] Iteration 4000, train loss = 0.176506, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920400\n",
      "[2018-06-02 00:37:52.054179] Iteration 4100, train loss = 0.169921, train accuracy = 0.976562\n",
      "[2018-06-02 00:38:06.071179] Iteration 4200, train loss = 0.145344, train accuracy = 1.000000\n",
      "[2018-06-02 00:38:20.061179] Iteration 4300, train loss = 0.143792, train accuracy = 1.000000\n",
      "[2018-06-02 00:38:34.142179] Iteration 4400, train loss = 0.152044, train accuracy = 0.992188\n",
      "[2018-06-02 00:38:48.228179] Iteration 4500, train loss = 0.177803, train accuracy = 0.984375\n",
      "[2018-06-02 00:39:02.251179] Iteration 4600, train loss = 0.150168, train accuracy = 0.992188\n",
      "[2018-06-02 00:39:16.391179] Iteration 4700, train loss = 0.147806, train accuracy = 0.992188\n",
      "[2018-06-02 00:39:30.437179] Iteration 4800, train loss = 0.173703, train accuracy = 0.992188\n",
      "[2018-06-02 00:39:44.449179] Iteration 4900, train loss = 0.154822, train accuracy = 0.992188\n",
      "[2018-06-02 00:39:58.527179] Iteration 5000, train loss = 0.152916, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920500\n",
      "[2018-06-02 00:40:15.681179] Iteration 5100, train loss = 0.150320, train accuracy = 1.000000\n",
      "[2018-06-02 00:40:29.674179] Iteration 5200, train loss = 0.193276, train accuracy = 0.976562\n",
      "[2018-06-02 00:40:43.625179] Iteration 5300, train loss = 0.145959, train accuracy = 1.000000\n",
      "[2018-06-02 00:40:57.654179] Iteration 5400, train loss = 0.189144, train accuracy = 0.976562\n",
      "[2018-06-02 00:41:11.739179] Iteration 5500, train loss = 0.142557, train accuracy = 1.000000\n",
      "[2018-06-02 00:41:25.762179] Iteration 5600, train loss = 0.141102, train accuracy = 1.000000\n",
      "[2018-06-02 00:41:39.748179] Iteration 5700, train loss = 0.145688, train accuracy = 1.000000\n",
      "[2018-06-02 00:41:53.727179] Iteration 5800, train loss = 0.158502, train accuracy = 0.984375\n",
      "[2018-06-02 00:42:07.779179] Iteration 5900, train loss = 0.174163, train accuracy = 0.992188\n",
      "[2018-06-02 00:42:21.840179] Iteration 6000, train loss = 0.164595, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920200\n",
      "[2018-06-02 00:42:38.947179] Iteration 6100, train loss = 0.159572, train accuracy = 0.992188\n",
      "[2018-06-02 00:42:52.969179] Iteration 6200, train loss = 0.137045, train accuracy = 1.000000\n",
      "[2018-06-02 00:43:07.012179] Iteration 6300, train loss = 0.159247, train accuracy = 0.992188\n",
      "[2018-06-02 00:43:21.056179] Iteration 6400, train loss = 0.148952, train accuracy = 0.992188\n",
      "[2018-06-02 00:43:35.073179] Iteration 6500, train loss = 0.149397, train accuracy = 1.000000\n",
      "[2018-06-02 00:43:49.173179] Iteration 6600, train loss = 0.165462, train accuracy = 0.992188\n",
      "[2018-06-02 00:44:03.161179] Iteration 6700, train loss = 0.141856, train accuracy = 1.000000\n",
      "[2018-06-02 00:44:17.201179] Iteration 6800, train loss = 0.146934, train accuracy = 1.000000\n",
      "[2018-06-02 00:44:31.243179] Iteration 6900, train loss = 0.165094, train accuracy = 0.984375\n",
      "[2018-06-02 00:44:45.271179] Iteration 7000, train loss = 0.190128, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920200\n",
      "[2018-06-02 00:45:02.332179] Iteration 7100, train loss = 0.159007, train accuracy = 0.992188\n",
      "[2018-06-02 00:45:16.327179] Iteration 7200, train loss = 0.144115, train accuracy = 1.000000\n",
      "[2018-06-02 00:45:30.361179] Iteration 7300, train loss = 0.141169, train accuracy = 1.000000\n",
      "[2018-06-02 00:45:44.428179] Iteration 7400, train loss = 0.161965, train accuracy = 0.992188\n",
      "[2018-06-02 00:45:58.520179] Iteration 7500, train loss = 0.156637, train accuracy = 0.992188\n",
      "[2018-06-02 00:46:12.607179] Iteration 7600, train loss = 0.139525, train accuracy = 1.000000\n",
      "[2018-06-02 00:46:26.693179] Iteration 7700, train loss = 0.159626, train accuracy = 1.000000\n",
      "[2018-06-02 00:46:40.699179] Iteration 7800, train loss = 0.158482, train accuracy = 0.984375\n",
      "[2018-06-02 00:46:54.756179] Iteration 7900, train loss = 0.158713, train accuracy = 1.000000\n",
      "[2018-06-02 00:47:08.831179] Iteration 8000, train loss = 0.150206, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-02 00:47:25.879179] Iteration 8100, train loss = 0.148493, train accuracy = 1.000000\n",
      "[2018-06-02 00:47:39.896179] Iteration 8200, train loss = 0.137401, train accuracy = 1.000000\n",
      "[2018-06-02 00:47:53.930179] Iteration 8300, train loss = 0.158716, train accuracy = 0.984375\n",
      "[2018-06-02 00:48:08.019179] Iteration 8400, train loss = 0.155316, train accuracy = 0.992188\n",
      "[2018-06-02 00:48:22.024179] Iteration 8500, train loss = 0.151524, train accuracy = 0.992188\n",
      "[2018-06-02 00:48:36.087179] Iteration 8600, train loss = 0.142254, train accuracy = 1.000000\n",
      "[2018-06-02 00:48:50.134179] Iteration 8700, train loss = 0.157401, train accuracy = 0.992188\n",
      "[2018-06-02 00:49:04.148179] Iteration 8800, train loss = 0.159277, train accuracy = 0.992188\n",
      "[2018-06-02 00:49:18.129179] Iteration 8900, train loss = 0.158766, train accuracy = 1.000000\n",
      "[2018-06-02 00:49:32.120179] Iteration 9000, train loss = 0.151785, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-02 00:49:49.193179] Iteration 9100, train loss = 0.159436, train accuracy = 0.992188\n",
      "[2018-06-02 00:50:03.176179] Iteration 9200, train loss = 0.150046, train accuracy = 0.992188\n",
      "[2018-06-02 00:50:17.190179] Iteration 9300, train loss = 0.145637, train accuracy = 1.000000\n",
      "[2018-06-02 00:50:31.160179] Iteration 9400, train loss = 0.158585, train accuracy = 0.992188\n",
      "[2018-06-02 00:50:45.176179] Iteration 9500, train loss = 0.146124, train accuracy = 1.000000\n",
      "[2018-06-02 00:50:59.211179] Iteration 9600, train loss = 0.156954, train accuracy = 0.992188\n",
      "[2018-06-02 00:51:13.243179] Iteration 9700, train loss = 0.170549, train accuracy = 0.976562\n",
      "[2018-06-02 00:51:27.325179] Iteration 9800, train loss = 0.162571, train accuracy = 0.992188\n",
      "[2018-06-02 00:51:41.330179] Iteration 9900, train loss = 0.147322, train accuracy = 1.000000\n",
      "[2018-06-02 00:51:55.353179] Iteration 10000, train loss = 0.158418, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921400\n",
      "[2018-06-02 00:52:12.436179] Iteration 10100, train loss = 0.145622, train accuracy = 1.000000\n",
      "[2018-06-02 00:52:26.452179] Iteration 10200, train loss = 0.145318, train accuracy = 1.000000\n",
      "[2018-06-02 00:52:40.532179] Iteration 10300, train loss = 0.151177, train accuracy = 0.992188\n",
      "[2018-06-02 00:52:54.528179] Iteration 10400, train loss = 0.142090, train accuracy = 1.000000\n",
      "[2018-06-02 00:53:08.544179] Iteration 10500, train loss = 0.143779, train accuracy = 1.000000\n",
      "[2018-06-02 00:53:22.568179] Iteration 10600, train loss = 0.138088, train accuracy = 1.000000\n",
      "[2018-06-02 00:53:36.628179] Iteration 10700, train loss = 0.178166, train accuracy = 0.976562\n",
      "[2018-06-02 00:53:50.714179] Iteration 10800, train loss = 0.154037, train accuracy = 0.992188\n",
      "[2018-06-02 00:54:04.767179] Iteration 10900, train loss = 0.140258, train accuracy = 1.000000\n",
      "[2018-06-02 00:54:18.800179] Iteration 11000, train loss = 0.164254, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 00:54:35.831179] Iteration 11100, train loss = 0.157381, train accuracy = 0.992188\n",
      "[2018-06-02 00:54:49.900179] Iteration 11200, train loss = 0.150439, train accuracy = 1.000000\n",
      "[2018-06-02 00:55:03.936179] Iteration 11300, train loss = 0.139626, train accuracy = 1.000000\n",
      "[2018-06-02 00:55:17.982179] Iteration 11400, train loss = 0.183530, train accuracy = 0.992188\n",
      "[2018-06-02 00:55:32.070179] Iteration 11500, train loss = 0.139679, train accuracy = 1.000000\n",
      "[2018-06-02 00:55:46.065179] Iteration 11600, train loss = 0.140093, train accuracy = 1.000000\n",
      "[2018-06-02 00:56:00.107179] Iteration 11700, train loss = 0.152038, train accuracy = 1.000000\n",
      "[2018-06-02 00:56:14.219179] Iteration 11800, train loss = 0.142333, train accuracy = 1.000000\n",
      "[2018-06-02 00:56:28.234179] Iteration 11900, train loss = 0.156529, train accuracy = 0.992188\n",
      "[2018-06-02 00:56:42.310179] Iteration 12000, train loss = 0.155003, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922000\n",
      "[2018-06-02 00:56:59.332179] Iteration 12100, train loss = 0.144266, train accuracy = 1.000000\n",
      "[2018-06-02 00:57:13.405179] Iteration 12200, train loss = 0.157790, train accuracy = 0.992188\n",
      "[2018-06-02 00:57:27.365179] Iteration 12300, train loss = 0.162443, train accuracy = 0.992188\n",
      "[2018-06-02 00:57:41.378179] Iteration 12400, train loss = 0.145212, train accuracy = 1.000000\n",
      "[2018-06-02 00:57:55.353179] Iteration 12500, train loss = 0.163405, train accuracy = 0.984375\n",
      "[2018-06-02 00:58:09.392179] Iteration 12600, train loss = 0.165576, train accuracy = 0.984375\n",
      "[2018-06-02 00:58:23.472179] Iteration 12700, train loss = 0.138323, train accuracy = 1.000000\n",
      "[2018-06-02 00:58:37.495179] Iteration 12800, train loss = 0.143767, train accuracy = 1.000000\n",
      "[2018-06-02 00:58:51.562179] Iteration 12900, train loss = 0.145639, train accuracy = 0.992188\n",
      "[2018-06-02 00:59:05.662179] Iteration 13000, train loss = 0.157268, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922000\n",
      "[2018-06-02 00:59:22.697179] Iteration 13100, train loss = 0.146615, train accuracy = 0.992188\n",
      "[2018-06-02 00:59:36.769179] Iteration 13200, train loss = 0.146149, train accuracy = 1.000000\n",
      "[2018-06-02 00:59:50.781179] Iteration 13300, train loss = 0.158044, train accuracy = 0.992188\n",
      "[2018-06-02 01:00:04.843179] Iteration 13400, train loss = 0.144420, train accuracy = 1.000000\n",
      "[2018-06-02 01:00:18.958179] Iteration 13500, train loss = 0.139088, train accuracy = 1.000000\n",
      "[2018-06-02 01:00:32.956179] Iteration 13600, train loss = 0.154132, train accuracy = 0.992188\n",
      "[2018-06-02 01:00:46.970179] Iteration 13700, train loss = 0.163829, train accuracy = 0.992188\n",
      "[2018-06-02 01:01:01.072179] Iteration 13800, train loss = 0.163423, train accuracy = 0.984375\n",
      "[2018-06-02 01:01:15.090179] Iteration 13900, train loss = 0.142314, train accuracy = 1.000000\n",
      "[2018-06-02 01:01:29.120179] Iteration 14000, train loss = 0.153689, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921200\n",
      "[2018-06-02 01:01:46.128179] Iteration 14100, train loss = 0.151820, train accuracy = 1.000000\n",
      "[2018-06-02 01:02:00.257179] Iteration 14200, train loss = 0.153612, train accuracy = 0.992188\n",
      "[2018-06-02 01:02:14.358179] Iteration 14300, train loss = 0.139907, train accuracy = 1.000000\n",
      "[2018-06-02 01:02:28.417179] Iteration 14400, train loss = 0.168515, train accuracy = 0.984375\n",
      "[2018-06-02 01:02:42.375179] Iteration 14500, train loss = 0.155751, train accuracy = 0.992188\n",
      "[2018-06-02 01:02:56.433179] Iteration 14600, train loss = 0.162690, train accuracy = 0.984375\n",
      "[2018-06-02 01:03:10.555179] Iteration 14700, train loss = 0.150243, train accuracy = 0.992188\n",
      "[2018-06-02 01:03:24.503179] Iteration 14800, train loss = 0.159914, train accuracy = 0.984375\n",
      "[2018-06-02 01:03:38.490179] Iteration 14900, train loss = 0.160423, train accuracy = 0.992188\n",
      "[2018-06-02 01:03:52.542179] Iteration 15000, train loss = 0.137741, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-02 01:04:09.638179] Iteration 15100, train loss = 0.151188, train accuracy = 1.000000\n",
      "[2018-06-02 01:04:23.578179] Iteration 15200, train loss = 0.136906, train accuracy = 1.000000\n",
      "[2018-06-02 01:04:37.649179] Iteration 15300, train loss = 0.155313, train accuracy = 0.992188\n",
      "[2018-06-02 01:04:51.653179] Iteration 15400, train loss = 0.138698, train accuracy = 1.000000\n",
      "[2018-06-02 01:05:05.661179] Iteration 15500, train loss = 0.142402, train accuracy = 1.000000\n",
      "[2018-06-02 01:05:19.647179] Iteration 15600, train loss = 0.156388, train accuracy = 0.992188\n",
      "[2018-06-02 01:05:33.761179] Iteration 15700, train loss = 0.170078, train accuracy = 0.984375\n",
      "[2018-06-02 01:05:47.787179] Iteration 15800, train loss = 0.171591, train accuracy = 0.984375\n",
      "[2018-06-02 01:06:01.828179] Iteration 15900, train loss = 0.138264, train accuracy = 1.000000\n",
      "[2018-06-02 01:06:15.827179] Iteration 16000, train loss = 0.141155, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922300\n",
      "[2018-06-02 01:06:32.857179] Iteration 16100, train loss = 0.161006, train accuracy = 0.992188\n",
      "[2018-06-02 01:06:46.916179] Iteration 16200, train loss = 0.137825, train accuracy = 1.000000\n",
      "[2018-06-02 01:07:01.029179] Iteration 16300, train loss = 0.149620, train accuracy = 0.992188\n",
      "[2018-06-02 01:07:15.127179] Iteration 16400, train loss = 0.151918, train accuracy = 0.992188\n",
      "[2018-06-02 01:07:29.112179] Iteration 16500, train loss = 0.150597, train accuracy = 1.000000\n",
      "[2018-06-02 01:07:43.146179] Iteration 16600, train loss = 0.143703, train accuracy = 1.000000\n",
      "[2018-06-02 01:07:57.203179] Iteration 16700, train loss = 0.147242, train accuracy = 1.000000\n",
      "[2018-06-02 01:08:11.204179] Iteration 16800, train loss = 0.139102, train accuracy = 1.000000\n",
      "[2018-06-02 01:08:25.251179] Iteration 16900, train loss = 0.150881, train accuracy = 0.992188\n",
      "[2018-06-02 01:08:39.382179] Iteration 17000, train loss = 0.153840, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922800\n",
      "[2018-06-02 01:08:56.410179] Iteration 17100, train loss = 0.151331, train accuracy = 0.992188\n",
      "[2018-06-02 01:09:10.494179] Iteration 17200, train loss = 0.147317, train accuracy = 0.992188\n",
      "[2018-06-02 01:09:24.501179] Iteration 17300, train loss = 0.171349, train accuracy = 0.976562\n",
      "[2018-06-02 01:09:38.500179] Iteration 17400, train loss = 0.137297, train accuracy = 1.000000\n",
      "[2018-06-02 01:09:52.548179] Iteration 17500, train loss = 0.150220, train accuracy = 0.992188\n",
      "[2018-06-02 01:10:06.545179] Iteration 17600, train loss = 0.153429, train accuracy = 1.000000\n",
      "[2018-06-02 01:10:20.596179] Iteration 17700, train loss = 0.148537, train accuracy = 1.000000\n",
      "[2018-06-02 01:10:34.638179] Iteration 17800, train loss = 0.147960, train accuracy = 1.000000\n",
      "[2018-06-02 01:10:48.717179] Iteration 17900, train loss = 0.156845, train accuracy = 0.992188\n",
      "[2018-06-02 01:11:02.722179] Iteration 18000, train loss = 0.141421, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 01:11:19.751179] Iteration 18100, train loss = 0.155159, train accuracy = 1.000000\n",
      "[2018-06-02 01:11:33.790179] Iteration 18200, train loss = 0.172249, train accuracy = 0.968750\n",
      "[2018-06-02 01:11:47.863179] Iteration 18300, train loss = 0.148031, train accuracy = 1.000000\n",
      "[2018-06-02 01:12:01.894179] Iteration 18400, train loss = 0.145728, train accuracy = 1.000000\n",
      "[2018-06-02 01:12:15.890179] Iteration 18500, train loss = 0.174175, train accuracy = 0.984375\n",
      "[2018-06-02 01:12:29.985179] Iteration 18600, train loss = 0.159207, train accuracy = 1.000000\n",
      "[2018-06-02 01:12:44.015179] Iteration 18700, train loss = 0.139254, train accuracy = 1.000000\n",
      "[2018-06-02 01:12:58.053179] Iteration 18800, train loss = 0.139715, train accuracy = 1.000000\n",
      "[2018-06-02 01:13:12.101179] Iteration 18900, train loss = 0.144042, train accuracy = 1.000000\n",
      "[2018-06-02 01:13:26.207179] Iteration 19000, train loss = 0.191669, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.922100\n",
      "[2018-06-02 01:13:43.312179] Iteration 19100, train loss = 0.149842, train accuracy = 0.992188\n",
      "[2018-06-02 01:13:57.352179] Iteration 19200, train loss = 0.158575, train accuracy = 0.992188\n",
      "[2018-06-02 01:14:11.402179] Iteration 19300, train loss = 0.142073, train accuracy = 1.000000\n",
      "[2018-06-02 01:14:25.408179] Iteration 19400, train loss = 0.142978, train accuracy = 1.000000\n",
      "[2018-06-02 01:14:39.472179] Iteration 19500, train loss = 0.159301, train accuracy = 1.000000\n",
      "[2018-06-02 01:14:53.540179] Iteration 19600, train loss = 0.197031, train accuracy = 0.984375\n",
      "[2018-06-02 01:15:07.591179] Iteration 19700, train loss = 0.173812, train accuracy = 0.976562\n",
      "[2018-06-02 01:15:21.642179] Iteration 19800, train loss = 0.143155, train accuracy = 1.000000\n",
      "[2018-06-02 01:15:35.678179] Iteration 19900, train loss = 0.143837, train accuracy = 1.000000\n",
      "[2018-06-02 01:15:49.802179] Iteration 20000, train loss = 0.144664, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922000\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 01:16:06.913179] Iteration 20100, train loss = 0.150689, train accuracy = 0.992188\n",
      "[2018-06-02 01:16:20.921179] Iteration 20200, train loss = 0.149806, train accuracy = 0.992188\n",
      "[2018-06-02 01:16:34.902179] Iteration 20300, train loss = 0.171471, train accuracy = 0.984375\n",
      "[2018-06-02 01:16:48.970179] Iteration 20400, train loss = 0.148160, train accuracy = 0.992188\n",
      "[2018-06-02 01:17:03.035179] Iteration 20500, train loss = 0.145616, train accuracy = 1.000000\n",
      "[2018-06-02 01:17:17.065179] Iteration 20600, train loss = 0.147238, train accuracy = 1.000000\n",
      "[2018-06-02 01:17:31.158179] Iteration 20700, train loss = 0.149311, train accuracy = 1.000000\n",
      "[2018-06-02 01:17:45.221179] Iteration 20800, train loss = 0.166510, train accuracy = 0.984375\n",
      "[2018-06-02 01:17:59.281179] Iteration 20900, train loss = 0.178842, train accuracy = 0.984375\n",
      "[2018-06-02 01:18:13.316179] Iteration 21000, train loss = 0.162064, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 01:18:30.283179] Iteration 21100, train loss = 0.142319, train accuracy = 1.000000\n",
      "[2018-06-02 01:18:44.381179] Iteration 21200, train loss = 0.170123, train accuracy = 0.984375\n",
      "[2018-06-02 01:18:58.312179] Iteration 21300, train loss = 0.154627, train accuracy = 0.992188\n",
      "[2018-06-02 01:19:12.289179] Iteration 21400, train loss = 0.172844, train accuracy = 0.992188\n",
      "[2018-06-02 01:19:26.298179] Iteration 21500, train loss = 0.144632, train accuracy = 1.000000\n",
      "[2018-06-02 01:19:40.333179] Iteration 21600, train loss = 0.145346, train accuracy = 1.000000\n",
      "[2018-06-02 01:19:54.362179] Iteration 21700, train loss = 0.168085, train accuracy = 0.992188\n",
      "[2018-06-02 01:20:08.454179] Iteration 21800, train loss = 0.136017, train accuracy = 1.000000\n",
      "[2018-06-02 01:20:22.464179] Iteration 21900, train loss = 0.149040, train accuracy = 1.000000\n",
      "[2018-06-02 01:20:36.580179] Iteration 22000, train loss = 0.140743, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 01:20:53.662179] Iteration 22100, train loss = 0.153367, train accuracy = 0.992188\n",
      "[2018-06-02 01:21:07.756179] Iteration 22200, train loss = 0.145510, train accuracy = 0.992188\n",
      "[2018-06-02 01:21:21.733179] Iteration 22300, train loss = 0.142950, train accuracy = 1.000000\n",
      "[2018-06-02 01:21:35.755179] Iteration 22400, train loss = 0.158735, train accuracy = 0.984375\n",
      "[2018-06-02 01:21:49.758179] Iteration 22500, train loss = 0.149524, train accuracy = 1.000000\n",
      "[2018-06-02 01:22:03.790179] Iteration 22600, train loss = 0.142354, train accuracy = 1.000000\n",
      "[2018-06-02 01:22:17.854179] Iteration 22700, train loss = 0.146548, train accuracy = 1.000000\n",
      "[2018-06-02 01:22:31.877179] Iteration 22800, train loss = 0.145107, train accuracy = 1.000000\n",
      "[2018-06-02 01:22:45.894179] Iteration 22900, train loss = 0.153360, train accuracy = 0.992188\n",
      "[2018-06-02 01:22:59.986179] Iteration 23000, train loss = 0.164343, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922200\n",
      "[2018-06-02 01:23:17.002179] Iteration 23100, train loss = 0.147124, train accuracy = 1.000000\n",
      "[2018-06-02 01:23:30.997179] Iteration 23200, train loss = 0.138632, train accuracy = 1.000000\n",
      "[2018-06-02 01:23:45.077179] Iteration 23300, train loss = 0.140350, train accuracy = 1.000000\n",
      "[2018-06-02 01:23:59.108179] Iteration 23400, train loss = 0.165566, train accuracy = 0.984375\n",
      "[2018-06-02 01:24:13.138179] Iteration 23500, train loss = 0.165812, train accuracy = 0.984375\n",
      "[2018-06-02 01:24:27.132179] Iteration 23600, train loss = 0.150088, train accuracy = 0.992188\n",
      "[2018-06-02 01:24:41.150179] Iteration 23700, train loss = 0.152483, train accuracy = 0.992188\n",
      "[2018-06-02 01:24:55.130179] Iteration 23800, train loss = 0.154760, train accuracy = 0.992188\n",
      "[2018-06-02 01:25:09.070179] Iteration 23900, train loss = 0.177120, train accuracy = 0.984375\n",
      "[2018-06-02 01:25:23.066179] Iteration 24000, train loss = 0.180541, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.922700\n",
      "[2018-06-02 01:25:40.093179] Iteration 24100, train loss = 0.146397, train accuracy = 0.992188\n",
      "[2018-06-02 01:25:54.124179] Iteration 24200, train loss = 0.163296, train accuracy = 0.984375\n",
      "[2018-06-02 01:26:08.125179] Iteration 24300, train loss = 0.159364, train accuracy = 0.984375\n",
      "[2018-06-02 01:26:22.240179] Iteration 24400, train loss = 0.149893, train accuracy = 1.000000\n",
      "[2018-06-02 01:26:36.302179] Iteration 24500, train loss = 0.168211, train accuracy = 0.992188\n",
      "[2018-06-02 01:26:50.390179] Iteration 24600, train loss = 0.137553, train accuracy = 1.000000\n",
      "[2018-06-02 01:27:04.371179] Iteration 24700, train loss = 0.155594, train accuracy = 0.992188\n",
      "[2018-06-02 01:27:18.359179] Iteration 24800, train loss = 0.174634, train accuracy = 0.992188\n",
      "[2018-06-02 01:27:32.351179] Iteration 24900, train loss = 0.154972, train accuracy = 0.992188\n",
      "[2018-06-02 01:27:46.367179] Iteration 25000, train loss = 0.144367, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922300\n",
      "[2018-06-02 01:28:03.425179] Iteration 25100, train loss = 0.144719, train accuracy = 0.992188\n",
      "[2018-06-02 01:28:17.468179] Iteration 25200, train loss = 0.147045, train accuracy = 1.000000\n",
      "[2018-06-02 01:28:31.511179] Iteration 25300, train loss = 0.150101, train accuracy = 1.000000\n",
      "[2018-06-02 01:28:45.512179] Iteration 25400, train loss = 0.141376, train accuracy = 1.000000\n",
      "[2018-06-02 01:28:59.494179] Iteration 25500, train loss = 0.145264, train accuracy = 1.000000\n",
      "[2018-06-02 01:29:13.476179] Iteration 25600, train loss = 0.140127, train accuracy = 1.000000\n",
      "[2018-06-02 01:29:27.535179] Iteration 25700, train loss = 0.149909, train accuracy = 0.992188\n",
      "[2018-06-02 01:29:41.562179] Iteration 25800, train loss = 0.144125, train accuracy = 1.000000\n",
      "[2018-06-02 01:29:55.533179] Iteration 25900, train loss = 0.148844, train accuracy = 1.000000\n",
      "[2018-06-02 01:30:09.606179] Iteration 26000, train loss = 0.148963, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922700\n",
      "[2018-06-02 01:30:26.700179] Iteration 26100, train loss = 0.139119, train accuracy = 1.000000\n",
      "[2018-06-02 01:30:40.732179] Iteration 26200, train loss = 0.146786, train accuracy = 1.000000\n",
      "[2018-06-02 01:30:54.720179] Iteration 26300, train loss = 0.136889, train accuracy = 1.000000\n",
      "[2018-06-02 01:31:08.774179] Iteration 26400, train loss = 0.167149, train accuracy = 0.984375\n",
      "[2018-06-02 01:31:22.853179] Iteration 26500, train loss = 0.151202, train accuracy = 1.000000\n",
      "[2018-06-02 01:31:36.924179] Iteration 26600, train loss = 0.155382, train accuracy = 1.000000\n",
      "[2018-06-02 01:31:50.997179] Iteration 26700, train loss = 0.152777, train accuracy = 0.992188\n",
      "[2018-06-02 01:32:04.978179] Iteration 26800, train loss = 0.164564, train accuracy = 0.984375\n",
      "[2018-06-02 01:32:19.070179] Iteration 26900, train loss = 0.146827, train accuracy = 0.992188\n",
      "[2018-06-02 01:32:33.105179] Iteration 27000, train loss = 0.137791, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922800\n",
      "[2018-06-02 01:32:50.180179] Iteration 27100, train loss = 0.155282, train accuracy = 0.992188\n",
      "[2018-06-02 01:33:04.213179] Iteration 27200, train loss = 0.146184, train accuracy = 1.000000\n",
      "[2018-06-02 01:33:18.316179] Iteration 27300, train loss = 0.146059, train accuracy = 1.000000\n",
      "[2018-06-02 01:33:32.357179] Iteration 27400, train loss = 0.149187, train accuracy = 0.992188\n",
      "[2018-06-02 01:33:46.378179] Iteration 27500, train loss = 0.150229, train accuracy = 0.992188\n",
      "[2018-06-02 01:34:00.367179] Iteration 27600, train loss = 0.142598, train accuracy = 1.000000\n",
      "[2018-06-02 01:34:14.345179] Iteration 27700, train loss = 0.138469, train accuracy = 1.000000\n",
      "[2018-06-02 01:34:28.430179] Iteration 27800, train loss = 0.160010, train accuracy = 0.992188\n",
      "[2018-06-02 01:34:42.425179] Iteration 27900, train loss = 0.152400, train accuracy = 0.992188\n",
      "[2018-06-02 01:34:56.479179] Iteration 28000, train loss = 0.160095, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922500\n",
      "[2018-06-02 01:35:13.516179] Iteration 28100, train loss = 0.147407, train accuracy = 1.000000\n",
      "[2018-06-02 01:35:27.517179] Iteration 28200, train loss = 0.147951, train accuracy = 1.000000\n",
      "[2018-06-02 01:35:41.543179] Iteration 28300, train loss = 0.147439, train accuracy = 1.000000\n",
      "[2018-06-02 01:35:55.612179] Iteration 28400, train loss = 0.161638, train accuracy = 0.992188\n",
      "[2018-06-02 01:36:09.603179] Iteration 28500, train loss = 0.148823, train accuracy = 1.000000\n",
      "[2018-06-02 01:36:23.680179] Iteration 28600, train loss = 0.155546, train accuracy = 0.992188\n",
      "[2018-06-02 01:36:37.757179] Iteration 28700, train loss = 0.156395, train accuracy = 0.992188\n",
      "[2018-06-02 01:36:51.757179] Iteration 28800, train loss = 0.163664, train accuracy = 0.984375\n",
      "[2018-06-02 01:37:05.818179] Iteration 28900, train loss = 0.145704, train accuracy = 1.000000\n",
      "[2018-06-02 01:37:19.805179] Iteration 29000, train loss = 0.157337, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922100\n",
      "[2018-06-02 01:37:36.871179] Iteration 29100, train loss = 0.148738, train accuracy = 0.992188\n",
      "[2018-06-02 01:37:50.890179] Iteration 29200, train loss = 0.156492, train accuracy = 0.992188\n",
      "[2018-06-02 01:38:04.935179] Iteration 29300, train loss = 0.140433, train accuracy = 1.000000\n",
      "[2018-06-02 01:38:18.989179] Iteration 29400, train loss = 0.148086, train accuracy = 0.992188\n",
      "[2018-06-02 01:38:33.045179] Iteration 29500, train loss = 0.141107, train accuracy = 1.000000\n",
      "[2018-06-02 01:38:47.043179] Iteration 29600, train loss = 0.155450, train accuracy = 0.992188\n",
      "[2018-06-02 01:39:01.014179] Iteration 29700, train loss = 0.161934, train accuracy = 0.984375\n",
      "[2018-06-02 01:39:15.066179] Iteration 29800, train loss = 0.152471, train accuracy = 0.984375\n",
      "[2018-06-02 01:39:29.130179] Iteration 29900, train loss = 0.143846, train accuracy = 1.000000\n",
      "[2018-06-02 01:39:43.219179] Iteration 30000, train loss = 0.142178, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922500\n",
      "[2018-06-02 01:40:00.305179] Iteration 30100, train loss = 0.139998, train accuracy = 1.000000\n",
      "[2018-06-02 01:40:14.317179] Iteration 30200, train loss = 0.159979, train accuracy = 0.992188\n",
      "[2018-06-02 01:40:28.334179] Iteration 30300, train loss = 0.144395, train accuracy = 1.000000\n",
      "[2018-06-02 01:40:42.370179] Iteration 30400, train loss = 0.150000, train accuracy = 1.000000\n",
      "[2018-06-02 01:40:56.424179] Iteration 30500, train loss = 0.140659, train accuracy = 1.000000\n",
      "[2018-06-02 01:41:10.463179] Iteration 30600, train loss = 0.161973, train accuracy = 0.992188\n",
      "[2018-06-02 01:41:24.480179] Iteration 30700, train loss = 0.155146, train accuracy = 1.000000\n",
      "[2018-06-02 01:41:38.538179] Iteration 30800, train loss = 0.142095, train accuracy = 1.000000\n",
      "[2018-06-02 01:41:52.617179] Iteration 30900, train loss = 0.160897, train accuracy = 0.992188\n",
      "[2018-06-02 01:42:06.704179] Iteration 31000, train loss = 0.136862, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922000\n",
      "[2018-06-02 01:42:23.805179] Iteration 31100, train loss = 0.142117, train accuracy = 1.000000\n",
      "[2018-06-02 01:42:37.770179] Iteration 31200, train loss = 0.162841, train accuracy = 0.992188\n",
      "[2018-06-02 01:42:51.828179] Iteration 31300, train loss = 0.164122, train accuracy = 0.992188\n",
      "[2018-06-02 01:43:05.924179] Iteration 31400, train loss = 0.141385, train accuracy = 1.000000\n",
      "[2018-06-02 01:43:19.951179] Iteration 31500, train loss = 0.147688, train accuracy = 0.992188\n",
      "[2018-06-02 01:43:34.055179] Iteration 31600, train loss = 0.148823, train accuracy = 1.000000\n",
      "[2018-06-02 01:43:48.171179] Iteration 31700, train loss = 0.168859, train accuracy = 0.984375\n",
      "[2018-06-02 01:44:02.254179] Iteration 31800, train loss = 0.146121, train accuracy = 1.000000\n",
      "[2018-06-02 01:44:16.280179] Iteration 31900, train loss = 0.162241, train accuracy = 0.992188\n",
      "[2018-06-02 01:44:30.285179] Iteration 32000, train loss = 0.156265, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922300\n",
      "[2018-06-02 01:44:47.297179] Iteration 32100, train loss = 0.142285, train accuracy = 1.000000\n",
      "[2018-06-02 01:45:01.369179] Iteration 32200, train loss = 0.141982, train accuracy = 1.000000\n",
      "[2018-06-02 01:45:15.578179] Iteration 32300, train loss = 0.137234, train accuracy = 1.000000\n",
      "[2018-06-02 01:45:29.662179] Iteration 32400, train loss = 0.148574, train accuracy = 0.992188\n",
      "[2018-06-02 01:45:43.747179] Iteration 32500, train loss = 0.158620, train accuracy = 0.992188\n",
      "[2018-06-02 01:45:57.755179] Iteration 32600, train loss = 0.150196, train accuracy = 1.000000\n",
      "[2018-06-02 01:46:11.760179] Iteration 32700, train loss = 0.166161, train accuracy = 0.992188\n",
      "[2018-06-02 01:46:25.825179] Iteration 32800, train loss = 0.159024, train accuracy = 0.992188\n",
      "[2018-06-02 01:46:39.899179] Iteration 32900, train loss = 0.144097, train accuracy = 1.000000\n",
      "[2018-06-02 01:46:54.000179] Iteration 33000, train loss = 0.143144, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922400\n",
      "[2018-06-02 01:47:11.262179] Iteration 33100, train loss = 0.142720, train accuracy = 1.000000\n",
      "[2018-06-02 01:47:25.277179] Iteration 33200, train loss = 0.143044, train accuracy = 1.000000\n",
      "[2018-06-02 01:47:39.269179] Iteration 33300, train loss = 0.156582, train accuracy = 0.984375\n",
      "[2018-06-02 01:47:53.281179] Iteration 33400, train loss = 0.156928, train accuracy = 0.992188\n",
      "[2018-06-02 01:48:07.285179] Iteration 33500, train loss = 0.137346, train accuracy = 1.000000\n",
      "[2018-06-02 01:48:21.262179] Iteration 33600, train loss = 0.148234, train accuracy = 1.000000\n",
      "[2018-06-02 01:48:35.284179] Iteration 33700, train loss = 0.159648, train accuracy = 0.992188\n",
      "[2018-06-02 01:48:49.282179] Iteration 33800, train loss = 0.161717, train accuracy = 0.992188\n",
      "[2018-06-02 01:49:03.355179] Iteration 33900, train loss = 0.156814, train accuracy = 0.992188\n",
      "[2018-06-02 01:49:17.456179] Iteration 34000, train loss = 0.149326, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921600\n",
      "[2018-06-02 01:49:34.514179] Iteration 34100, train loss = 0.160372, train accuracy = 0.992188\n",
      "[2018-06-02 01:49:48.499179] Iteration 34200, train loss = 0.142365, train accuracy = 1.000000\n",
      "[2018-06-02 01:50:02.561179] Iteration 34300, train loss = 0.140857, train accuracy = 1.000000\n",
      "[2018-06-02 01:50:16.639179] Iteration 34400, train loss = 0.153366, train accuracy = 1.000000\n",
      "[2018-06-02 01:50:30.685179] Iteration 34500, train loss = 0.147215, train accuracy = 1.000000\n",
      "[2018-06-02 01:50:44.765179] Iteration 34600, train loss = 0.157215, train accuracy = 0.984375\n",
      "[2018-06-02 01:50:58.772179] Iteration 34700, train loss = 0.151262, train accuracy = 0.992188\n",
      "[2018-06-02 01:51:12.805179] Iteration 34800, train loss = 0.139260, train accuracy = 1.000000\n",
      "[2018-06-02 01:51:26.860179] Iteration 34900, train loss = 0.182244, train accuracy = 0.984375\n",
      "[2018-06-02 01:51:40.852179] Iteration 35000, train loss = 0.159601, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922500\n",
      "[2018-06-02 01:51:57.911179] Iteration 35100, train loss = 0.141687, train accuracy = 1.000000\n",
      "[2018-06-02 01:52:11.949179] Iteration 35200, train loss = 0.157628, train accuracy = 0.992188\n",
      "[2018-06-02 01:52:25.943179] Iteration 35300, train loss = 0.150037, train accuracy = 0.992188\n",
      "[2018-06-02 01:52:39.956179] Iteration 35400, train loss = 0.162310, train accuracy = 0.992188\n",
      "[2018-06-02 01:52:53.977179] Iteration 35500, train loss = 0.154290, train accuracy = 1.000000\n",
      "[2018-06-02 01:53:07.995179] Iteration 35600, train loss = 0.154890, train accuracy = 0.992188\n",
      "[2018-06-02 01:53:22.016179] Iteration 35700, train loss = 0.176237, train accuracy = 0.976562\n",
      "[2018-06-02 01:53:36.038179] Iteration 35800, train loss = 0.140917, train accuracy = 1.000000\n",
      "[2018-06-02 01:53:50.040179] Iteration 35900, train loss = 0.146000, train accuracy = 1.000000\n",
      "[2018-06-02 01:54:04.162179] Iteration 36000, train loss = 0.147352, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921800\n",
      "[2018-06-02 01:54:21.311179] Iteration 36100, train loss = 0.145070, train accuracy = 1.000000\n",
      "[2018-06-02 01:54:35.398179] Iteration 36200, train loss = 0.145735, train accuracy = 1.000000\n",
      "[2018-06-02 01:54:49.478179] Iteration 36300, train loss = 0.146459, train accuracy = 1.000000\n",
      "[2018-06-02 01:55:03.501179] Iteration 36400, train loss = 0.145654, train accuracy = 1.000000\n",
      "[2018-06-02 01:55:17.554179] Iteration 36500, train loss = 0.138033, train accuracy = 1.000000\n",
      "[2018-06-02 01:55:31.591179] Iteration 36600, train loss = 0.219288, train accuracy = 0.968750\n",
      "[2018-06-02 01:55:45.648179] Iteration 36700, train loss = 0.159087, train accuracy = 1.000000\n",
      "[2018-06-02 01:55:59.652179] Iteration 36800, train loss = 0.138041, train accuracy = 1.000000\n",
      "[2018-06-02 01:56:13.674179] Iteration 36900, train loss = 0.140530, train accuracy = 1.000000\n",
      "[2018-06-02 01:56:27.713179] Iteration 37000, train loss = 0.171967, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.922400\n",
      "[2018-06-02 01:56:44.749179] Iteration 37100, train loss = 0.143399, train accuracy = 1.000000\n",
      "[2018-06-02 01:56:58.759179] Iteration 37200, train loss = 0.147547, train accuracy = 1.000000\n",
      "[2018-06-02 01:57:12.802179] Iteration 37300, train loss = 0.150136, train accuracy = 0.992188\n",
      "[2018-06-02 01:57:26.815179] Iteration 37400, train loss = 0.148944, train accuracy = 0.992188\n",
      "[2018-06-02 01:57:40.796179] Iteration 37500, train loss = 0.163116, train accuracy = 0.992188\n",
      "[2018-06-02 01:57:54.851179] Iteration 37600, train loss = 0.182411, train accuracy = 0.976562\n",
      "[2018-06-02 01:58:08.880179] Iteration 37700, train loss = 0.176446, train accuracy = 0.984375\n",
      "[2018-06-02 01:58:22.959179] Iteration 37800, train loss = 0.148652, train accuracy = 0.992188\n",
      "[2018-06-02 01:58:36.972179] Iteration 37900, train loss = 0.139266, train accuracy = 1.000000\n",
      "[2018-06-02 01:58:51.007179] Iteration 38000, train loss = 0.141808, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922100\n",
      "[2018-06-02 01:59:08.015179] Iteration 38100, train loss = 0.150401, train accuracy = 0.992188\n",
      "[2018-06-02 01:59:22.085179] Iteration 38200, train loss = 0.146484, train accuracy = 1.000000\n",
      "[2018-06-02 01:59:36.112179] Iteration 38300, train loss = 0.145347, train accuracy = 1.000000\n",
      "[2018-06-02 01:59:50.233179] Iteration 38400, train loss = 0.162160, train accuracy = 0.984375\n",
      "[2018-06-02 02:00:04.250179] Iteration 38500, train loss = 0.151279, train accuracy = 0.992188\n",
      "[2018-06-02 02:00:18.248179] Iteration 38600, train loss = 0.150530, train accuracy = 1.000000\n",
      "[2018-06-02 02:00:32.235179] Iteration 38700, train loss = 0.203836, train accuracy = 0.976562\n",
      "[2018-06-02 02:00:46.243179] Iteration 38800, train loss = 0.150656, train accuracy = 1.000000\n",
      "[2018-06-02 02:01:00.256179] Iteration 38900, train loss = 0.164432, train accuracy = 0.984375\n",
      "[2018-06-02 02:01:14.336179] Iteration 39000, train loss = 0.150867, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922700\n",
      "[2018-06-02 02:01:31.429179] Iteration 39100, train loss = 0.144132, train accuracy = 1.000000\n",
      "[2018-06-02 02:01:45.398179] Iteration 39200, train loss = 0.148711, train accuracy = 0.992188\n",
      "[2018-06-02 02:01:59.437179] Iteration 39300, train loss = 0.143874, train accuracy = 1.000000\n",
      "[2018-06-02 02:02:13.429179] Iteration 39400, train loss = 0.159617, train accuracy = 0.984375\n",
      "[2018-06-02 02:02:27.477179] Iteration 39500, train loss = 0.157804, train accuracy = 0.992188\n",
      "[2018-06-02 02:02:41.507179] Iteration 39600, train loss = 0.151139, train accuracy = 0.992188\n",
      "[2018-06-02 02:02:55.563179] Iteration 39700, train loss = 0.154475, train accuracy = 0.992188\n",
      "[2018-06-02 02:03:09.564179] Iteration 39800, train loss = 0.149844, train accuracy = 1.000000\n",
      "[2018-06-02 02:03:23.716179] Iteration 39900, train loss = 0.151777, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三轮  量化\n",
    "prune_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.922200\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.03125    -0.0625      0.00210401  0.125      -0.00109894 -0.03125\n",
      "  0.015625    0.03125     0.01261852 -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.85)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 02:06:18.760179] Iteration 100, train loss = 0.152778, train accuracy = 1.000000\n",
      "[2018-06-02 02:06:32.800179] Iteration 200, train loss = 0.147963, train accuracy = 0.992188\n",
      "[2018-06-02 02:06:46.843179] Iteration 300, train loss = 0.158600, train accuracy = 0.984375\n",
      "[2018-06-02 02:07:00.870179] Iteration 400, train loss = 0.154394, train accuracy = 0.984375\n",
      "[2018-06-02 02:07:14.895179] Iteration 500, train loss = 0.152890, train accuracy = 1.000000\n",
      "[2018-06-02 02:07:28.926179] Iteration 600, train loss = 0.140052, train accuracy = 1.000000\n",
      "[2018-06-02 02:07:43.022179] Iteration 700, train loss = 0.159428, train accuracy = 0.992188\n",
      "[2018-06-02 02:07:57.029179] Iteration 800, train loss = 0.150812, train accuracy = 0.992188\n",
      "[2018-06-02 02:08:11.099179] Iteration 900, train loss = 0.154687, train accuracy = 0.992188\n",
      "[2018-06-02 02:08:25.134179] Iteration 1000, train loss = 0.159914, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.918300\n",
      "[2018-06-02 02:08:42.194179] Iteration 1100, train loss = 0.163961, train accuracy = 0.992188\n",
      "[2018-06-02 02:08:56.252179] Iteration 1200, train loss = 0.145486, train accuracy = 1.000000\n",
      "[2018-06-02 02:09:10.273179] Iteration 1300, train loss = 0.173547, train accuracy = 0.992188\n",
      "[2018-06-02 02:09:24.261179] Iteration 1400, train loss = 0.163345, train accuracy = 0.992188\n",
      "[2018-06-02 02:09:38.335179] Iteration 1500, train loss = 0.141045, train accuracy = 1.000000\n",
      "[2018-06-02 02:09:52.410179] Iteration 1600, train loss = 0.154286, train accuracy = 0.992188\n",
      "[2018-06-02 02:10:06.417179] Iteration 1700, train loss = 0.184388, train accuracy = 0.984375\n",
      "[2018-06-02 02:10:20.474179] Iteration 1800, train loss = 0.143005, train accuracy = 1.000000\n",
      "[2018-06-02 02:10:34.555179] Iteration 1900, train loss = 0.152726, train accuracy = 1.000000\n",
      "[2018-06-02 02:10:48.594179] Iteration 2000, train loss = 0.149374, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919700\n",
      "[2018-06-02 02:11:05.678179] Iteration 2100, train loss = 0.145135, train accuracy = 1.000000\n",
      "[2018-06-02 02:11:19.727179] Iteration 2200, train loss = 0.154671, train accuracy = 0.992188\n",
      "[2018-06-02 02:11:33.828179] Iteration 2300, train loss = 0.142941, train accuracy = 1.000000\n",
      "[2018-06-02 02:11:47.902179] Iteration 2400, train loss = 0.147114, train accuracy = 0.992188\n",
      "[2018-06-02 02:12:01.920179] Iteration 2500, train loss = 0.164063, train accuracy = 0.992188\n",
      "[2018-06-02 02:12:15.981179] Iteration 2600, train loss = 0.141574, train accuracy = 1.000000\n",
      "[2018-06-02 02:12:30.098179] Iteration 2700, train loss = 0.152211, train accuracy = 0.992188\n",
      "[2018-06-02 02:12:44.129179] Iteration 2800, train loss = 0.184558, train accuracy = 0.976562\n",
      "[2018-06-02 02:12:58.252179] Iteration 2900, train loss = 0.155801, train accuracy = 1.000000\n",
      "[2018-06-02 02:13:12.325179] Iteration 3000, train loss = 0.149394, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919900\n",
      "[2018-06-02 02:13:29.510179] Iteration 3100, train loss = 0.142473, train accuracy = 1.000000\n",
      "[2018-06-02 02:13:43.598179] Iteration 3200, train loss = 0.144992, train accuracy = 1.000000\n",
      "[2018-06-02 02:13:57.667179] Iteration 3300, train loss = 0.152673, train accuracy = 1.000000\n",
      "[2018-06-02 02:14:11.724179] Iteration 3400, train loss = 0.152347, train accuracy = 0.992188\n",
      "[2018-06-02 02:14:25.757179] Iteration 3500, train loss = 0.149457, train accuracy = 0.992188\n",
      "[2018-06-02 02:14:39.891179] Iteration 3600, train loss = 0.159724, train accuracy = 0.984375\n",
      "[2018-06-02 02:14:53.996179] Iteration 3700, train loss = 0.199847, train accuracy = 0.976562\n",
      "[2018-06-02 02:15:08.056179] Iteration 3800, train loss = 0.172257, train accuracy = 0.992188\n",
      "[2018-06-02 02:15:22.162179] Iteration 3900, train loss = 0.146658, train accuracy = 0.992188\n",
      "[2018-06-02 02:15:36.238179] Iteration 4000, train loss = 0.161153, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920100\n",
      "[2018-06-02 02:15:53.383179] Iteration 4100, train loss = 0.171257, train accuracy = 0.984375\n",
      "[2018-06-02 02:16:07.405179] Iteration 4200, train loss = 0.151179, train accuracy = 1.000000\n",
      "[2018-06-02 02:16:21.431179] Iteration 4300, train loss = 0.140032, train accuracy = 1.000000\n",
      "[2018-06-02 02:16:35.513179] Iteration 4400, train loss = 0.139163, train accuracy = 1.000000\n",
      "[2018-06-02 02:16:49.493179] Iteration 4500, train loss = 0.141535, train accuracy = 1.000000\n",
      "[2018-06-02 02:17:03.569179] Iteration 4600, train loss = 0.156366, train accuracy = 0.992188\n",
      "[2018-06-02 02:17:17.690179] Iteration 4700, train loss = 0.139307, train accuracy = 1.000000\n",
      "[2018-06-02 02:17:31.681179] Iteration 4800, train loss = 0.160402, train accuracy = 1.000000\n",
      "[2018-06-02 02:17:45.738179] Iteration 4900, train loss = 0.178007, train accuracy = 0.984375\n",
      "[2018-06-02 02:17:59.764179] Iteration 5000, train loss = 0.167240, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.920400\n",
      "[2018-06-02 02:18:16.923179] Iteration 5100, train loss = 0.161986, train accuracy = 0.984375\n",
      "[2018-06-02 02:18:30.897179] Iteration 5200, train loss = 0.138521, train accuracy = 1.000000\n",
      "[2018-06-02 02:18:44.954179] Iteration 5300, train loss = 0.150941, train accuracy = 1.000000\n",
      "[2018-06-02 02:18:58.979179] Iteration 5400, train loss = 0.141173, train accuracy = 1.000000\n",
      "[2018-06-02 02:19:13.045179] Iteration 5500, train loss = 0.166569, train accuracy = 0.984375\n",
      "[2018-06-02 02:19:27.079179] Iteration 5600, train loss = 0.138132, train accuracy = 1.000000\n",
      "[2018-06-02 02:19:41.154179] Iteration 5700, train loss = 0.143564, train accuracy = 1.000000\n",
      "[2018-06-02 02:19:55.193179] Iteration 5800, train loss = 0.143527, train accuracy = 1.000000\n",
      "[2018-06-02 02:20:09.246179] Iteration 5900, train loss = 0.141962, train accuracy = 1.000000\n",
      "[2018-06-02 02:20:23.253179] Iteration 6000, train loss = 0.146068, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920200\n",
      "[2018-06-02 02:20:40.318179] Iteration 6100, train loss = 0.136644, train accuracy = 1.000000\n",
      "[2018-06-02 02:20:54.512179] Iteration 6200, train loss = 0.144160, train accuracy = 1.000000\n",
      "[2018-06-02 02:21:08.569179] Iteration 6300, train loss = 0.175423, train accuracy = 0.984375\n",
      "[2018-06-02 02:21:22.639179] Iteration 6400, train loss = 0.141748, train accuracy = 1.000000\n",
      "[2018-06-02 02:21:36.634179] Iteration 6500, train loss = 0.140186, train accuracy = 1.000000\n",
      "[2018-06-02 02:21:50.705179] Iteration 6600, train loss = 0.143216, train accuracy = 1.000000\n",
      "[2018-06-02 02:22:04.773179] Iteration 6700, train loss = 0.157245, train accuracy = 0.984375\n",
      "[2018-06-02 02:22:18.795179] Iteration 6800, train loss = 0.147405, train accuracy = 1.000000\n",
      "[2018-06-02 02:22:32.859179] Iteration 6900, train loss = 0.151966, train accuracy = 0.992188\n",
      "[2018-06-02 02:22:46.940179] Iteration 7000, train loss = 0.139606, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920000\n",
      "[2018-06-02 02:23:04.006179] Iteration 7100, train loss = 0.153694, train accuracy = 0.992188\n",
      "[2018-06-02 02:23:18.068179] Iteration 7200, train loss = 0.186102, train accuracy = 0.984375\n",
      "[2018-06-02 02:23:32.159179] Iteration 7300, train loss = 0.157589, train accuracy = 0.992188\n",
      "[2018-06-02 02:23:46.248179] Iteration 7400, train loss = 0.157552, train accuracy = 0.992188\n",
      "[2018-06-02 02:24:00.344179] Iteration 7500, train loss = 0.146742, train accuracy = 1.000000\n",
      "[2018-06-02 02:24:14.386179] Iteration 7600, train loss = 0.154536, train accuracy = 0.992188\n",
      "[2018-06-02 02:24:28.385179] Iteration 7700, train loss = 0.149141, train accuracy = 1.000000\n",
      "[2018-06-02 02:24:42.422179] Iteration 7800, train loss = 0.148418, train accuracy = 0.992188\n",
      "[2018-06-02 02:24:56.500179] Iteration 7900, train loss = 0.155046, train accuracy = 0.992188\n",
      "[2018-06-02 02:25:10.541179] Iteration 8000, train loss = 0.176755, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.920200\n",
      "[2018-06-02 02:25:27.576179] Iteration 8100, train loss = 0.193739, train accuracy = 0.984375\n",
      "[2018-06-02 02:25:41.632179] Iteration 8200, train loss = 0.148960, train accuracy = 0.992188\n",
      "[2018-06-02 02:25:55.762179] Iteration 8300, train loss = 0.145404, train accuracy = 1.000000\n",
      "[2018-06-02 02:26:09.792179] Iteration 8400, train loss = 0.162773, train accuracy = 0.992188\n",
      "[2018-06-02 02:26:23.847179] Iteration 8500, train loss = 0.152048, train accuracy = 1.000000\n",
      "[2018-06-02 02:26:37.916179] Iteration 8600, train loss = 0.161475, train accuracy = 0.992188\n",
      "[2018-06-02 02:26:51.894179] Iteration 8700, train loss = 0.172123, train accuracy = 0.984375\n",
      "[2018-06-02 02:27:05.915179] Iteration 8800, train loss = 0.146962, train accuracy = 1.000000\n",
      "[2018-06-02 02:27:19.965179] Iteration 8900, train loss = 0.151735, train accuracy = 1.000000\n",
      "[2018-06-02 02:27:33.991179] Iteration 9000, train loss = 0.164985, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.920000\n",
      "[2018-06-02 02:27:51.049179] Iteration 9100, train loss = 0.142961, train accuracy = 0.992188\n",
      "[2018-06-02 02:28:05.132179] Iteration 9200, train loss = 0.155986, train accuracy = 0.992188\n",
      "[2018-06-02 02:28:19.163179] Iteration 9300, train loss = 0.143986, train accuracy = 1.000000\n",
      "[2018-06-02 02:28:33.288179] Iteration 9400, train loss = 0.157918, train accuracy = 0.992188\n",
      "[2018-06-02 02:28:47.325179] Iteration 9500, train loss = 0.149773, train accuracy = 1.000000\n",
      "[2018-06-02 02:29:01.377179] Iteration 9600, train loss = 0.167743, train accuracy = 0.992188\n",
      "[2018-06-02 02:29:15.496179] Iteration 9700, train loss = 0.162546, train accuracy = 0.984375\n",
      "[2018-06-02 02:29:29.476179] Iteration 9800, train loss = 0.144923, train accuracy = 1.000000\n",
      "[2018-06-02 02:29:43.501179] Iteration 9900, train loss = 0.176820, train accuracy = 0.984375\n",
      "[2018-06-02 02:29:57.506179] Iteration 10000, train loss = 0.145818, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920000\n",
      "[2018-06-02 02:30:14.677179] Iteration 10100, train loss = 0.150735, train accuracy = 0.992188\n",
      "[2018-06-02 02:30:28.704179] Iteration 10200, train loss = 0.164375, train accuracy = 0.992188\n",
      "[2018-06-02 02:30:42.790179] Iteration 10300, train loss = 0.136848, train accuracy = 1.000000\n",
      "[2018-06-02 02:30:56.945179] Iteration 10400, train loss = 0.140932, train accuracy = 1.000000\n",
      "[2018-06-02 02:31:10.974179] Iteration 10500, train loss = 0.165287, train accuracy = 0.992188\n",
      "[2018-06-02 02:31:24.980179] Iteration 10600, train loss = 0.148849, train accuracy = 1.000000\n",
      "[2018-06-02 02:31:39.009179] Iteration 10700, train loss = 0.164677, train accuracy = 0.992188\n",
      "[2018-06-02 02:31:53.117179] Iteration 10800, train loss = 0.155333, train accuracy = 0.992188\n",
      "[2018-06-02 02:32:07.232179] Iteration 10900, train loss = 0.149068, train accuracy = 0.992188\n",
      "[2018-06-02 02:32:21.296179] Iteration 11000, train loss = 0.141300, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.919700\n",
      "[2018-06-02 02:32:38.396179] Iteration 11100, train loss = 0.143801, train accuracy = 1.000000\n",
      "[2018-06-02 02:32:52.481179] Iteration 11200, train loss = 0.145189, train accuracy = 1.000000\n",
      "[2018-06-02 02:33:06.531179] Iteration 11300, train loss = 0.163039, train accuracy = 0.984375\n",
      "[2018-06-02 02:33:20.577179] Iteration 11400, train loss = 0.143965, train accuracy = 1.000000\n",
      "[2018-06-02 02:33:34.570179] Iteration 11500, train loss = 0.151180, train accuracy = 0.984375\n",
      "[2018-06-02 02:33:48.588179] Iteration 11600, train loss = 0.151027, train accuracy = 0.992188\n",
      "[2018-06-02 02:34:02.592179] Iteration 11700, train loss = 0.155397, train accuracy = 0.992188\n",
      "[2018-06-02 02:34:16.612179] Iteration 11800, train loss = 0.152722, train accuracy = 0.992188\n",
      "[2018-06-02 02:34:30.638179] Iteration 11900, train loss = 0.146351, train accuracy = 1.000000\n",
      "[2018-06-02 02:34:44.607179] Iteration 12000, train loss = 0.143942, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920800\n",
      "[2018-06-02 02:35:01.683179] Iteration 12100, train loss = 0.138552, train accuracy = 1.000000\n",
      "[2018-06-02 02:35:15.775179] Iteration 12200, train loss = 0.138855, train accuracy = 1.000000\n",
      "[2018-06-02 02:35:29.819179] Iteration 12300, train loss = 0.156956, train accuracy = 0.976562\n",
      "[2018-06-02 02:35:43.852179] Iteration 12400, train loss = 0.143736, train accuracy = 1.000000\n",
      "[2018-06-02 02:35:57.897179] Iteration 12500, train loss = 0.142033, train accuracy = 1.000000\n",
      "[2018-06-02 02:36:11.991179] Iteration 12600, train loss = 0.141134, train accuracy = 1.000000\n",
      "[2018-06-02 02:36:26.014179] Iteration 12700, train loss = 0.161447, train accuracy = 0.984375\n",
      "[2018-06-02 02:36:40.177179] Iteration 12800, train loss = 0.162357, train accuracy = 0.992188\n",
      "[2018-06-02 02:36:54.267179] Iteration 12900, train loss = 0.162364, train accuracy = 0.992188\n",
      "[2018-06-02 02:37:08.322179] Iteration 13000, train loss = 0.148700, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-02 02:37:25.493179] Iteration 13100, train loss = 0.161050, train accuracy = 0.992188\n",
      "[2018-06-02 02:37:39.634179] Iteration 13200, train loss = 0.141129, train accuracy = 1.000000\n",
      "[2018-06-02 02:37:53.751179] Iteration 13300, train loss = 0.144215, train accuracy = 1.000000\n",
      "[2018-06-02 02:38:07.791179] Iteration 13400, train loss = 0.148611, train accuracy = 0.992188\n",
      "[2018-06-02 02:38:21.871179] Iteration 13500, train loss = 0.139351, train accuracy = 1.000000\n",
      "[2018-06-02 02:38:35.972179] Iteration 13600, train loss = 0.143440, train accuracy = 1.000000\n",
      "[2018-06-02 02:38:50.048179] Iteration 13700, train loss = 0.159290, train accuracy = 0.992188\n",
      "[2018-06-02 02:39:04.121179] Iteration 13800, train loss = 0.164109, train accuracy = 0.992188\n",
      "[2018-06-02 02:39:18.180179] Iteration 13900, train loss = 0.155667, train accuracy = 0.992188\n",
      "[2018-06-02 02:39:32.218179] Iteration 14000, train loss = 0.157364, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-02 02:39:49.293179] Iteration 14100, train loss = 0.167755, train accuracy = 0.976562\n",
      "[2018-06-02 02:40:03.399179] Iteration 14200, train loss = 0.140081, train accuracy = 1.000000\n",
      "[2018-06-02 02:40:17.477179] Iteration 14300, train loss = 0.150175, train accuracy = 0.992188\n",
      "[2018-06-02 02:40:31.534179] Iteration 14400, train loss = 0.172893, train accuracy = 0.984375\n",
      "[2018-06-02 02:40:45.657179] Iteration 14500, train loss = 0.152008, train accuracy = 1.000000\n",
      "[2018-06-02 02:40:59.699179] Iteration 14600, train loss = 0.161488, train accuracy = 0.992188\n",
      "[2018-06-02 02:41:13.752179] Iteration 14700, train loss = 0.136799, train accuracy = 1.000000\n",
      "[2018-06-02 02:41:27.783179] Iteration 14800, train loss = 0.151978, train accuracy = 1.000000\n",
      "[2018-06-02 02:41:41.791179] Iteration 14900, train loss = 0.146463, train accuracy = 0.992188\n",
      "[2018-06-02 02:41:55.919179] Iteration 15000, train loss = 0.140939, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920900\n",
      "[2018-06-02 02:42:12.986179] Iteration 15100, train loss = 0.172959, train accuracy = 0.984375\n",
      "[2018-06-02 02:42:27.025179] Iteration 15200, train loss = 0.146241, train accuracy = 1.000000\n",
      "[2018-06-02 02:42:41.075179] Iteration 15300, train loss = 0.172548, train accuracy = 0.984375\n",
      "[2018-06-02 02:42:55.138179] Iteration 15400, train loss = 0.178113, train accuracy = 0.984375\n",
      "[2018-06-02 02:43:09.214179] Iteration 15500, train loss = 0.136292, train accuracy = 1.000000\n",
      "[2018-06-02 02:43:23.259179] Iteration 15600, train loss = 0.146671, train accuracy = 1.000000\n",
      "[2018-06-02 02:43:37.316179] Iteration 15700, train loss = 0.140295, train accuracy = 1.000000\n",
      "[2018-06-02 02:43:51.363179] Iteration 15800, train loss = 0.146556, train accuracy = 0.992188\n",
      "[2018-06-02 02:44:05.389179] Iteration 15900, train loss = 0.160457, train accuracy = 0.992188\n",
      "[2018-06-02 02:44:19.452179] Iteration 16000, train loss = 0.182604, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921700\n",
      "[2018-06-02 02:44:36.519179] Iteration 16100, train loss = 0.143279, train accuracy = 1.000000\n",
      "[2018-06-02 02:44:50.599179] Iteration 16200, train loss = 0.148739, train accuracy = 1.000000\n",
      "[2018-06-02 02:45:04.713179] Iteration 16300, train loss = 0.142431, train accuracy = 1.000000\n",
      "[2018-06-02 02:45:18.845179] Iteration 16400, train loss = 0.145527, train accuracy = 1.000000\n",
      "[2018-06-02 02:45:32.851179] Iteration 16500, train loss = 0.139262, train accuracy = 1.000000\n",
      "[2018-06-02 02:45:46.879179] Iteration 16600, train loss = 0.143594, train accuracy = 1.000000\n",
      "[2018-06-02 02:46:00.944179] Iteration 16700, train loss = 0.139818, train accuracy = 1.000000\n",
      "[2018-06-02 02:46:14.998179] Iteration 16800, train loss = 0.137068, train accuracy = 1.000000\n",
      "[2018-06-02 02:46:28.966179] Iteration 16900, train loss = 0.148100, train accuracy = 1.000000\n",
      "[2018-06-02 02:46:42.949179] Iteration 17000, train loss = 0.161639, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 02:47:00.059179] Iteration 17100, train loss = 0.169485, train accuracy = 0.984375\n",
      "[2018-06-02 02:47:14.139179] Iteration 17200, train loss = 0.144331, train accuracy = 1.000000\n",
      "[2018-06-02 02:47:28.193179] Iteration 17300, train loss = 0.184920, train accuracy = 0.984375\n",
      "[2018-06-02 02:47:42.278179] Iteration 17400, train loss = 0.141010, train accuracy = 1.000000\n",
      "[2018-06-02 02:47:56.377179] Iteration 17500, train loss = 0.148936, train accuracy = 1.000000\n",
      "[2018-06-02 02:48:10.436179] Iteration 17600, train loss = 0.148452, train accuracy = 0.992188\n",
      "[2018-06-02 02:48:24.446179] Iteration 17700, train loss = 0.153411, train accuracy = 1.000000\n",
      "[2018-06-02 02:48:38.495179] Iteration 17800, train loss = 0.143780, train accuracy = 1.000000\n",
      "[2018-06-02 02:48:52.593179] Iteration 17900, train loss = 0.167957, train accuracy = 0.992188\n",
      "[2018-06-02 02:49:06.636179] Iteration 18000, train loss = 0.201164, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.921200\n",
      "[2018-06-02 02:49:23.716179] Iteration 18100, train loss = 0.146257, train accuracy = 1.000000\n",
      "[2018-06-02 02:49:37.762179] Iteration 18200, train loss = 0.158430, train accuracy = 0.992188\n",
      "[2018-06-02 02:49:51.845179] Iteration 18300, train loss = 0.160526, train accuracy = 0.992188\n",
      "[2018-06-02 02:50:05.965179] Iteration 18400, train loss = 0.143394, train accuracy = 1.000000\n",
      "[2018-06-02 02:50:19.955179] Iteration 18500, train loss = 0.191083, train accuracy = 0.992188\n",
      "[2018-06-02 02:50:33.971179] Iteration 18600, train loss = 0.155506, train accuracy = 0.992188\n",
      "[2018-06-02 02:50:48.061179] Iteration 18700, train loss = 0.156096, train accuracy = 1.000000\n",
      "[2018-06-02 02:51:02.153179] Iteration 18800, train loss = 0.181470, train accuracy = 0.992188\n",
      "[2018-06-02 02:51:16.143179] Iteration 18900, train loss = 0.139347, train accuracy = 1.000000\n",
      "[2018-06-02 02:51:30.159179] Iteration 19000, train loss = 0.148733, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920900\n",
      "[2018-06-02 02:51:47.278179] Iteration 19100, train loss = 0.154458, train accuracy = 0.992188\n",
      "[2018-06-02 02:52:01.322179] Iteration 19200, train loss = 0.161816, train accuracy = 0.984375\n",
      "[2018-06-02 02:52:15.338179] Iteration 19300, train loss = 0.158478, train accuracy = 0.992188\n",
      "[2018-06-02 02:52:29.361179] Iteration 19400, train loss = 0.146518, train accuracy = 1.000000\n",
      "[2018-06-02 02:52:43.387179] Iteration 19500, train loss = 0.144564, train accuracy = 1.000000\n",
      "[2018-06-02 02:52:57.509179] Iteration 19600, train loss = 0.146901, train accuracy = 1.000000\n",
      "[2018-06-02 02:53:11.681179] Iteration 19700, train loss = 0.168278, train accuracy = 0.992188\n",
      "[2018-06-02 02:53:25.720179] Iteration 19800, train loss = 0.142009, train accuracy = 1.000000\n",
      "[2018-06-02 02:53:39.752179] Iteration 19900, train loss = 0.164194, train accuracy = 0.984375\n",
      "[2018-06-02 02:53:53.826179] Iteration 20000, train loss = 0.150741, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 02:54:10.915179] Iteration 20100, train loss = 0.159825, train accuracy = 0.984375\n",
      "[2018-06-02 02:54:24.997179] Iteration 20200, train loss = 0.148986, train accuracy = 1.000000\n",
      "[2018-06-02 02:54:39.086179] Iteration 20300, train loss = 0.173567, train accuracy = 0.984375\n",
      "[2018-06-02 02:54:53.112179] Iteration 20400, train loss = 0.140534, train accuracy = 1.000000\n",
      "[2018-06-02 02:55:07.147179] Iteration 20500, train loss = 0.162135, train accuracy = 0.992188\n",
      "[2018-06-02 02:55:21.272179] Iteration 20600, train loss = 0.152611, train accuracy = 1.000000\n",
      "[2018-06-02 02:55:35.406179] Iteration 20700, train loss = 0.152778, train accuracy = 1.000000\n",
      "[2018-06-02 02:55:49.442179] Iteration 20800, train loss = 0.151289, train accuracy = 0.992188\n",
      "[2018-06-02 02:56:03.488179] Iteration 20900, train loss = 0.158344, train accuracy = 0.992188\n",
      "[2018-06-02 02:56:17.569179] Iteration 21000, train loss = 0.151256, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921200\n",
      "[2018-06-02 02:56:34.631179] Iteration 21100, train loss = 0.157261, train accuracy = 0.992188\n",
      "[2018-06-02 02:56:48.655179] Iteration 21200, train loss = 0.151297, train accuracy = 0.992188\n",
      "[2018-06-02 02:57:02.705179] Iteration 21300, train loss = 0.164276, train accuracy = 0.992188\n",
      "[2018-06-02 02:57:16.833179] Iteration 21400, train loss = 0.157632, train accuracy = 1.000000\n",
      "[2018-06-02 02:57:30.909179] Iteration 21500, train loss = 0.156028, train accuracy = 0.992188\n",
      "[2018-06-02 02:57:44.890179] Iteration 21600, train loss = 0.185114, train accuracy = 0.984375\n",
      "[2018-06-02 02:57:58.955179] Iteration 21700, train loss = 0.147315, train accuracy = 1.000000\n",
      "[2018-06-02 02:58:13.103179] Iteration 21800, train loss = 0.137988, train accuracy = 1.000000\n",
      "[2018-06-02 02:58:27.186179] Iteration 21900, train loss = 0.137231, train accuracy = 1.000000\n",
      "[2018-06-02 02:58:41.291179] Iteration 22000, train loss = 0.177002, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921200\n",
      "[2018-06-02 02:58:58.368179] Iteration 22100, train loss = 0.153081, train accuracy = 0.992188\n",
      "[2018-06-02 02:59:12.408179] Iteration 22200, train loss = 0.145294, train accuracy = 1.000000\n",
      "[2018-06-02 02:59:26.420179] Iteration 22300, train loss = 0.145449, train accuracy = 1.000000\n",
      "[2018-06-02 02:59:40.493179] Iteration 22400, train loss = 0.138151, train accuracy = 1.000000\n",
      "[2018-06-02 02:59:54.499179] Iteration 22500, train loss = 0.158221, train accuracy = 0.992188\n",
      "[2018-06-02 03:00:08.519179] Iteration 22600, train loss = 0.145651, train accuracy = 1.000000\n",
      "[2018-06-02 03:00:22.595179] Iteration 22700, train loss = 0.157570, train accuracy = 0.992188\n",
      "[2018-06-02 03:00:36.631179] Iteration 22800, train loss = 0.157732, train accuracy = 0.992188\n",
      "[2018-06-02 03:00:50.681179] Iteration 22900, train loss = 0.140791, train accuracy = 1.000000\n",
      "[2018-06-02 03:01:04.778179] Iteration 23000, train loss = 0.151260, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:01:21.834179] Iteration 23100, train loss = 0.163340, train accuracy = 0.984375\n",
      "[2018-06-02 03:01:35.904179] Iteration 23200, train loss = 0.145144, train accuracy = 1.000000\n",
      "[2018-06-02 03:01:49.944179] Iteration 23300, train loss = 0.146022, train accuracy = 1.000000\n",
      "[2018-06-02 03:02:03.938179] Iteration 23400, train loss = 0.143609, train accuracy = 1.000000\n",
      "[2018-06-02 03:02:17.912179] Iteration 23500, train loss = 0.138678, train accuracy = 1.000000\n",
      "[2018-06-02 03:02:31.944179] Iteration 23600, train loss = 0.138585, train accuracy = 1.000000\n",
      "[2018-06-02 03:02:46.075179] Iteration 23700, train loss = 0.140491, train accuracy = 1.000000\n",
      "[2018-06-02 03:03:00.100179] Iteration 23800, train loss = 0.180840, train accuracy = 0.984375\n",
      "[2018-06-02 03:03:14.127179] Iteration 23900, train loss = 0.140854, train accuracy = 1.000000\n",
      "[2018-06-02 03:03:28.132179] Iteration 24000, train loss = 0.150010, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:03:45.222179] Iteration 24100, train loss = 0.151410, train accuracy = 0.992188\n",
      "[2018-06-02 03:03:59.298179] Iteration 24200, train loss = 0.147507, train accuracy = 1.000000\n",
      "[2018-06-02 03:04:13.381179] Iteration 24300, train loss = 0.149702, train accuracy = 0.992188\n",
      "[2018-06-02 03:04:27.499179] Iteration 24400, train loss = 0.157701, train accuracy = 0.992188\n",
      "[2018-06-02 03:04:41.536179] Iteration 24500, train loss = 0.148146, train accuracy = 0.992188\n",
      "[2018-06-02 03:04:55.545179] Iteration 24600, train loss = 0.146443, train accuracy = 1.000000\n",
      "[2018-06-02 03:05:09.612179] Iteration 24700, train loss = 0.154353, train accuracy = 0.992188\n",
      "[2018-06-02 03:05:23.623179] Iteration 24800, train loss = 0.144024, train accuracy = 1.000000\n",
      "[2018-06-02 03:05:37.620179] Iteration 24900, train loss = 0.173298, train accuracy = 0.984375\n",
      "[2018-06-02 03:05:51.628179] Iteration 25000, train loss = 0.141803, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921300\n",
      "[2018-06-02 03:06:08.698179] Iteration 25100, train loss = 0.152744, train accuracy = 0.992188\n",
      "[2018-06-02 03:06:22.772179] Iteration 25200, train loss = 0.160483, train accuracy = 1.000000\n",
      "[2018-06-02 03:06:36.763179] Iteration 25300, train loss = 0.158295, train accuracy = 0.992188\n",
      "[2018-06-02 03:06:50.769179] Iteration 25400, train loss = 0.174429, train accuracy = 0.984375\n",
      "[2018-06-02 03:07:04.792179] Iteration 25500, train loss = 0.157104, train accuracy = 0.984375\n",
      "[2018-06-02 03:07:18.877179] Iteration 25600, train loss = 0.154679, train accuracy = 0.984375\n",
      "[2018-06-02 03:07:32.931179] Iteration 25700, train loss = 0.144356, train accuracy = 1.000000\n",
      "[2018-06-02 03:07:46.972179] Iteration 25800, train loss = 0.152760, train accuracy = 1.000000\n",
      "[2018-06-02 03:08:01.050179] Iteration 25900, train loss = 0.163878, train accuracy = 0.976562\n",
      "[2018-06-02 03:08:15.075179] Iteration 26000, train loss = 0.152388, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921500\n",
      "[2018-06-02 03:08:32.105179] Iteration 26100, train loss = 0.139176, train accuracy = 1.000000\n",
      "[2018-06-02 03:08:46.119179] Iteration 26200, train loss = 0.160034, train accuracy = 0.992188\n",
      "[2018-06-02 03:09:00.135179] Iteration 26300, train loss = 0.148051, train accuracy = 0.992188\n",
      "[2018-06-02 03:09:14.113179] Iteration 26400, train loss = 0.143530, train accuracy = 1.000000\n",
      "[2018-06-02 03:09:28.205179] Iteration 26500, train loss = 0.156830, train accuracy = 1.000000\n",
      "[2018-06-02 03:09:42.270179] Iteration 26600, train loss = 0.149454, train accuracy = 1.000000\n",
      "[2018-06-02 03:09:56.324179] Iteration 26700, train loss = 0.149211, train accuracy = 0.992188\n",
      "[2018-06-02 03:10:10.385179] Iteration 26800, train loss = 0.141958, train accuracy = 1.000000\n",
      "[2018-06-02 03:10:24.404179] Iteration 26900, train loss = 0.150257, train accuracy = 1.000000\n",
      "[2018-06-02 03:10:38.439179] Iteration 27000, train loss = 0.147737, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920900\n",
      "[2018-06-02 03:10:55.445179] Iteration 27100, train loss = 0.138320, train accuracy = 1.000000\n",
      "[2018-06-02 03:11:09.477179] Iteration 27200, train loss = 0.142147, train accuracy = 1.000000\n",
      "[2018-06-02 03:11:23.566179] Iteration 27300, train loss = 0.147182, train accuracy = 1.000000\n",
      "[2018-06-02 03:11:37.571179] Iteration 27400, train loss = 0.157114, train accuracy = 1.000000\n",
      "[2018-06-02 03:11:51.595179] Iteration 27500, train loss = 0.162755, train accuracy = 0.984375\n",
      "[2018-06-02 03:12:05.661179] Iteration 27600, train loss = 0.168607, train accuracy = 0.984375\n",
      "[2018-06-02 03:12:19.725179] Iteration 27700, train loss = 0.137585, train accuracy = 1.000000\n",
      "[2018-06-02 03:12:33.783179] Iteration 27800, train loss = 0.151147, train accuracy = 0.992188\n",
      "[2018-06-02 03:12:47.841179] Iteration 27900, train loss = 0.138423, train accuracy = 1.000000\n",
      "[2018-06-02 03:13:01.888179] Iteration 28000, train loss = 0.143423, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:13:18.991179] Iteration 28100, train loss = 0.163785, train accuracy = 0.984375\n",
      "[2018-06-02 03:13:33.118179] Iteration 28200, train loss = 0.181261, train accuracy = 0.992188\n",
      "[2018-06-02 03:13:47.185179] Iteration 28300, train loss = 0.136154, train accuracy = 1.000000\n",
      "[2018-06-02 03:14:01.364179] Iteration 28400, train loss = 0.142915, train accuracy = 1.000000\n",
      "[2018-06-02 03:14:15.464179] Iteration 28500, train loss = 0.162944, train accuracy = 0.984375\n",
      "[2018-06-02 03:14:29.493179] Iteration 28600, train loss = 0.144189, train accuracy = 1.000000\n",
      "[2018-06-02 03:14:43.546179] Iteration 28700, train loss = 0.151883, train accuracy = 0.992188\n",
      "[2018-06-02 03:14:57.629179] Iteration 28800, train loss = 0.145634, train accuracy = 1.000000\n",
      "[2018-06-02 03:15:11.616179] Iteration 28900, train loss = 0.156522, train accuracy = 0.992188\n",
      "[2018-06-02 03:15:25.658179] Iteration 29000, train loss = 0.155623, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921100\n",
      "[2018-06-02 03:15:42.709179] Iteration 29100, train loss = 0.144861, train accuracy = 1.000000\n",
      "[2018-06-02 03:15:56.755179] Iteration 29200, train loss = 0.140317, train accuracy = 1.000000\n",
      "[2018-06-02 03:16:10.760179] Iteration 29300, train loss = 0.143663, train accuracy = 1.000000\n",
      "[2018-06-02 03:16:24.908179] Iteration 29400, train loss = 0.162393, train accuracy = 0.992188\n",
      "[2018-06-02 03:16:38.988179] Iteration 29500, train loss = 0.141972, train accuracy = 1.000000\n",
      "[2018-06-02 03:16:53.016179] Iteration 29600, train loss = 0.145295, train accuracy = 1.000000\n",
      "[2018-06-02 03:17:07.105179] Iteration 29700, train loss = 0.150888, train accuracy = 0.992188\n",
      "[2018-06-02 03:17:21.126179] Iteration 29800, train loss = 0.184455, train accuracy = 0.984375\n",
      "[2018-06-02 03:17:35.166179] Iteration 29900, train loss = 0.154887, train accuracy = 0.992188\n",
      "[2018-06-02 03:17:49.251179] Iteration 30000, train loss = 0.140825, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920900\n",
      "[2018-06-02 03:18:06.314179] Iteration 30100, train loss = 0.139088, train accuracy = 1.000000\n",
      "[2018-06-02 03:18:20.365179] Iteration 30200, train loss = 0.152734, train accuracy = 0.992188\n",
      "[2018-06-02 03:18:34.399179] Iteration 30300, train loss = 0.151933, train accuracy = 1.000000\n",
      "[2018-06-02 03:18:48.370179] Iteration 30400, train loss = 0.159538, train accuracy = 0.992188\n",
      "[2018-06-02 03:19:02.535179] Iteration 30500, train loss = 0.147064, train accuracy = 1.000000\n",
      "[2018-06-02 03:19:16.534179] Iteration 30600, train loss = 0.153738, train accuracy = 0.984375\n",
      "[2018-06-02 03:19:30.591179] Iteration 30700, train loss = 0.141746, train accuracy = 1.000000\n",
      "[2018-06-02 03:19:44.587179] Iteration 30800, train loss = 0.139163, train accuracy = 1.000000\n",
      "[2018-06-02 03:19:58.658179] Iteration 30900, train loss = 0.151596, train accuracy = 0.992188\n",
      "[2018-06-02 03:20:12.755179] Iteration 31000, train loss = 0.151990, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-02 03:20:29.844179] Iteration 31100, train loss = 0.146024, train accuracy = 1.000000\n",
      "[2018-06-02 03:20:43.906179] Iteration 31200, train loss = 0.165232, train accuracy = 0.992188\n",
      "[2018-06-02 03:20:57.940179] Iteration 31300, train loss = 0.155938, train accuracy = 0.992188\n",
      "[2018-06-02 03:21:11.969179] Iteration 31400, train loss = 0.142268, train accuracy = 1.000000\n",
      "[2018-06-02 03:21:26.081179] Iteration 31500, train loss = 0.160380, train accuracy = 0.992188\n",
      "[2018-06-02 03:21:40.158179] Iteration 31600, train loss = 0.155142, train accuracy = 0.992188\n",
      "[2018-06-02 03:21:54.149179] Iteration 31700, train loss = 0.142528, train accuracy = 1.000000\n",
      "[2018-06-02 03:22:08.253179] Iteration 31800, train loss = 0.149070, train accuracy = 0.992188\n",
      "[2018-06-02 03:22:22.343179] Iteration 31900, train loss = 0.140982, train accuracy = 1.000000\n",
      "[2018-06-02 03:22:36.360179] Iteration 32000, train loss = 0.148142, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.920900\n",
      "[2018-06-02 03:22:53.434179] Iteration 32100, train loss = 0.148793, train accuracy = 1.000000\n",
      "[2018-06-02 03:23:07.450179] Iteration 32200, train loss = 0.188965, train accuracy = 0.984375\n",
      "[2018-06-02 03:23:21.500179] Iteration 32300, train loss = 0.145143, train accuracy = 1.000000\n",
      "[2018-06-02 03:23:35.578179] Iteration 32400, train loss = 0.148072, train accuracy = 0.992188\n",
      "[2018-06-02 03:23:49.642179] Iteration 32500, train loss = 0.170887, train accuracy = 0.992188\n",
      "[2018-06-02 03:24:03.675179] Iteration 32600, train loss = 0.149635, train accuracy = 0.992188\n",
      "[2018-06-02 03:24:17.713179] Iteration 32700, train loss = 0.157278, train accuracy = 0.992188\n",
      "[2018-06-02 03:24:31.718179] Iteration 32800, train loss = 0.141191, train accuracy = 1.000000\n",
      "[2018-06-02 03:24:45.791179] Iteration 32900, train loss = 0.153315, train accuracy = 0.992188\n",
      "[2018-06-02 03:24:59.871179] Iteration 33000, train loss = 0.145460, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:25:16.937179] Iteration 33100, train loss = 0.142163, train accuracy = 1.000000\n",
      "[2018-06-02 03:25:31.030179] Iteration 33200, train loss = 0.140773, train accuracy = 1.000000\n",
      "[2018-06-02 03:25:45.019179] Iteration 33300, train loss = 0.153979, train accuracy = 0.992188\n",
      "[2018-06-02 03:25:59.052179] Iteration 33400, train loss = 0.158313, train accuracy = 0.992188\n",
      "[2018-06-02 03:26:13.046179] Iteration 33500, train loss = 0.158726, train accuracy = 0.992188\n",
      "[2018-06-02 03:26:27.083179] Iteration 33600, train loss = 0.156078, train accuracy = 0.992188\n",
      "[2018-06-02 03:26:41.087179] Iteration 33700, train loss = 0.177481, train accuracy = 0.992188\n",
      "[2018-06-02 03:26:55.158179] Iteration 33800, train loss = 0.140747, train accuracy = 1.000000\n",
      "[2018-06-02 03:27:09.268179] Iteration 33900, train loss = 0.150319, train accuracy = 0.992188\n",
      "[2018-06-02 03:27:23.353179] Iteration 34000, train loss = 0.166813, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:27:40.505179] Iteration 34100, train loss = 0.153440, train accuracy = 1.000000\n",
      "[2018-06-02 03:27:54.595179] Iteration 34200, train loss = 0.170783, train accuracy = 0.976562\n",
      "[2018-06-02 03:28:08.617179] Iteration 34300, train loss = 0.147285, train accuracy = 1.000000\n",
      "[2018-06-02 03:28:22.731179] Iteration 34400, train loss = 0.145263, train accuracy = 1.000000\n",
      "[2018-06-02 03:28:36.778179] Iteration 34500, train loss = 0.148161, train accuracy = 1.000000\n",
      "[2018-06-02 03:28:50.820179] Iteration 34600, train loss = 0.164777, train accuracy = 0.992188\n",
      "[2018-06-02 03:29:04.951179] Iteration 34700, train loss = 0.139733, train accuracy = 1.000000\n",
      "[2018-06-02 03:29:18.958179] Iteration 34800, train loss = 0.139738, train accuracy = 1.000000\n",
      "[2018-06-02 03:29:33.068179] Iteration 34900, train loss = 0.161274, train accuracy = 0.992188\n",
      "[2018-06-02 03:29:47.105179] Iteration 35000, train loss = 0.168791, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-02 03:30:04.162179] Iteration 35100, train loss = 0.144411, train accuracy = 1.000000\n",
      "[2018-06-02 03:30:18.247179] Iteration 35200, train loss = 0.149988, train accuracy = 1.000000\n",
      "[2018-06-02 03:30:32.230179] Iteration 35300, train loss = 0.143958, train accuracy = 1.000000\n",
      "[2018-06-02 03:30:46.343179] Iteration 35400, train loss = 0.144217, train accuracy = 1.000000\n",
      "[2018-06-02 03:31:00.353179] Iteration 35500, train loss = 0.138328, train accuracy = 1.000000\n",
      "[2018-06-02 03:31:14.406179] Iteration 35600, train loss = 0.166959, train accuracy = 0.976562\n",
      "[2018-06-02 03:31:28.481179] Iteration 35700, train loss = 0.141340, train accuracy = 1.000000\n",
      "[2018-06-02 03:31:42.544179] Iteration 35800, train loss = 0.153594, train accuracy = 0.992188\n",
      "[2018-06-02 03:31:56.633179] Iteration 35900, train loss = 0.153524, train accuracy = 0.992188\n",
      "[2018-06-02 03:32:10.698179] Iteration 36000, train loss = 0.169866, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 03:32:27.769179] Iteration 36100, train loss = 0.149553, train accuracy = 0.992188\n",
      "[2018-06-02 03:32:41.825179] Iteration 36200, train loss = 0.142819, train accuracy = 1.000000\n",
      "[2018-06-02 03:32:55.903179] Iteration 36300, train loss = 0.146285, train accuracy = 1.000000\n",
      "[2018-06-02 03:33:09.961179] Iteration 36400, train loss = 0.145428, train accuracy = 1.000000\n",
      "[2018-06-02 03:33:24.010179] Iteration 36500, train loss = 0.160296, train accuracy = 0.992188\n",
      "[2018-06-02 03:33:38.039179] Iteration 36600, train loss = 0.141413, train accuracy = 1.000000\n",
      "[2018-06-02 03:33:52.105179] Iteration 36700, train loss = 0.138544, train accuracy = 1.000000\n",
      "[2018-06-02 03:34:06.179179] Iteration 36800, train loss = 0.150841, train accuracy = 0.992188\n",
      "[2018-06-02 03:34:20.230179] Iteration 36900, train loss = 0.178010, train accuracy = 0.984375\n",
      "[2018-06-02 03:34:34.226179] Iteration 37000, train loss = 0.155142, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921200\n",
      "[2018-06-02 03:34:51.262179] Iteration 37100, train loss = 0.155370, train accuracy = 1.000000\n",
      "[2018-06-02 03:35:05.348179] Iteration 37200, train loss = 0.158586, train accuracy = 0.992188\n",
      "[2018-06-02 03:35:19.463179] Iteration 37300, train loss = 0.156874, train accuracy = 0.992188\n",
      "[2018-06-02 03:35:33.488179] Iteration 37400, train loss = 0.166455, train accuracy = 0.992188\n",
      "[2018-06-02 03:35:47.533179] Iteration 37500, train loss = 0.142807, train accuracy = 1.000000\n",
      "[2018-06-02 03:36:01.538179] Iteration 37600, train loss = 0.141641, train accuracy = 1.000000\n",
      "[2018-06-02 03:36:15.559179] Iteration 37700, train loss = 0.142961, train accuracy = 1.000000\n",
      "[2018-06-02 03:36:29.623179] Iteration 37800, train loss = 0.143986, train accuracy = 1.000000\n",
      "[2018-06-02 03:36:43.696179] Iteration 37900, train loss = 0.148542, train accuracy = 0.992188\n",
      "[2018-06-02 03:36:57.749179] Iteration 38000, train loss = 0.170688, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.920700\n",
      "[2018-06-02 03:37:14.795179] Iteration 38100, train loss = 0.150325, train accuracy = 1.000000\n",
      "[2018-06-02 03:37:28.826179] Iteration 38200, train loss = 0.156990, train accuracy = 1.000000\n",
      "[2018-06-02 03:37:42.895179] Iteration 38300, train loss = 0.139784, train accuracy = 1.000000\n",
      "[2018-06-02 03:37:56.921179] Iteration 38400, train loss = 0.164342, train accuracy = 0.984375\n",
      "[2018-06-02 03:38:11.024179] Iteration 38500, train loss = 0.148156, train accuracy = 1.000000\n",
      "[2018-06-02 03:38:25.064179] Iteration 38600, train loss = 0.146371, train accuracy = 1.000000\n",
      "[2018-06-02 03:38:39.127179] Iteration 38700, train loss = 0.185741, train accuracy = 0.976562\n",
      "[2018-06-02 03:38:53.147179] Iteration 38800, train loss = 0.166440, train accuracy = 0.992188\n",
      "[2018-06-02 03:39:07.145179] Iteration 38900, train loss = 0.153847, train accuracy = 0.992188\n",
      "[2018-06-02 03:39:21.203179] Iteration 39000, train loss = 0.141082, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921100\n",
      "[2018-06-02 03:39:38.359179] Iteration 39100, train loss = 0.139829, train accuracy = 1.000000\n",
      "[2018-06-02 03:39:52.364179] Iteration 39200, train loss = 0.151024, train accuracy = 0.992188\n",
      "[2018-06-02 03:40:06.345179] Iteration 39300, train loss = 0.150275, train accuracy = 0.992188\n",
      "[2018-06-02 03:40:20.456179] Iteration 39400, train loss = 0.147923, train accuracy = 1.000000\n",
      "[2018-06-02 03:40:34.495179] Iteration 39500, train loss = 0.146641, train accuracy = 0.992188\n",
      "[2018-06-02 03:40:48.507179] Iteration 39600, train loss = 0.158378, train accuracy = 0.984375\n",
      "[2018-06-02 03:41:02.616179] Iteration 39700, train loss = 0.146374, train accuracy = 0.992188\n",
      "[2018-06-02 03:41:16.623179] Iteration 39800, train loss = 0.143109, train accuracy = 1.000000\n",
      "[2018-06-02 03:41:30.696179] Iteration 39900, train loss = 0.152022, train accuracy = 0.992188\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四轮  量化  \n",
    "prune_rate = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.920700\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.03125    -0.0625     -0.00487323  0.125       0.00081287 -0.03125\n",
      "  0.015625    0.03125     0.015625   -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,1)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 03:45:22.661179] Iteration 100, train loss = 0.151596, train accuracy = 0.992188\n",
      "[2018-06-02 03:45:36.691179] Iteration 200, train loss = 0.153536, train accuracy = 0.992188\n",
      "[2018-06-02 03:45:50.697179] Iteration 300, train loss = 0.154657, train accuracy = 1.000000\n",
      "[2018-06-02 03:46:04.749179] Iteration 400, train loss = 0.153922, train accuracy = 1.000000\n",
      "[2018-06-02 03:46:18.806179] Iteration 500, train loss = 0.187812, train accuracy = 0.976562\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 03:46:32.884179] Iteration 600, train loss = 0.151425, train accuracy = 1.000000\n",
      "[2018-06-02 03:46:46.915179] Iteration 700, train loss = 0.155951, train accuracy = 0.992188\n",
      "[2018-06-02 03:47:01.045179] Iteration 800, train loss = 0.199580, train accuracy = 0.968750\n",
      "[2018-06-02 03:47:15.187179] Iteration 900, train loss = 0.176851, train accuracy = 0.992188\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "  if step <= 500:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune后的准确率\n",
      "Test accuracy = 0.917800\n"
     ]
    }
   ],
   "source": [
    "print('prune后的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.03125    -0.0625     -0.00390625  0.125       0.         -0.03125\n",
      "  0.015625    0.03125     0.015625   -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
