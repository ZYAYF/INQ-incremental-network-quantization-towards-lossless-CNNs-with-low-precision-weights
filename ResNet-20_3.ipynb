{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "# import ipdb\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('residual_net_n', 3, '')\n",
    "tf.app.flags.DEFINE_string('train_tf_path', './data/train.tf', '')\n",
    "tf.app.flags.DEFINE_string('val_tf_path', './data/test.tf', '')\n",
    "tf.app.flags.DEFINE_integer('train_batch_size', 128, '')\n",
    "tf.app.flags.DEFINE_integer('val_batch_size', 100, '')\n",
    "tf.app.flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay')\n",
    "tf.app.flags.DEFINE_integer('summary_interval', 100, 'Interval for summary.')\n",
    "tf.app.flags.DEFINE_integer('val_interval', 1000, 'Interval for evaluation.')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000, 'Maximum number of iterations.')\n",
    "tf.app.flags.DEFINE_integer('save_interval', 5000, '')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  with tf.device('/cpu:0'):\n",
    "    params = tf.constant(embedding_params)\n",
    "    embedding = tf.gather(params, label)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_in, n_out, k, s, p='SAME', bias=False, scope='conv'):\n",
    "  with tf.variable_scope(scope):\n",
    "    kernel = tf.Variable(\n",
    "      tf.truncated_normal([k, k, n_in, n_out],\n",
    "        stddev=math.sqrt(2/(k*k*n_in))),\n",
    "      name='weight')\n",
    "    tf.add_to_collection('weights', kernel)\n",
    "    conv = tf.nn.conv2d(x, kernel, [1,s,s,1], padding=p)\n",
    "    if bias:\n",
    "      bias = tf.get_variable('bias', [n_out], initializer=tf.constant_initializer(0.0))\n",
    "      tf.add_to_collection('biases', bias)\n",
    "      conv = tf.nn.bias_add(conv, bias)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn', affine=True):\n",
    "  \"\"\"\n",
    "  Batch normalization on convolutional maps.\n",
    "  Args:\n",
    "    x: Tensor, 4D BHWD input maps\n",
    "    n_out: integer, depth of input maps\n",
    "    phase_train: boolean tf.Variable, true indicates training phase\n",
    "    scope: string, variable scope\n",
    "    affine: whether to affine-transform outputs\n",
    "  Return:\n",
    "    normed: batch-normalized maps\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "      name='beta', trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "      name='gamma', trainable=affine)\n",
    "    tf.add_to_collection('biases', beta)\n",
    "    tf.add_to_collection('weights', gamma)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "      ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "      with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train,\n",
    "      mean_var_with_update,\n",
    "      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \n",
    "      beta, gamma, 1e-3, affine)\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x, n_in, n_out, subsample, phase_train, scope='res_block'):\n",
    "  with tf.variable_scope(scope):\n",
    "    if subsample:\n",
    "      y = conv2d(x, n_in, n_out, 3, 2, 'SAME', False, scope='conv_1')\n",
    "      shortcut = conv2d(x, n_in, n_out, 3, 2, 'SAME',\n",
    "                False, scope='shortcut')\n",
    "    else:\n",
    "      y = conv2d(x, n_in, n_out, 3, 1, 'SAME', False, scope='conv_1')\n",
    "      shortcut = tf.identity(x, name='shortcut')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_1')\n",
    "    y = tf.nn.relu(y, name='relu_1')\n",
    "    y = conv2d(y, n_out, n_out, 3, 1, 'SAME', True, scope='conv_2')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_2')\n",
    "    y = y + shortcut\n",
    "    y = tf.nn.relu(y, name='relu_2')\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_group(x, n_in, n_out, n, first_subsample, phase_train, scope='res_group'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = residual_block(x, n_in, n_out, first_subsample, phase_train, scope='block_1')\n",
    "    for i in range(n - 1):\n",
    "      y = residual_block(y, n_out, n_out, False, phase_train, scope='block_%d' % (i + 2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_net(x, n, n_classes, phase_train, scope='res_net'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = conv2d(x, 3, 16, 3, 1, 'SAME', False, scope='conv_init')\n",
    "    y = batch_norm(y, 16, phase_train, scope='bn_init')\n",
    "    y = tf.nn.relu(y, name='relu_init')\n",
    "    y = residual_group(y, 16, 16, n, False, phase_train, scope='group_1')\n",
    "    y = residual_group(y, 16, 32, n, True, phase_train, scope='group_2')\n",
    "    y = residual_group(y, 32, 64, n, True, phase_train, scope='group_3')\n",
    "#     y = conv2d(y, 64, n_classes, 1, 1, 'SAME', True, scope='conv_last')\n",
    "    y = tf.nn.avg_pool(y, [1, 8, 8, 1], [1, 1, 1, 1], 'VALID', name='avg_pool')\n",
    "    y = tf.reshape(y, [-1, 64])\n",
    "    w = tf.get_variable(name='weight_fc', shape=[64, n_classes], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('weights', w)\n",
    "    b = tf.get_variable(name='weight_biase', shape=[n_classes], initializer=tf.constant_initializer(0))\n",
    "    tf.add_to_collection('last_biases', b)\n",
    "    y = tf.matmul(y, w) + b\n",
    "#     y = tf.squeeze(y, squeeze_dims=[1, 2])\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, labels, scope='loss'):\n",
    "  with tf.variable_scope(scope):\n",
    "    # entropy loss\n",
    "    targets = one_hot_embedding(labels, 10)\n",
    "    entropy_loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets),\n",
    "      name='entropy_loss')\n",
    "    tf.add_to_collection('losses', entropy_loss)\n",
    "    # weight l2 decay loss\n",
    "    weight_l2_losses = [tf.nn.l2_loss(o) for o in tf.get_collection('weights')]\n",
    "    weight_decay_loss = FLAGS.weight_decay*tf.add_n(weight_l2_losses)\n",
    "    tf.add_to_collection('losses', weight_decay_loss)\n",
    "  # for var in tf.get_collection('losses'):\n",
    "    # tf.scalar_summary('losses/' + var.op.name, var)\n",
    "  # total loss\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _accuracy(logits, gt_label, scope='accuracy'):\n",
    "  with tf.variable_scope(scope):\n",
    "    pred_label = tf.argmax(logits, 1)\n",
    "    acc = 1.0 - tf.nn.zero_fraction(\n",
    "      tf.cast(tf.equal(pred_label, gt_label), tf.int32))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_op(loss, global_step, learning_rate):\n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params, name='gradients')\n",
    "  optim = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "  update = optim.apply_gradients(zip(gradients, params))\n",
    "  with tf.control_dependencies([update]):\n",
    "    train_op = tf.no_op(name='train_op')\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "  # meanstd = joblib.load(FLAGS.mean_std_path)\n",
    "  # mean, std = meanstd['mean'], meanstd['std']\n",
    "  mean = [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(\n",
    "    image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_batch(train_records_path, batch_size):\n",
    "  with tf.variable_scope('train_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "      train_image = normalize_image(train_image)\n",
    "      train_image = random_distort_image(train_image)\n",
    "      train_image_batch, train_label_batch = tf.train.shuffle_batch(\n",
    "        [train_image, train_label], batch_size=batch_size, num_threads=4,\n",
    "        capacity=50000,\n",
    "        min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  with tf.variable_scope('evaluate_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "      test_image = normalize_image(test_image)\n",
    "      test_image_batch, test_label_batch = tf.train.batch(\n",
    "        [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "        capacity=10000)\n",
    "  return test_image_batch, test_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-5533cdbb8cb6>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch = make_train_batch(FLAGS.train_tf_path, FLAGS.train_batch_size)\n",
    "val_image_batch, val_label_batch = make_validation_batch(FLAGS.val_tf_path, FLAGS.val_batch_size)\n",
    "\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "\n",
    "\n",
    "logits = residual_net(image_batch, FLAGS.residual_net_n, 10, phase_train)\n",
    "\n",
    "\n",
    "loss = _loss(logits, label_batch)\n",
    "accuracy = _accuracy(logits, label_batch)\n",
    "\n",
    "# train one step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "INFO:tensorflow:Restoring parameters from ./full_precision/res20/model/res.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-train_batch/input_producer-train_batch/input_producer/input_producer_EnqueueMany, started daemon 140462677554944)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140463164069632)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140463172462336)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140463180855040)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140462669162240)>,\n",
       " <Thread(QueueRunnerThread-evaluate_batch/input_producer-evaluate_batch/input_producer/input_producer_EnqueueMany, started daemon 140462660769536)>,\n",
       " <Thread(QueueRunnerThread-evaluate_batch/batch/fifo_queue-evaluate_batch/batch/fifo_queue_enqueue, started daemon 140462652376832)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print('Initializing...')\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'./full_precision/res20/model/res.ckpt')\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test accuracy = 0.921200\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# validation\n",
    "\n",
    "print('Evaluating...')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "  fetches = [logits, label_batch, loss]\n",
    "  session_outputs = sess.run(\n",
    "    fetches, {phase_train.name: False})\n",
    "  val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "  val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "  val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(\n",
    "  pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_prune_on_grads(grads_and_vars, dict_nzidx):\n",
    "    for key, nzidx in dict_nzidx.items():\n",
    "        count = 0\n",
    "        for grad, var in grads_and_vars:\n",
    "            if var.name == key:\n",
    "                nzidx_obj = tf.cast(tf.constant(sess.run(dict_nzidx[key])), tf.float32)\n",
    "                grads_and_vars[count] = (tf.multiply(nzidx_obj, grad), var)\n",
    "            count += 1\n",
    "    return grads_and_vars\n",
    "\n",
    "def apply_inq(weights, inq_dict, var_name, prune_rate):  \n",
    "    for target in var_name:\n",
    "        wl = target\n",
    "        bit = 8\n",
    "\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = sess.run(weight_obj)\n",
    "\n",
    "        weight_rest = np.reshape(weight_arr, [-1])\n",
    "        dic_tem = np.reshape(sess.run(inq_dict[wl]), [-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest, [-1]))), 0)\n",
    "        num_prune = int(len(weight_rest) * prune_rate)\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]]\n",
    "\n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        n2 = n1 + 1 - bit / 4\n",
    "        upper_bound = 2 ** (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        lower_bound = 2 ** (n1 + 1 - bit / 4)\n",
    "\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2 ** (np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0] * 4 / 3)))) * np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr = np.reshape(weight_rest, np.shape(weight_arr))\n",
    "        dic_tem[idx_rest[:num_prune]] = np.zeros_like(dic_tem[idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = tf.cast(np.reshape(dic_tem, np.shape(sess.run(inq_dict[wl]))), tf.float32)\n",
    "        sess.run(weights[wl].assign(weight_arr))\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮  量化\n",
    "prune_rate =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.921200\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.06590594 -0.06288774  0.03332484  0.15145954  0.02605803 -0.04664146\n",
      "  0.02131429 -0.0513982  -0.00593484 -0.13121311]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.5)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 12:13:24.733235] Iteration 100, train loss = 1.668981, train accuracy = 0.398438\n",
      "[2018-07-16 12:13:28.392934] Iteration 200, train loss = 1.283499, train accuracy = 0.539062\n",
      "[2018-07-16 12:13:32.073727] Iteration 300, train loss = 1.237708, train accuracy = 0.578125\n",
      "[2018-07-16 12:13:35.867228] Iteration 400, train loss = 1.069113, train accuracy = 0.664062\n",
      "[2018-07-16 12:13:39.607867] Iteration 500, train loss = 0.958440, train accuracy = 0.703125\n",
      "[2018-07-16 12:13:43.329169] Iteration 600, train loss = 0.791875, train accuracy = 0.765625\n",
      "[2018-07-16 12:13:47.039085] Iteration 700, train loss = 0.643393, train accuracy = 0.804688\n",
      "[2018-07-16 12:13:50.674877] Iteration 800, train loss = 0.850728, train accuracy = 0.734375\n",
      "[2018-07-16 12:13:54.349189] Iteration 900, train loss = 0.749124, train accuracy = 0.765625\n",
      "[2018-07-16 12:13:57.987759] Iteration 1000, train loss = 0.629714, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.619100\n",
      "[2018-07-16 12:14:02.768246] Iteration 1100, train loss = 0.826360, train accuracy = 0.718750\n",
      "[2018-07-16 12:14:06.423231] Iteration 1200, train loss = 0.768248, train accuracy = 0.718750\n",
      "[2018-07-16 12:14:10.071279] Iteration 1300, train loss = 0.648880, train accuracy = 0.804688\n",
      "[2018-07-16 12:14:13.759880] Iteration 1400, train loss = 0.637356, train accuracy = 0.804688\n",
      "[2018-07-16 12:14:17.460005] Iteration 1500, train loss = 0.606728, train accuracy = 0.796875\n",
      "[2018-07-16 12:14:21.150926] Iteration 1600, train loss = 0.570117, train accuracy = 0.843750\n",
      "[2018-07-16 12:14:24.791951] Iteration 1700, train loss = 0.637746, train accuracy = 0.804688\n",
      "[2018-07-16 12:14:28.436960] Iteration 1800, train loss = 0.538498, train accuracy = 0.828125\n",
      "[2018-07-16 12:14:32.111571] Iteration 1900, train loss = 0.461107, train accuracy = 0.859375\n",
      "[2018-07-16 12:14:35.764708] Iteration 2000, train loss = 0.474614, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.754900\n",
      "[2018-07-16 12:14:40.542767] Iteration 2100, train loss = 0.608857, train accuracy = 0.828125\n",
      "[2018-07-16 12:14:44.182121] Iteration 2200, train loss = 0.679725, train accuracy = 0.773438\n",
      "[2018-07-16 12:14:47.832229] Iteration 2300, train loss = 0.627207, train accuracy = 0.796875\n",
      "[2018-07-16 12:14:51.475601] Iteration 2400, train loss = 0.464086, train accuracy = 0.843750\n",
      "[2018-07-16 12:14:55.183155] Iteration 2500, train loss = 0.454104, train accuracy = 0.851562\n",
      "[2018-07-16 12:14:58.840673] Iteration 2600, train loss = 0.431514, train accuracy = 0.890625\n",
      "[2018-07-16 12:15:02.532800] Iteration 2700, train loss = 0.586105, train accuracy = 0.812500\n",
      "[2018-07-16 12:15:06.179225] Iteration 2800, train loss = 0.676755, train accuracy = 0.796875\n",
      "[2018-07-16 12:15:09.813585] Iteration 2900, train loss = 0.589817, train accuracy = 0.812500\n",
      "[2018-07-16 12:15:13.483575] Iteration 3000, train loss = 0.539402, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.795100\n",
      "[2018-07-16 12:15:18.251816] Iteration 3100, train loss = 0.502553, train accuracy = 0.843750\n",
      "[2018-07-16 12:15:21.922892] Iteration 3200, train loss = 0.466702, train accuracy = 0.890625\n",
      "[2018-07-16 12:15:25.584863] Iteration 3300, train loss = 0.373640, train accuracy = 0.867188\n",
      "[2018-07-16 12:15:29.238954] Iteration 3400, train loss = 0.520281, train accuracy = 0.843750\n",
      "[2018-07-16 12:15:32.904723] Iteration 3500, train loss = 0.577295, train accuracy = 0.812500\n",
      "[2018-07-16 12:15:36.599861] Iteration 3600, train loss = 0.428073, train accuracy = 0.898438\n",
      "[2018-07-16 12:15:40.259055] Iteration 3700, train loss = 0.414508, train accuracy = 0.875000\n",
      "[2018-07-16 12:15:43.909364] Iteration 3800, train loss = 0.404101, train accuracy = 0.890625\n",
      "[2018-07-16 12:15:47.547906] Iteration 3900, train loss = 0.454545, train accuracy = 0.890625\n",
      "[2018-07-16 12:15:51.226466] Iteration 4000, train loss = 0.549435, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.810800\n",
      "[2018-07-16 12:15:56.009799] Iteration 4100, train loss = 0.483819, train accuracy = 0.828125\n",
      "[2018-07-16 12:15:59.686245] Iteration 4200, train loss = 0.340719, train accuracy = 0.914062\n",
      "[2018-07-16 12:16:03.326772] Iteration 4300, train loss = 0.507952, train accuracy = 0.828125\n",
      "[2018-07-16 12:16:06.981886] Iteration 4400, train loss = 0.351049, train accuracy = 0.914062\n",
      "[2018-07-16 12:16:10.623087] Iteration 4500, train loss = 0.355632, train accuracy = 0.898438\n",
      "[2018-07-16 12:16:14.330083] Iteration 4600, train loss = 0.483937, train accuracy = 0.867188\n",
      "[2018-07-16 12:16:17.961832] Iteration 4700, train loss = 0.328673, train accuracy = 0.937500\n",
      "[2018-07-16 12:16:21.608290] Iteration 4800, train loss = 0.402997, train accuracy = 0.906250\n",
      "[2018-07-16 12:16:25.259813] Iteration 4900, train loss = 0.323642, train accuracy = 0.921875\n",
      "[2018-07-16 12:16:28.899461] Iteration 5000, train loss = 0.457305, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.820400\n",
      "[2018-07-16 12:16:33.666495] Iteration 5100, train loss = 0.402169, train accuracy = 0.875000\n",
      "[2018-07-16 12:16:37.317802] Iteration 5200, train loss = 0.469838, train accuracy = 0.875000\n",
      "[2018-07-16 12:16:40.953120] Iteration 5300, train loss = 0.399373, train accuracy = 0.882812\n",
      "[2018-07-16 12:16:44.599143] Iteration 5400, train loss = 0.416531, train accuracy = 0.843750\n",
      "[2018-07-16 12:16:48.242167] Iteration 5500, train loss = 0.555451, train accuracy = 0.828125\n",
      "[2018-07-16 12:16:51.938158] Iteration 5600, train loss = 0.412881, train accuracy = 0.875000\n",
      "[2018-07-16 12:16:55.701533] Iteration 5700, train loss = 0.443445, train accuracy = 0.875000\n",
      "[2018-07-16 12:16:59.383987] Iteration 5800, train loss = 0.389091, train accuracy = 0.890625\n",
      "[2018-07-16 12:17:03.022829] Iteration 5900, train loss = 0.368198, train accuracy = 0.914062\n",
      "[2018-07-16 12:17:06.695620] Iteration 6000, train loss = 0.403469, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.804700\n",
      "[2018-07-16 12:17:11.477185] Iteration 6100, train loss = 0.544227, train accuracy = 0.851562\n",
      "[2018-07-16 12:17:15.125508] Iteration 6200, train loss = 0.476296, train accuracy = 0.859375\n",
      "[2018-07-16 12:17:18.749007] Iteration 6300, train loss = 0.459311, train accuracy = 0.867188\n",
      "[2018-07-16 12:17:22.396189] Iteration 6400, train loss = 0.271286, train accuracy = 0.921875\n",
      "[2018-07-16 12:17:26.033698] Iteration 6500, train loss = 0.302702, train accuracy = 0.945312\n",
      "[2018-07-16 12:17:29.658990] Iteration 6600, train loss = 0.454593, train accuracy = 0.875000\n",
      "[2018-07-16 12:17:33.287331] Iteration 6700, train loss = 0.391863, train accuracy = 0.890625\n",
      "[2018-07-16 12:17:36.982436] Iteration 6800, train loss = 0.446220, train accuracy = 0.875000\n",
      "[2018-07-16 12:17:40.636829] Iteration 6900, train loss = 0.409450, train accuracy = 0.898438\n",
      "[2018-07-16 12:17:44.277979] Iteration 7000, train loss = 0.388987, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.817600\n",
      "[2018-07-16 12:17:49.056147] Iteration 7100, train loss = 0.499239, train accuracy = 0.843750\n",
      "[2018-07-16 12:17:52.721125] Iteration 7200, train loss = 0.464133, train accuracy = 0.867188\n",
      "[2018-07-16 12:17:56.354626] Iteration 7300, train loss = 0.353892, train accuracy = 0.914062\n",
      "[2018-07-16 12:17:59.988346] Iteration 7400, train loss = 0.420561, train accuracy = 0.890625\n",
      "[2018-07-16 12:18:03.629783] Iteration 7500, train loss = 0.337147, train accuracy = 0.898438\n",
      "[2018-07-16 12:18:07.259375] Iteration 7600, train loss = 0.411817, train accuracy = 0.875000\n",
      "[2018-07-16 12:18:10.913520] Iteration 7700, train loss = 0.370299, train accuracy = 0.898438\n",
      "[2018-07-16 12:18:14.560274] Iteration 7800, train loss = 0.328890, train accuracy = 0.906250\n",
      "[2018-07-16 12:18:18.242709] Iteration 7900, train loss = 0.372283, train accuracy = 0.898438\n",
      "[2018-07-16 12:18:21.909050] Iteration 8000, train loss = 0.332857, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.849200\n",
      "[2018-07-16 12:18:26.696107] Iteration 8100, train loss = 0.419170, train accuracy = 0.875000\n",
      "[2018-07-16 12:18:30.329336] Iteration 8200, train loss = 0.330364, train accuracy = 0.906250\n",
      "[2018-07-16 12:18:33.988500] Iteration 8300, train loss = 0.432114, train accuracy = 0.859375\n",
      "[2018-07-16 12:18:37.643582] Iteration 8400, train loss = 0.350687, train accuracy = 0.882812\n",
      "[2018-07-16 12:18:41.272283] Iteration 8500, train loss = 0.325418, train accuracy = 0.914062\n",
      "[2018-07-16 12:18:44.899633] Iteration 8600, train loss = 0.424613, train accuracy = 0.890625\n",
      "[2018-07-16 12:18:48.533414] Iteration 8700, train loss = 0.369719, train accuracy = 0.906250\n",
      "[2018-07-16 12:18:52.176787] Iteration 8800, train loss = 0.433944, train accuracy = 0.859375\n",
      "[2018-07-16 12:18:55.844336] Iteration 8900, train loss = 0.251040, train accuracy = 0.937500\n",
      "[2018-07-16 12:18:59.491689] Iteration 9000, train loss = 0.265327, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.854800\n",
      "[2018-07-16 12:19:04.416702] Iteration 9100, train loss = 0.343523, train accuracy = 0.898438\n",
      "[2018-07-16 12:19:08.052108] Iteration 9200, train loss = 0.360466, train accuracy = 0.867188\n",
      "[2018-07-16 12:19:11.694504] Iteration 9300, train loss = 0.449075, train accuracy = 0.890625\n",
      "[2018-07-16 12:19:15.332297] Iteration 9400, train loss = 0.403554, train accuracy = 0.882812\n",
      "[2018-07-16 12:19:18.987553] Iteration 9500, train loss = 0.441730, train accuracy = 0.898438\n",
      "[2018-07-16 12:19:22.643543] Iteration 9600, train loss = 0.343338, train accuracy = 0.906250\n",
      "[2018-07-16 12:19:26.284201] Iteration 9700, train loss = 0.375095, train accuracy = 0.882812\n",
      "[2018-07-16 12:19:29.928728] Iteration 9800, train loss = 0.389704, train accuracy = 0.843750\n",
      "[2018-07-16 12:19:33.563953] Iteration 9900, train loss = 0.319985, train accuracy = 0.906250\n",
      "[2018-07-16 12:19:37.198233] Iteration 10000, train loss = 0.238643, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.828000\n",
      "[2018-07-16 12:19:41.967160] Iteration 10100, train loss = 0.317268, train accuracy = 0.906250\n",
      "[2018-07-16 12:19:45.648007] Iteration 10200, train loss = 0.372766, train accuracy = 0.914062\n",
      "[2018-07-16 12:19:49.273004] Iteration 10300, train loss = 0.439632, train accuracy = 0.851562\n",
      "[2018-07-16 12:19:52.952858] Iteration 10400, train loss = 0.382467, train accuracy = 0.906250\n",
      "[2018-07-16 12:19:56.630942] Iteration 10500, train loss = 0.365687, train accuracy = 0.882812\n",
      "[2018-07-16 12:20:00.272021] Iteration 10600, train loss = 0.360704, train accuracy = 0.906250\n",
      "[2018-07-16 12:20:03.907579] Iteration 10700, train loss = 0.299952, train accuracy = 0.906250\n",
      "[2018-07-16 12:20:07.572375] Iteration 10800, train loss = 0.349179, train accuracy = 0.890625\n",
      "[2018-07-16 12:20:11.225887] Iteration 10900, train loss = 0.350433, train accuracy = 0.882812\n",
      "[2018-07-16 12:20:14.870826] Iteration 11000, train loss = 0.446436, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.843600\n",
      "[2018-07-16 12:20:19.635968] Iteration 11100, train loss = 0.435390, train accuracy = 0.835938\n",
      "[2018-07-16 12:20:23.268967] Iteration 11200, train loss = 0.416990, train accuracy = 0.875000\n",
      "[2018-07-16 12:20:26.977230] Iteration 11300, train loss = 0.279973, train accuracy = 0.929688\n",
      "[2018-07-16 12:20:30.596640] Iteration 11400, train loss = 0.344085, train accuracy = 0.898438\n",
      "[2018-07-16 12:20:34.235000] Iteration 11500, train loss = 0.387884, train accuracy = 0.882812\n",
      "[2018-07-16 12:20:37.883530] Iteration 11600, train loss = 0.311318, train accuracy = 0.890625\n",
      "[2018-07-16 12:20:41.520650] Iteration 11700, train loss = 0.358415, train accuracy = 0.898438\n",
      "[2018-07-16 12:20:45.160838] Iteration 11800, train loss = 0.362706, train accuracy = 0.898438\n",
      "[2018-07-16 12:20:48.814972] Iteration 11900, train loss = 0.325744, train accuracy = 0.921875\n",
      "[2018-07-16 12:20:52.462118] Iteration 12000, train loss = 0.291707, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.862400\n",
      "[2018-07-16 12:20:57.248706] Iteration 12100, train loss = 0.266474, train accuracy = 0.953125\n",
      "[2018-07-16 12:21:00.892065] Iteration 12200, train loss = 0.312607, train accuracy = 0.906250\n",
      "[2018-07-16 12:21:04.535715] Iteration 12300, train loss = 0.264585, train accuracy = 0.929688\n",
      "[2018-07-16 12:21:08.249773] Iteration 12400, train loss = 0.289234, train accuracy = 0.914062\n",
      "[2018-07-16 12:21:11.880159] Iteration 12500, train loss = 0.231675, train accuracy = 0.945312\n",
      "[2018-07-16 12:21:15.509735] Iteration 12600, train loss = 0.385632, train accuracy = 0.906250\n",
      "[2018-07-16 12:21:19.134950] Iteration 12700, train loss = 0.266645, train accuracy = 0.929688\n",
      "[2018-07-16 12:21:22.785740] Iteration 12800, train loss = 0.286900, train accuracy = 0.914062\n",
      "[2018-07-16 12:21:26.432877] Iteration 12900, train loss = 0.361639, train accuracy = 0.921875\n",
      "[2018-07-16 12:21:30.081796] Iteration 13000, train loss = 0.315625, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.867700\n",
      "[2018-07-16 12:21:34.867750] Iteration 13100, train loss = 0.433187, train accuracy = 0.882812\n",
      "[2018-07-16 12:21:38.515159] Iteration 13200, train loss = 0.359412, train accuracy = 0.882812\n",
      "[2018-07-16 12:21:42.155054] Iteration 13300, train loss = 0.270829, train accuracy = 0.945312\n",
      "[2018-07-16 12:21:45.801997] Iteration 13400, train loss = 0.288643, train accuracy = 0.921875\n",
      "[2018-07-16 12:21:49.537108] Iteration 13500, train loss = 0.237923, train accuracy = 0.937500\n",
      "[2018-07-16 12:21:53.182903] Iteration 13600, train loss = 0.250230, train accuracy = 0.953125\n",
      "[2018-07-16 12:21:56.813192] Iteration 13700, train loss = 0.275833, train accuracy = 0.921875\n",
      "[2018-07-16 12:22:00.449018] Iteration 13800, train loss = 0.282957, train accuracy = 0.914062\n",
      "[2018-07-16 12:22:04.104802] Iteration 13900, train loss = 0.262381, train accuracy = 0.937500\n",
      "[2018-07-16 12:22:07.736151] Iteration 14000, train loss = 0.332557, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.855600\n",
      "[2018-07-16 12:22:12.493920] Iteration 14100, train loss = 0.220605, train accuracy = 0.968750\n",
      "[2018-07-16 12:22:16.134185] Iteration 14200, train loss = 0.239052, train accuracy = 0.945312\n",
      "[2018-07-16 12:22:19.781652] Iteration 14300, train loss = 0.292644, train accuracy = 0.929688\n",
      "[2018-07-16 12:22:23.433319] Iteration 14400, train loss = 0.342159, train accuracy = 0.906250\n",
      "[2018-07-16 12:22:27.090153] Iteration 14500, train loss = 0.338539, train accuracy = 0.906250\n",
      "[2018-07-16 12:22:30.770734] Iteration 14600, train loss = 0.478789, train accuracy = 0.875000\n",
      "[2018-07-16 12:22:34.433010] Iteration 14700, train loss = 0.274828, train accuracy = 0.921875\n",
      "[2018-07-16 12:22:38.094882] Iteration 14800, train loss = 0.277144, train accuracy = 0.937500\n",
      "[2018-07-16 12:22:41.727447] Iteration 14900, train loss = 0.220850, train accuracy = 0.953125\n",
      "[2018-07-16 12:22:45.366527] Iteration 15000, train loss = 0.240691, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.863100\n",
      "[2018-07-16 12:22:50.130008] Iteration 15100, train loss = 0.251827, train accuracy = 0.937500\n",
      "[2018-07-16 12:22:53.755861] Iteration 15200, train loss = 0.364373, train accuracy = 0.898438\n",
      "[2018-07-16 12:22:57.402221] Iteration 15300, train loss = 0.256213, train accuracy = 0.945312\n",
      "[2018-07-16 12:23:01.041511] Iteration 15400, train loss = 0.210546, train accuracy = 0.960938\n",
      "[2018-07-16 12:23:04.675908] Iteration 15500, train loss = 0.248765, train accuracy = 0.937500\n",
      "[2018-07-16 12:23:08.330071] Iteration 15600, train loss = 0.316466, train accuracy = 0.898438\n",
      "[2018-07-16 12:23:11.969314] Iteration 15700, train loss = 0.305736, train accuracy = 0.906250\n",
      "[2018-07-16 12:23:15.705381] Iteration 15800, train loss = 0.242705, train accuracy = 0.937500\n",
      "[2018-07-16 12:23:19.360963] Iteration 15900, train loss = 0.275703, train accuracy = 0.921875\n",
      "[2018-07-16 12:23:23.017916] Iteration 16000, train loss = 0.293059, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.833600\n",
      "[2018-07-16 12:23:27.821522] Iteration 16100, train loss = 0.275780, train accuracy = 0.929688\n",
      "[2018-07-16 12:23:31.471332] Iteration 16200, train loss = 0.323152, train accuracy = 0.921875\n",
      "[2018-07-16 12:23:35.119358] Iteration 16300, train loss = 0.325095, train accuracy = 0.914062\n",
      "[2018-07-16 12:23:38.753050] Iteration 16400, train loss = 0.260485, train accuracy = 0.945312\n",
      "[2018-07-16 12:23:42.396687] Iteration 16500, train loss = 0.234075, train accuracy = 0.953125\n",
      "[2018-07-16 12:23:46.044065] Iteration 16600, train loss = 0.283432, train accuracy = 0.937500\n",
      "[2018-07-16 12:23:49.683139] Iteration 16700, train loss = 0.385930, train accuracy = 0.906250\n",
      "[2018-07-16 12:23:53.361643] Iteration 16800, train loss = 0.340613, train accuracy = 0.890625\n",
      "[2018-07-16 12:23:57.078962] Iteration 16900, train loss = 0.301664, train accuracy = 0.906250\n",
      "[2018-07-16 12:24:00.744758] Iteration 17000, train loss = 0.247765, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.855100\n",
      "[2018-07-16 12:24:05.541343] Iteration 17100, train loss = 0.323063, train accuracy = 0.921875\n",
      "[2018-07-16 12:24:09.167509] Iteration 17200, train loss = 0.289885, train accuracy = 0.914062\n",
      "[2018-07-16 12:24:12.817125] Iteration 17300, train loss = 0.280574, train accuracy = 0.914062\n",
      "[2018-07-16 12:24:16.466751] Iteration 17400, train loss = 0.304728, train accuracy = 0.890625\n",
      "[2018-07-16 12:24:20.110345] Iteration 17500, train loss = 0.297150, train accuracy = 0.929688\n",
      "[2018-07-16 12:24:23.756110] Iteration 17600, train loss = 0.306284, train accuracy = 0.898438\n",
      "[2018-07-16 12:24:27.388330] Iteration 17700, train loss = 0.360696, train accuracy = 0.882812\n",
      "[2018-07-16 12:24:31.023685] Iteration 17800, train loss = 0.318632, train accuracy = 0.914062\n",
      "[2018-07-16 12:24:34.659872] Iteration 17900, train loss = 0.261461, train accuracy = 0.953125\n",
      "[2018-07-16 12:24:38.412515] Iteration 18000, train loss = 0.245423, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.863800\n",
      "[2018-07-16 12:24:43.173072] Iteration 18100, train loss = 0.253094, train accuracy = 0.953125\n",
      "[2018-07-16 12:24:46.815573] Iteration 18200, train loss = 0.255724, train accuracy = 0.937500\n",
      "[2018-07-16 12:24:50.475181] Iteration 18300, train loss = 0.247406, train accuracy = 0.937500\n",
      "[2018-07-16 12:24:54.121067] Iteration 18400, train loss = 0.239353, train accuracy = 0.953125\n",
      "[2018-07-16 12:24:57.767081] Iteration 18500, train loss = 0.252072, train accuracy = 0.953125\n",
      "[2018-07-16 12:25:01.402812] Iteration 18600, train loss = 0.377288, train accuracy = 0.890625\n",
      "[2018-07-16 12:25:05.047926] Iteration 18700, train loss = 0.268833, train accuracy = 0.921875\n",
      "[2018-07-16 12:25:08.697852] Iteration 18800, train loss = 0.223396, train accuracy = 0.953125\n",
      "[2018-07-16 12:25:12.344145] Iteration 18900, train loss = 0.388360, train accuracy = 0.890625\n",
      "[2018-07-16 12:25:15.978022] Iteration 19000, train loss = 0.346296, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.857000\n",
      "[2018-07-16 12:25:20.836367] Iteration 19100, train loss = 0.245459, train accuracy = 0.937500\n",
      "[2018-07-16 12:25:24.500414] Iteration 19200, train loss = 0.164535, train accuracy = 0.976562\n",
      "[2018-07-16 12:25:28.133376] Iteration 19300, train loss = 0.316250, train accuracy = 0.929688\n",
      "[2018-07-16 12:25:31.788992] Iteration 19400, train loss = 0.218666, train accuracy = 0.968750\n",
      "[2018-07-16 12:25:35.443048] Iteration 19500, train loss = 0.280139, train accuracy = 0.929688\n",
      "[2018-07-16 12:25:39.095174] Iteration 19600, train loss = 0.166880, train accuracy = 0.968750\n",
      "[2018-07-16 12:25:42.749635] Iteration 19700, train loss = 0.302944, train accuracy = 0.890625\n",
      "[2018-07-16 12:25:46.390094] Iteration 19800, train loss = 0.235782, train accuracy = 0.953125\n",
      "[2018-07-16 12:25:50.027920] Iteration 19900, train loss = 0.293568, train accuracy = 0.937500\n",
      "[2018-07-16 12:25:53.656703] Iteration 20000, train loss = 0.224710, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.871600\n",
      "[2018-07-16 12:25:58.407537] Iteration 20100, train loss = 0.240230, train accuracy = 0.945312\n",
      "[2018-07-16 12:26:02.110608] Iteration 20200, train loss = 0.290829, train accuracy = 0.929688\n",
      "[2018-07-16 12:26:05.770899] Iteration 20300, train loss = 0.251806, train accuracy = 0.929688\n",
      "[2018-07-16 12:26:09.412110] Iteration 20400, train loss = 0.258694, train accuracy = 0.937500\n",
      "[2018-07-16 12:26:13.041197] Iteration 20500, train loss = 0.317775, train accuracy = 0.921875\n",
      "[2018-07-16 12:26:16.697299] Iteration 20600, train loss = 0.300287, train accuracy = 0.921875\n",
      "[2018-07-16 12:26:20.348932] Iteration 20700, train loss = 0.328842, train accuracy = 0.914062\n",
      "[2018-07-16 12:26:23.994886] Iteration 20800, train loss = 0.203691, train accuracy = 0.945312\n",
      "[2018-07-16 12:26:27.627547] Iteration 20900, train loss = 0.222060, train accuracy = 0.953125\n",
      "[2018-07-16 12:26:31.279366] Iteration 21000, train loss = 0.352723, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.839100\n",
      "[2018-07-16 12:26:36.093820] Iteration 21100, train loss = 0.200715, train accuracy = 0.953125\n",
      "[2018-07-16 12:26:39.736849] Iteration 21200, train loss = 0.303918, train accuracy = 0.929688\n",
      "[2018-07-16 12:26:43.441400] Iteration 21300, train loss = 0.279992, train accuracy = 0.929688\n",
      "[2018-07-16 12:26:47.095906] Iteration 21400, train loss = 0.341276, train accuracy = 0.898438\n",
      "[2018-07-16 12:26:50.740935] Iteration 21500, train loss = 0.297886, train accuracy = 0.945312\n",
      "[2018-07-16 12:26:54.372726] Iteration 21600, train loss = 0.376400, train accuracy = 0.890625\n",
      "[2018-07-16 12:26:58.002053] Iteration 21700, train loss = 0.265777, train accuracy = 0.906250\n",
      "[2018-07-16 12:27:01.633576] Iteration 21800, train loss = 0.277587, train accuracy = 0.929688\n",
      "[2018-07-16 12:27:05.286623] Iteration 21900, train loss = 0.318035, train accuracy = 0.906250\n",
      "[2018-07-16 12:27:08.932173] Iteration 22000, train loss = 0.244333, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.861300\n",
      "[2018-07-16 12:27:13.729025] Iteration 22100, train loss = 0.263663, train accuracy = 0.914062\n",
      "[2018-07-16 12:27:17.377691] Iteration 22200, train loss = 0.288390, train accuracy = 0.929688\n",
      "[2018-07-16 12:27:21.020258] Iteration 22300, train loss = 0.258571, train accuracy = 0.945312\n",
      "[2018-07-16 12:27:24.662639] Iteration 22400, train loss = 0.233005, train accuracy = 0.960938\n",
      "[2018-07-16 12:27:28.410843] Iteration 22500, train loss = 0.231247, train accuracy = 0.937500\n",
      "[2018-07-16 12:27:32.069336] Iteration 22600, train loss = 0.356453, train accuracy = 0.945312\n",
      "[2018-07-16 12:27:35.722308] Iteration 22700, train loss = 0.273043, train accuracy = 0.929688\n",
      "[2018-07-16 12:27:39.362772] Iteration 22800, train loss = 0.318698, train accuracy = 0.890625\n",
      "[2018-07-16 12:27:43.003506] Iteration 22900, train loss = 0.278830, train accuracy = 0.921875\n",
      "[2018-07-16 12:27:46.632900] Iteration 23000, train loss = 0.265316, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.855500\n",
      "[2018-07-16 12:27:51.402094] Iteration 23100, train loss = 0.215674, train accuracy = 0.960938\n",
      "[2018-07-16 12:27:55.047199] Iteration 23200, train loss = 0.297497, train accuracy = 0.929688\n",
      "[2018-07-16 12:27:58.692862] Iteration 23300, train loss = 0.297914, train accuracy = 0.906250\n",
      "[2018-07-16 12:28:02.334093] Iteration 23400, train loss = 0.287745, train accuracy = 0.937500\n",
      "[2018-07-16 12:28:05.975538] Iteration 23500, train loss = 0.211926, train accuracy = 0.968750\n",
      "[2018-07-16 12:28:09.689125] Iteration 23600, train loss = 0.344018, train accuracy = 0.929688\n",
      "[2018-07-16 12:28:13.353246] Iteration 23700, train loss = 0.307086, train accuracy = 0.929688\n",
      "[2018-07-16 12:28:16.995311] Iteration 23800, train loss = 0.266690, train accuracy = 0.929688\n",
      "[2018-07-16 12:28:20.634059] Iteration 23900, train loss = 0.245679, train accuracy = 0.945312\n",
      "[2018-07-16 12:28:24.279718] Iteration 24000, train loss = 0.290495, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.849800\n",
      "[2018-07-16 12:28:29.019428] Iteration 24100, train loss = 0.227050, train accuracy = 0.960938\n",
      "[2018-07-16 12:28:32.666218] Iteration 24200, train loss = 0.285853, train accuracy = 0.914062\n",
      "[2018-07-16 12:28:36.302136] Iteration 24300, train loss = 0.229603, train accuracy = 0.929688\n",
      "[2018-07-16 12:28:39.955114] Iteration 24400, train loss = 0.237532, train accuracy = 0.945312\n",
      "[2018-07-16 12:28:43.625652] Iteration 24500, train loss = 0.300629, train accuracy = 0.914062\n",
      "[2018-07-16 12:28:47.265960] Iteration 24600, train loss = 0.254098, train accuracy = 0.953125\n",
      "[2018-07-16 12:28:50.977525] Iteration 24700, train loss = 0.200517, train accuracy = 0.953125\n",
      "[2018-07-16 12:28:54.621628] Iteration 24800, train loss = 0.306790, train accuracy = 0.898438\n",
      "[2018-07-16 12:28:58.284902] Iteration 24900, train loss = 0.258013, train accuracy = 0.921875\n",
      "[2018-07-16 12:29:01.974540] Iteration 25000, train loss = 0.278805, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.867800\n",
      "[2018-07-16 12:29:06.754071] Iteration 25100, train loss = 0.308744, train accuracy = 0.906250\n",
      "[2018-07-16 12:29:10.385621] Iteration 25200, train loss = 0.278426, train accuracy = 0.953125\n",
      "[2018-07-16 12:29:14.029705] Iteration 25300, train loss = 0.242767, train accuracy = 0.953125\n",
      "[2018-07-16 12:29:17.664282] Iteration 25400, train loss = 0.226443, train accuracy = 0.968750\n",
      "[2018-07-16 12:29:21.289735] Iteration 25500, train loss = 0.295812, train accuracy = 0.921875\n",
      "[2018-07-16 12:29:24.927983] Iteration 25600, train loss = 0.313304, train accuracy = 0.937500\n",
      "[2018-07-16 12:29:28.607549] Iteration 25700, train loss = 0.307956, train accuracy = 0.929688\n",
      "[2018-07-16 12:29:32.329698] Iteration 25800, train loss = 0.236823, train accuracy = 0.921875\n",
      "[2018-07-16 12:29:35.991934] Iteration 25900, train loss = 0.291677, train accuracy = 0.929688\n",
      "[2018-07-16 12:29:39.635719] Iteration 26000, train loss = 0.264475, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.823200\n",
      "[2018-07-16 12:29:44.449866] Iteration 26100, train loss = 0.259316, train accuracy = 0.914062\n",
      "[2018-07-16 12:29:48.083252] Iteration 26200, train loss = 0.257499, train accuracy = 0.945312\n",
      "[2018-07-16 12:29:51.728780] Iteration 26300, train loss = 0.244519, train accuracy = 0.945312\n",
      "[2018-07-16 12:29:55.366685] Iteration 26400, train loss = 0.216620, train accuracy = 0.945312\n",
      "[2018-07-16 12:29:59.006387] Iteration 26500, train loss = 0.275008, train accuracy = 0.929688\n",
      "[2018-07-16 12:30:02.672805] Iteration 26600, train loss = 0.255357, train accuracy = 0.960938\n",
      "[2018-07-16 12:30:06.328317] Iteration 26700, train loss = 0.200646, train accuracy = 0.960938\n",
      "[2018-07-16 12:30:09.990039] Iteration 26800, train loss = 0.279064, train accuracy = 0.921875\n",
      "[2018-07-16 12:30:13.681743] Iteration 26900, train loss = 0.314381, train accuracy = 0.937500\n",
      "[2018-07-16 12:30:17.364532] Iteration 27000, train loss = 0.221348, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.833500\n",
      "[2018-07-16 12:30:22.138734] Iteration 27100, train loss = 0.343023, train accuracy = 0.906250\n",
      "[2018-07-16 12:30:25.787604] Iteration 27200, train loss = 0.330867, train accuracy = 0.914062\n",
      "[2018-07-16 12:30:29.438563] Iteration 27300, train loss = 0.184519, train accuracy = 0.953125\n",
      "[2018-07-16 12:30:33.097296] Iteration 27400, train loss = 0.247371, train accuracy = 0.968750\n",
      "[2018-07-16 12:30:36.754020] Iteration 27500, train loss = 0.189431, train accuracy = 0.976562\n",
      "[2018-07-16 12:30:40.399515] Iteration 27600, train loss = 0.258216, train accuracy = 0.937500\n",
      "[2018-07-16 12:30:44.046018] Iteration 27700, train loss = 0.258603, train accuracy = 0.945312\n",
      "[2018-07-16 12:30:47.675148] Iteration 27800, train loss = 0.269535, train accuracy = 0.945312\n",
      "[2018-07-16 12:30:51.303232] Iteration 27900, train loss = 0.284805, train accuracy = 0.914062\n",
      "[2018-07-16 12:30:54.941223] Iteration 28000, train loss = 0.244988, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.871500\n",
      "[2018-07-16 12:30:59.868913] Iteration 28100, train loss = 0.237576, train accuracy = 0.953125\n",
      "[2018-07-16 12:31:03.507331] Iteration 28200, train loss = 0.208532, train accuracy = 0.960938\n",
      "[2018-07-16 12:31:07.170971] Iteration 28300, train loss = 0.321371, train accuracy = 0.914062\n",
      "[2018-07-16 12:31:10.835136] Iteration 28400, train loss = 0.307829, train accuracy = 0.914062\n",
      "[2018-07-16 12:31:14.482184] Iteration 28500, train loss = 0.199890, train accuracy = 0.968750\n",
      "[2018-07-16 12:31:18.140373] Iteration 28600, train loss = 0.225698, train accuracy = 0.953125\n",
      "[2018-07-16 12:31:21.789539] Iteration 28700, train loss = 0.326040, train accuracy = 0.906250\n",
      "[2018-07-16 12:31:25.438535] Iteration 28800, train loss = 0.263627, train accuracy = 0.960938\n",
      "[2018-07-16 12:31:29.099186] Iteration 28900, train loss = 0.250480, train accuracy = 0.929688\n",
      "[2018-07-16 12:31:32.738565] Iteration 29000, train loss = 0.173056, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.851200\n",
      "[2018-07-16 12:31:37.540934] Iteration 29100, train loss = 0.287461, train accuracy = 0.914062\n",
      "[2018-07-16 12:31:41.252758] Iteration 29200, train loss = 0.210250, train accuracy = 0.960938\n",
      "[2018-07-16 12:31:44.904477] Iteration 29300, train loss = 0.264952, train accuracy = 0.937500\n",
      "[2018-07-16 12:31:48.538037] Iteration 29400, train loss = 0.265269, train accuracy = 0.929688\n",
      "[2018-07-16 12:31:52.193537] Iteration 29500, train loss = 0.268553, train accuracy = 0.921875\n",
      "[2018-07-16 12:31:55.851208] Iteration 29600, train loss = 0.193356, train accuracy = 0.960938\n",
      "[2018-07-16 12:31:59.498417] Iteration 29700, train loss = 0.204650, train accuracy = 0.976562\n",
      "[2018-07-16 12:32:03.135480] Iteration 29800, train loss = 0.196608, train accuracy = 0.968750\n",
      "[2018-07-16 12:32:06.773524] Iteration 29900, train loss = 0.243385, train accuracy = 0.937500\n",
      "[2018-07-16 12:32:10.419490] Iteration 30000, train loss = 0.236615, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.860600\n",
      "[2018-07-16 12:32:15.193478] Iteration 30100, train loss = 0.223456, train accuracy = 0.960938\n",
      "[2018-07-16 12:32:18.825262] Iteration 30200, train loss = 0.266369, train accuracy = 0.937500\n",
      "[2018-07-16 12:32:22.535327] Iteration 30300, train loss = 0.291225, train accuracy = 0.914062\n",
      "[2018-07-16 12:32:26.170175] Iteration 30400, train loss = 0.195428, train accuracy = 0.960938\n",
      "[2018-07-16 12:32:29.807554] Iteration 30500, train loss = 0.182551, train accuracy = 0.960938\n",
      "[2018-07-16 12:32:33.439235] Iteration 30600, train loss = 0.312289, train accuracy = 0.914062\n",
      "[2018-07-16 12:32:37.075738] Iteration 30700, train loss = 0.248488, train accuracy = 0.929688\n",
      "[2018-07-16 12:32:40.722743] Iteration 30800, train loss = 0.278903, train accuracy = 0.937500\n",
      "[2018-07-16 12:32:44.365444] Iteration 30900, train loss = 0.264643, train accuracy = 0.914062\n",
      "[2018-07-16 12:32:48.022045] Iteration 31000, train loss = 0.239819, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.861100\n",
      "[2018-07-16 12:32:52.800548] Iteration 31100, train loss = 0.291220, train accuracy = 0.921875\n",
      "[2018-07-16 12:32:56.454722] Iteration 31200, train loss = 0.175820, train accuracy = 0.968750\n",
      "[2018-07-16 12:33:00.110338] Iteration 31300, train loss = 0.211919, train accuracy = 0.968750\n",
      "[2018-07-16 12:33:03.809581] Iteration 31400, train loss = 0.268751, train accuracy = 0.914062\n",
      "[2018-07-16 12:33:07.455157] Iteration 31500, train loss = 0.330565, train accuracy = 0.921875\n",
      "[2018-07-16 12:33:11.104241] Iteration 31600, train loss = 0.236992, train accuracy = 0.953125\n",
      "[2018-07-16 12:33:14.751236] Iteration 31700, train loss = 0.223403, train accuracy = 0.945312\n",
      "[2018-07-16 12:33:18.379982] Iteration 31800, train loss = 0.258045, train accuracy = 0.921875\n",
      "[2018-07-16 12:33:22.027440] Iteration 31900, train loss = 0.256080, train accuracy = 0.937500\n",
      "[2018-07-16 12:33:25.662316] Iteration 32000, train loss = 0.252447, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.861800\n",
      "[2018-07-16 12:33:30.421525] Iteration 32100, train loss = 0.156895, train accuracy = 0.976562\n",
      "[2018-07-16 12:33:34.054008] Iteration 32200, train loss = 0.194827, train accuracy = 0.968750\n",
      "[2018-07-16 12:33:37.705106] Iteration 32300, train loss = 0.351689, train accuracy = 0.867188\n",
      "[2018-07-16 12:33:41.351962] Iteration 32400, train loss = 0.294067, train accuracy = 0.929688\n",
      "[2018-07-16 12:33:45.099431] Iteration 32500, train loss = 0.179223, train accuracy = 0.976562\n",
      "[2018-07-16 12:33:48.748527] Iteration 32600, train loss = 0.210263, train accuracy = 0.953125\n",
      "[2018-07-16 12:33:52.426550] Iteration 32700, train loss = 0.308466, train accuracy = 0.914062\n",
      "[2018-07-16 12:33:56.067297] Iteration 32800, train loss = 0.209533, train accuracy = 0.937500\n",
      "[2018-07-16 12:33:59.707858] Iteration 32900, train loss = 0.208160, train accuracy = 0.953125\n",
      "[2018-07-16 12:34:03.345125] Iteration 33000, train loss = 0.258864, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.875500\n",
      "[2018-07-16 12:34:08.108854] Iteration 33100, train loss = 0.248566, train accuracy = 0.937500\n",
      "[2018-07-16 12:34:11.742875] Iteration 33200, train loss = 0.219265, train accuracy = 0.945312\n",
      "[2018-07-16 12:34:15.418092] Iteration 33300, train loss = 0.213468, train accuracy = 0.968750\n",
      "[2018-07-16 12:34:19.065885] Iteration 33400, train loss = 0.226936, train accuracy = 0.945312\n",
      "[2018-07-16 12:34:22.726715] Iteration 33500, train loss = 0.223387, train accuracy = 0.953125\n",
      "[2018-07-16 12:34:26.425388] Iteration 33600, train loss = 0.295275, train accuracy = 0.906250\n",
      "[2018-07-16 12:34:30.117120] Iteration 33700, train loss = 0.257763, train accuracy = 0.953125\n",
      "[2018-07-16 12:34:33.772659] Iteration 33800, train loss = 0.226203, train accuracy = 0.953125\n",
      "[2018-07-16 12:34:37.429354] Iteration 33900, train loss = 0.230409, train accuracy = 0.929688\n",
      "[2018-07-16 12:34:41.072848] Iteration 34000, train loss = 0.218878, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.866900\n",
      "[2018-07-16 12:34:45.824711] Iteration 34100, train loss = 0.214326, train accuracy = 0.937500\n",
      "[2018-07-16 12:34:49.456140] Iteration 34200, train loss = 0.273557, train accuracy = 0.937500\n",
      "[2018-07-16 12:34:53.092239] Iteration 34300, train loss = 0.283841, train accuracy = 0.921875\n",
      "[2018-07-16 12:34:56.710309] Iteration 34400, train loss = 0.221774, train accuracy = 0.945312\n",
      "[2018-07-16 12:35:00.362579] Iteration 34500, train loss = 0.139652, train accuracy = 0.984375\n",
      "[2018-07-16 12:35:04.015422] Iteration 34600, train loss = 0.204506, train accuracy = 0.960938\n",
      "[2018-07-16 12:35:07.685321] Iteration 34700, train loss = 0.256783, train accuracy = 0.945312\n",
      "[2018-07-16 12:35:11.439622] Iteration 34800, train loss = 0.230169, train accuracy = 0.929688\n",
      "[2018-07-16 12:35:15.096914] Iteration 34900, train loss = 0.263834, train accuracy = 0.921875\n",
      "[2018-07-16 12:35:18.744484] Iteration 35000, train loss = 0.227533, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.876900\n",
      "[2018-07-16 12:35:23.524015] Iteration 35100, train loss = 0.215054, train accuracy = 0.953125\n",
      "[2018-07-16 12:35:27.176499] Iteration 35200, train loss = 0.175665, train accuracy = 0.960938\n",
      "[2018-07-16 12:35:30.821928] Iteration 35300, train loss = 0.196734, train accuracy = 0.953125\n",
      "[2018-07-16 12:35:34.453231] Iteration 35400, train loss = 0.245918, train accuracy = 0.953125\n",
      "[2018-07-16 12:35:38.080889] Iteration 35500, train loss = 0.155942, train accuracy = 0.976562\n",
      "[2018-07-16 12:35:41.712499] Iteration 35600, train loss = 0.219461, train accuracy = 0.953125\n",
      "[2018-07-16 12:35:45.350521] Iteration 35700, train loss = 0.228936, train accuracy = 0.945312\n",
      "[2018-07-16 12:35:48.989609] Iteration 35800, train loss = 0.199809, train accuracy = 0.953125\n",
      "[2018-07-16 12:35:52.732615] Iteration 35900, train loss = 0.217609, train accuracy = 0.960938\n",
      "[2018-07-16 12:35:56.373579] Iteration 36000, train loss = 0.190748, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.855200\n",
      "[2018-07-16 12:36:01.144121] Iteration 36100, train loss = 0.171518, train accuracy = 0.968750\n",
      "[2018-07-16 12:36:04.783151] Iteration 36200, train loss = 0.226378, train accuracy = 0.960938\n",
      "[2018-07-16 12:36:08.429352] Iteration 36300, train loss = 0.193569, train accuracy = 0.968750\n",
      "[2018-07-16 12:36:12.093968] Iteration 36400, train loss = 0.226342, train accuracy = 0.953125\n",
      "[2018-07-16 12:36:15.755743] Iteration 36500, train loss = 0.206188, train accuracy = 0.960938\n",
      "[2018-07-16 12:36:19.408119] Iteration 36600, train loss = 0.281223, train accuracy = 0.937500\n",
      "[2018-07-16 12:36:23.049332] Iteration 36700, train loss = 0.165443, train accuracy = 0.976562\n",
      "[2018-07-16 12:36:26.682930] Iteration 36800, train loss = 0.200223, train accuracy = 0.953125\n",
      "[2018-07-16 12:36:30.313760] Iteration 36900, train loss = 0.190790, train accuracy = 0.968750\n",
      "[2018-07-16 12:36:34.001310] Iteration 37000, train loss = 0.365823, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.865900\n",
      "[2018-07-16 12:36:38.805872] Iteration 37100, train loss = 0.241269, train accuracy = 0.960938\n",
      "[2018-07-16 12:36:42.458598] Iteration 37200, train loss = 0.304126, train accuracy = 0.906250\n",
      "[2018-07-16 12:36:46.106134] Iteration 37300, train loss = 0.227460, train accuracy = 0.945312\n",
      "[2018-07-16 12:36:49.763656] Iteration 37400, train loss = 0.207384, train accuracy = 0.953125\n",
      "[2018-07-16 12:36:53.407239] Iteration 37500, train loss = 0.210217, train accuracy = 0.953125\n",
      "[2018-07-16 12:36:57.058601] Iteration 37600, train loss = 0.185380, train accuracy = 0.968750\n",
      "[2018-07-16 12:37:00.714651] Iteration 37700, train loss = 0.290406, train accuracy = 0.921875\n",
      "[2018-07-16 12:37:04.362804] Iteration 37800, train loss = 0.236290, train accuracy = 0.960938\n",
      "[2018-07-16 12:37:08.018107] Iteration 37900, train loss = 0.192551, train accuracy = 0.968750\n",
      "[2018-07-16 12:37:11.647823] Iteration 38000, train loss = 0.220499, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.827500\n",
      "[2018-07-16 12:37:16.477277] Iteration 38100, train loss = 0.248921, train accuracy = 0.937500\n",
      "[2018-07-16 12:37:20.110402] Iteration 38200, train loss = 0.246251, train accuracy = 0.921875\n",
      "[2018-07-16 12:37:23.759951] Iteration 38300, train loss = 0.207743, train accuracy = 0.953125\n",
      "[2018-07-16 12:37:27.398274] Iteration 38400, train loss = 0.223649, train accuracy = 0.960938\n",
      "[2018-07-16 12:37:31.049872] Iteration 38500, train loss = 0.182813, train accuracy = 0.968750\n",
      "[2018-07-16 12:37:34.713529] Iteration 38600, train loss = 0.187807, train accuracy = 0.960938\n",
      "[2018-07-16 12:37:38.370029] Iteration 38700, train loss = 0.255751, train accuracy = 0.937500\n",
      "[2018-07-16 12:37:42.018071] Iteration 38800, train loss = 0.209500, train accuracy = 0.960938\n",
      "[2018-07-16 12:37:45.668212] Iteration 38900, train loss = 0.246218, train accuracy = 0.960938\n",
      "[2018-07-16 12:37:49.306019] Iteration 39000, train loss = 0.213721, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.859400\n",
      "[2018-07-16 12:37:54.112948] Iteration 39100, train loss = 0.215921, train accuracy = 0.953125\n",
      "[2018-07-16 12:37:57.831928] Iteration 39200, train loss = 0.169039, train accuracy = 0.968750\n",
      "[2018-07-16 12:38:01.461734] Iteration 39300, train loss = 0.235149, train accuracy = 0.953125\n",
      "[2018-07-16 12:38:05.091510] Iteration 39400, train loss = 0.174415, train accuracy = 0.960938\n",
      "[2018-07-16 12:38:08.719074] Iteration 39500, train loss = 0.178008, train accuracy = 0.960938\n",
      "[2018-07-16 12:38:12.358833] Iteration 39600, train loss = 0.229211, train accuracy = 0.937500\n",
      "[2018-07-16 12:38:15.993533] Iteration 39700, train loss = 0.231018, train accuracy = 0.960938\n",
      "[2018-07-16 12:38:19.645334] Iteration 39800, train loss = 0.255802, train accuracy = 0.945312\n",
      "[2018-07-16 12:38:23.297712] Iteration 39900, train loss = 0.249181, train accuracy = 0.929688\n"
     ]
    }
   ],
   "source": [
    "curr_lr = 0\n",
    "for step in range(40000):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二轮 量化\n",
    "prune_rate = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.864800\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.0481069   0.125      -0.05013176 -0.05461586\n",
      "  0.10150167  0.          0.02769161 -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.75)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-16 12:38:54.178120] Iteration 100, train loss = 1.588848, train accuracy = 0.429688\n",
      "[2018-07-16 12:38:57.798802] Iteration 200, train loss = 1.389496, train accuracy = 0.570312\n",
      "[2018-07-16 12:39:01.445302] Iteration 300, train loss = 1.200422, train accuracy = 0.562500\n",
      "[2018-07-16 12:39:05.092193] Iteration 400, train loss = 1.136769, train accuracy = 0.632812\n",
      "[2018-07-16 12:39:08.725678] Iteration 500, train loss = 1.040576, train accuracy = 0.648438\n",
      "[2018-07-16 12:39:12.381444] Iteration 600, train loss = 1.013668, train accuracy = 0.679688\n",
      "[2018-07-16 12:39:16.038974] Iteration 700, train loss = 0.920621, train accuracy = 0.757812\n",
      "[2018-07-16 12:39:19.683931] Iteration 800, train loss = 0.883226, train accuracy = 0.703125\n",
      "[2018-07-16 12:39:23.378991] Iteration 900, train loss = 0.989371, train accuracy = 0.710938\n",
      "[2018-07-16 12:39:26.988513] Iteration 1000, train loss = 0.881680, train accuracy = 0.710938\n",
      "Evaluating...\n",
      "Test accuracy = 0.639300\n",
      "[2018-07-16 12:39:31.636105] Iteration 1100, train loss = 0.856332, train accuracy = 0.718750\n",
      "[2018-07-16 12:39:35.239246] Iteration 1200, train loss = 0.842979, train accuracy = 0.710938\n",
      "[2018-07-16 12:39:38.836402] Iteration 1300, train loss = 0.772436, train accuracy = 0.742188\n",
      "[2018-07-16 12:39:42.445707] Iteration 1400, train loss = 0.924176, train accuracy = 0.695312\n",
      "[2018-07-16 12:39:46.077641] Iteration 1500, train loss = 0.700377, train accuracy = 0.765625\n",
      "[2018-07-16 12:39:49.674680] Iteration 1600, train loss = 0.705092, train accuracy = 0.796875\n",
      "[2018-07-16 12:39:53.323754] Iteration 1700, train loss = 0.752038, train accuracy = 0.773438\n",
      "[2018-07-16 12:39:57.015715] Iteration 1800, train loss = 0.592732, train accuracy = 0.820312\n",
      "[2018-07-16 12:40:00.690204] Iteration 1900, train loss = 0.905369, train accuracy = 0.656250\n",
      "[2018-07-16 12:40:04.372510] Iteration 2000, train loss = 0.708644, train accuracy = 0.773438\n",
      "Evaluating...\n",
      "Test accuracy = 0.741400\n",
      "[2018-07-16 12:40:09.162788] Iteration 2100, train loss = 0.808148, train accuracy = 0.757812\n",
      "[2018-07-16 12:40:12.832734] Iteration 2200, train loss = 0.596590, train accuracy = 0.789062\n",
      "[2018-07-16 12:40:16.479778] Iteration 2300, train loss = 0.703989, train accuracy = 0.773438\n",
      "[2018-07-16 12:40:20.116923] Iteration 2400, train loss = 0.651228, train accuracy = 0.796875\n",
      "[2018-07-16 12:40:23.836534] Iteration 2500, train loss = 0.555993, train accuracy = 0.843750\n",
      "[2018-07-16 12:40:27.460498] Iteration 2600, train loss = 0.633671, train accuracy = 0.820312\n",
      "[2018-07-16 12:40:31.102029] Iteration 2700, train loss = 0.610107, train accuracy = 0.804688\n",
      "[2018-07-16 12:40:34.718411] Iteration 2800, train loss = 0.555246, train accuracy = 0.820312\n",
      "[2018-07-16 12:40:38.348117] Iteration 2900, train loss = 0.587126, train accuracy = 0.804688\n",
      "[2018-07-16 12:40:41.989327] Iteration 3000, train loss = 0.719542, train accuracy = 0.757812\n",
      "Evaluating...\n",
      "Test accuracy = 0.670400\n",
      "[2018-07-16 12:40:46.762953] Iteration 3100, train loss = 0.606018, train accuracy = 0.835938\n",
      "[2018-07-16 12:40:50.409075] Iteration 3200, train loss = 0.580313, train accuracy = 0.820312\n",
      "[2018-07-16 12:40:54.054599] Iteration 3300, train loss = 0.502004, train accuracy = 0.812500\n",
      "[2018-07-16 12:40:57.686533] Iteration 3400, train loss = 0.581395, train accuracy = 0.828125\n",
      "[2018-07-16 12:41:01.326907] Iteration 3500, train loss = 0.736892, train accuracy = 0.734375\n",
      "[2018-07-16 12:41:05.019177] Iteration 3600, train loss = 0.616135, train accuracy = 0.796875\n",
      "[2018-07-16 12:41:08.641195] Iteration 3700, train loss = 0.503829, train accuracy = 0.882812\n",
      "[2018-07-16 12:41:12.283206] Iteration 3800, train loss = 0.525770, train accuracy = 0.812500\n",
      "[2018-07-16 12:41:15.900995] Iteration 3900, train loss = 0.572399, train accuracy = 0.820312\n",
      "[2018-07-16 12:41:19.537756] Iteration 4000, train loss = 0.435787, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.793000\n",
      "[2018-07-16 12:41:24.303621] Iteration 4100, train loss = 0.525964, train accuracy = 0.851562\n",
      "[2018-07-16 12:41:27.938674] Iteration 4200, train loss = 0.576407, train accuracy = 0.820312\n",
      "[2018-07-16 12:41:31.581860] Iteration 4300, train loss = 0.585401, train accuracy = 0.781250\n",
      "[2018-07-16 12:41:35.205478] Iteration 4400, train loss = 0.652856, train accuracy = 0.812500\n",
      "[2018-07-16 12:41:38.852147] Iteration 4500, train loss = 0.613319, train accuracy = 0.835938\n",
      "[2018-07-16 12:41:42.480516] Iteration 4600, train loss = 0.620921, train accuracy = 0.812500\n",
      "[2018-07-16 12:41:46.136440] Iteration 4700, train loss = 0.569510, train accuracy = 0.820312\n",
      "[2018-07-16 12:41:49.831875] Iteration 4800, train loss = 0.511094, train accuracy = 0.851562\n",
      "[2018-07-16 12:41:53.456776] Iteration 4900, train loss = 0.598531, train accuracy = 0.835938\n",
      "[2018-07-16 12:41:57.076567] Iteration 5000, train loss = 0.521347, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.784200\n",
      "[2018-07-16 12:42:01.818021] Iteration 5100, train loss = 0.448569, train accuracy = 0.820312\n",
      "[2018-07-16 12:42:05.439131] Iteration 5200, train loss = 0.528346, train accuracy = 0.843750\n",
      "[2018-07-16 12:42:09.073737] Iteration 5300, train loss = 0.647455, train accuracy = 0.789062\n",
      "[2018-07-16 12:42:12.721973] Iteration 5400, train loss = 0.608986, train accuracy = 0.820312\n",
      "[2018-07-16 12:42:16.365716] Iteration 5500, train loss = 0.554812, train accuracy = 0.867188\n",
      "[2018-07-16 12:42:20.030340] Iteration 5600, train loss = 0.399521, train accuracy = 0.914062\n",
      "[2018-07-16 12:42:23.666218] Iteration 5700, train loss = 0.423862, train accuracy = 0.875000\n",
      "[2018-07-16 12:42:27.304494] Iteration 5800, train loss = 0.580638, train accuracy = 0.804688\n",
      "[2018-07-16 12:42:31.044878] Iteration 5900, train loss = 0.721561, train accuracy = 0.812500\n",
      "[2018-07-16 12:42:34.696526] Iteration 6000, train loss = 0.508239, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.805300\n",
      "[2018-07-16 12:42:39.472342] Iteration 6100, train loss = 0.501871, train accuracy = 0.882812\n",
      "[2018-07-16 12:42:43.115927] Iteration 6200, train loss = 0.515508, train accuracy = 0.835938\n",
      "[2018-07-16 12:42:46.743321] Iteration 6300, train loss = 0.466628, train accuracy = 0.843750\n",
      "[2018-07-16 12:42:50.370531] Iteration 6400, train loss = 0.503088, train accuracy = 0.843750\n",
      "[2018-07-16 12:42:53.994105] Iteration 6500, train loss = 0.603411, train accuracy = 0.781250\n",
      "[2018-07-16 12:42:57.616493] Iteration 6600, train loss = 0.402491, train accuracy = 0.867188\n",
      "[2018-07-16 12:43:01.239456] Iteration 6700, train loss = 0.400670, train accuracy = 0.890625\n",
      "[2018-07-16 12:43:04.880808] Iteration 6800, train loss = 0.458300, train accuracy = 0.843750\n",
      "[2018-07-16 12:43:08.526997] Iteration 6900, train loss = 0.514568, train accuracy = 0.851562\n",
      "[2018-07-16 12:43:12.233901] Iteration 7000, train loss = 0.498734, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.792200\n",
      "[2018-07-16 12:43:17.008770] Iteration 7100, train loss = 0.477402, train accuracy = 0.875000\n",
      "[2018-07-16 12:43:20.656062] Iteration 7200, train loss = 0.400075, train accuracy = 0.867188\n",
      "[2018-07-16 12:43:24.305456] Iteration 7300, train loss = 0.456173, train accuracy = 0.882812\n",
      "[2018-07-16 12:43:27.931177] Iteration 7400, train loss = 0.485695, train accuracy = 0.875000\n",
      "[2018-07-16 12:43:31.568537] Iteration 7500, train loss = 0.616897, train accuracy = 0.820312\n",
      "[2018-07-16 12:43:35.206896] Iteration 7600, train loss = 0.484859, train accuracy = 0.859375\n",
      "[2018-07-16 12:43:38.830653] Iteration 7700, train loss = 0.428579, train accuracy = 0.867188\n",
      "[2018-07-16 12:43:42.446117] Iteration 7800, train loss = 0.455270, train accuracy = 0.875000\n",
      "[2018-07-16 12:43:46.082697] Iteration 7900, train loss = 0.367229, train accuracy = 0.867188\n",
      "[2018-07-16 12:43:49.733816] Iteration 8000, train loss = 0.318203, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.805000\n",
      "[2018-07-16 12:43:54.566953] Iteration 8100, train loss = 0.533865, train accuracy = 0.851562\n",
      "[2018-07-16 12:43:58.204536] Iteration 8200, train loss = 0.442052, train accuracy = 0.851562\n",
      "[2018-07-16 12:44:01.851404] Iteration 8300, train loss = 0.471372, train accuracy = 0.843750\n",
      "[2018-07-16 12:44:05.497846] Iteration 8400, train loss = 0.524786, train accuracy = 0.843750\n",
      "[2018-07-16 12:44:09.133627] Iteration 8500, train loss = 0.608181, train accuracy = 0.828125\n",
      "[2018-07-16 12:44:12.767329] Iteration 8600, train loss = 0.454119, train accuracy = 0.890625\n",
      "[2018-07-16 12:44:16.407584] Iteration 8700, train loss = 0.440563, train accuracy = 0.867188\n",
      "[2018-07-16 12:44:20.034721] Iteration 8800, train loss = 0.540924, train accuracy = 0.828125\n",
      "[2018-07-16 12:44:23.676181] Iteration 8900, train loss = 0.498252, train accuracy = 0.828125\n",
      "[2018-07-16 12:44:27.301639] Iteration 9000, train loss = 0.487750, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.834300\n",
      "[2018-07-16 12:44:32.052813] Iteration 9100, train loss = 0.548565, train accuracy = 0.812500\n",
      "[2018-07-16 12:44:35.747139] Iteration 9200, train loss = 0.332144, train accuracy = 0.898438\n",
      "[2018-07-16 12:44:39.407996] Iteration 9300, train loss = 0.536117, train accuracy = 0.843750\n",
      "[2018-07-16 12:44:43.059419] Iteration 9400, train loss = 0.450442, train accuracy = 0.875000\n",
      "[2018-07-16 12:44:46.697403] Iteration 9500, train loss = 0.443433, train accuracy = 0.890625\n",
      "[2018-07-16 12:44:50.342321] Iteration 9600, train loss = 0.464690, train accuracy = 0.890625\n",
      "[2018-07-16 12:44:53.968902] Iteration 9700, train loss = 0.356243, train accuracy = 0.906250\n",
      "[2018-07-16 12:44:57.608241] Iteration 9800, train loss = 0.428853, train accuracy = 0.875000\n",
      "[2018-07-16 12:45:01.243810] Iteration 9900, train loss = 0.417936, train accuracy = 0.882812\n",
      "[2018-07-16 12:45:04.893173] Iteration 10000, train loss = 0.447772, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.824000\n",
      "[2018-07-16 12:45:09.645838] Iteration 10100, train loss = 0.301326, train accuracy = 0.914062\n",
      "[2018-07-16 12:45:13.265547] Iteration 10200, train loss = 0.417596, train accuracy = 0.882812\n",
      "[2018-07-16 12:45:16.991690] Iteration 10300, train loss = 0.448494, train accuracy = 0.882812\n",
      "[2018-07-16 12:45:20.613634] Iteration 10400, train loss = 0.330174, train accuracy = 0.921875\n",
      "[2018-07-16 12:45:24.251969] Iteration 10500, train loss = 0.377735, train accuracy = 0.882812\n",
      "[2018-07-16 12:45:27.880793] Iteration 10600, train loss = 0.559167, train accuracy = 0.820312\n",
      "[2018-07-16 12:45:31.537003] Iteration 10700, train loss = 0.452093, train accuracy = 0.875000\n",
      "[2018-07-16 12:45:35.191988] Iteration 10800, train loss = 0.426052, train accuracy = 0.867188\n",
      "[2018-07-16 12:45:38.827541] Iteration 10900, train loss = 0.318160, train accuracy = 0.898438\n",
      "[2018-07-16 12:45:42.458896] Iteration 11000, train loss = 0.359955, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.823500\n",
      "[2018-07-16 12:45:47.251982] Iteration 11100, train loss = 0.432348, train accuracy = 0.898438\n",
      "[2018-07-16 12:45:50.886635] Iteration 11200, train loss = 0.505516, train accuracy = 0.867188\n",
      "[2018-07-16 12:45:54.530029] Iteration 11300, train loss = 0.338377, train accuracy = 0.882812\n",
      "[2018-07-16 12:45:58.195957] Iteration 11400, train loss = 0.479552, train accuracy = 0.843750\n",
      "[2018-07-16 12:46:01.874104] Iteration 11500, train loss = 0.410416, train accuracy = 0.851562\n",
      "[2018-07-16 12:46:05.513215] Iteration 11600, train loss = 0.430049, train accuracy = 0.898438\n",
      "[2018-07-16 12:46:09.135751] Iteration 11700, train loss = 0.469592, train accuracy = 0.835938\n",
      "[2018-07-16 12:46:12.776811] Iteration 11800, train loss = 0.414450, train accuracy = 0.859375\n",
      "[2018-07-16 12:46:16.418809] Iteration 11900, train loss = 0.434057, train accuracy = 0.875000\n",
      "[2018-07-16 12:46:20.071314] Iteration 12000, train loss = 0.483331, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.838100\n",
      "[2018-07-16 12:46:24.823797] Iteration 12100, train loss = 0.438097, train accuracy = 0.898438\n",
      "[2018-07-16 12:46:28.455643] Iteration 12200, train loss = 0.467166, train accuracy = 0.875000\n",
      "[2018-07-16 12:46:32.087245] Iteration 12300, train loss = 0.443420, train accuracy = 0.859375\n",
      "[2018-07-16 12:46:35.739873] Iteration 12400, train loss = 0.463553, train accuracy = 0.859375\n",
      "[2018-07-16 12:46:39.379539] Iteration 12500, train loss = 0.418285, train accuracy = 0.882812\n",
      "[2018-07-16 12:46:43.103863] Iteration 12600, train loss = 0.343551, train accuracy = 0.890625\n",
      "[2018-07-16 12:46:46.735541] Iteration 12700, train loss = 0.474488, train accuracy = 0.882812\n",
      "[2018-07-16 12:46:50.358174] Iteration 12800, train loss = 0.482409, train accuracy = 0.859375\n",
      "[2018-07-16 12:46:53.982533] Iteration 12900, train loss = 0.614124, train accuracy = 0.812500\n",
      "[2018-07-16 12:46:57.615725] Iteration 13000, train loss = 0.381833, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.831400\n",
      "[2018-07-16 12:47:02.368992] Iteration 13100, train loss = 0.487342, train accuracy = 0.835938\n",
      "[2018-07-16 12:47:05.998540] Iteration 13200, train loss = 0.401980, train accuracy = 0.898438\n",
      "[2018-07-16 12:47:09.642715] Iteration 13300, train loss = 0.348166, train accuracy = 0.898438\n",
      "[2018-07-16 12:47:13.282370] Iteration 13400, train loss = 0.436301, train accuracy = 0.875000\n",
      "[2018-07-16 12:47:16.939837] Iteration 13500, train loss = 0.526002, train accuracy = 0.859375\n",
      "[2018-07-16 12:47:20.578060] Iteration 13600, train loss = 0.333762, train accuracy = 0.906250\n",
      "[2018-07-16 12:47:24.295887] Iteration 13700, train loss = 0.375482, train accuracy = 0.890625\n",
      "[2018-07-16 12:47:27.940352] Iteration 13800, train loss = 0.342676, train accuracy = 0.898438\n",
      "[2018-07-16 12:47:31.575059] Iteration 13900, train loss = 0.391127, train accuracy = 0.890625\n",
      "[2018-07-16 12:47:35.193172] Iteration 14000, train loss = 0.443032, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.851400\n",
      "[2018-07-16 12:47:39.928377] Iteration 14100, train loss = 0.326370, train accuracy = 0.921875\n",
      "[2018-07-16 12:47:43.578245] Iteration 14200, train loss = 0.456297, train accuracy = 0.859375\n",
      "[2018-07-16 12:47:47.214061] Iteration 14300, train loss = 0.393466, train accuracy = 0.875000\n",
      "[2018-07-16 12:47:50.859670] Iteration 14400, train loss = 0.332710, train accuracy = 0.914062\n",
      "[2018-07-16 12:47:54.501160] Iteration 14500, train loss = 0.370067, train accuracy = 0.914062\n",
      "[2018-07-16 12:47:58.133905] Iteration 14600, train loss = 0.465874, train accuracy = 0.890625\n",
      "[2018-07-16 12:48:01.781088] Iteration 14700, train loss = 0.359739, train accuracy = 0.906250\n",
      "[2018-07-16 12:48:05.494923] Iteration 14800, train loss = 0.364627, train accuracy = 0.906250\n",
      "[2018-07-16 12:48:09.122091] Iteration 14900, train loss = 0.313305, train accuracy = 0.937500\n",
      "[2018-07-16 12:48:12.749547] Iteration 15000, train loss = 0.429130, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.827300\n",
      "[2018-07-16 12:48:17.531431] Iteration 15100, train loss = 0.492411, train accuracy = 0.859375\n",
      "[2018-07-16 12:48:21.152225] Iteration 15200, train loss = 0.335067, train accuracy = 0.898438\n",
      "[2018-07-16 12:48:24.783204] Iteration 15300, train loss = 0.401815, train accuracy = 0.882812\n",
      "[2018-07-16 12:48:28.402320] Iteration 15400, train loss = 0.304248, train accuracy = 0.953125\n",
      "[2018-07-16 12:48:32.042741] Iteration 15500, train loss = 0.443079, train accuracy = 0.851562\n",
      "[2018-07-16 12:48:35.672448] Iteration 15600, train loss = 0.409813, train accuracy = 0.882812\n",
      "[2018-07-16 12:48:39.316031] Iteration 15700, train loss = 0.271624, train accuracy = 0.945312\n",
      "[2018-07-16 12:48:42.940987] Iteration 15800, train loss = 0.334125, train accuracy = 0.898438\n",
      "[2018-07-16 12:48:46.649239] Iteration 15900, train loss = 0.479514, train accuracy = 0.835938\n",
      "[2018-07-16 12:48:50.312204] Iteration 16000, train loss = 0.374152, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.832100\n",
      "[2018-07-16 12:48:55.063796] Iteration 16100, train loss = 0.431500, train accuracy = 0.875000\n",
      "[2018-07-16 12:48:58.714846] Iteration 16200, train loss = 0.371728, train accuracy = 0.890625\n",
      "[2018-07-16 12:49:02.350765] Iteration 16300, train loss = 0.419332, train accuracy = 0.875000\n",
      "[2018-07-16 12:49:05.993784] Iteration 16400, train loss = 0.333693, train accuracy = 0.882812\n",
      "[2018-07-16 12:49:09.616252] Iteration 16500, train loss = 0.280869, train accuracy = 0.953125\n",
      "[2018-07-16 12:49:13.236025] Iteration 16600, train loss = 0.438503, train accuracy = 0.882812\n",
      "[2018-07-16 12:49:16.867509] Iteration 16700, train loss = 0.371749, train accuracy = 0.914062\n",
      "[2018-07-16 12:49:20.489913] Iteration 16800, train loss = 0.374381, train accuracy = 0.914062\n",
      "[2018-07-16 12:49:24.144298] Iteration 16900, train loss = 0.414772, train accuracy = 0.867188\n",
      "[2018-07-16 12:49:27.782464] Iteration 17000, train loss = 0.273274, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.849100\n",
      "[2018-07-16 12:49:32.711386] Iteration 17100, train loss = 0.368428, train accuracy = 0.890625\n",
      "[2018-07-16 12:49:36.359396] Iteration 17200, train loss = 0.358570, train accuracy = 0.890625\n",
      "[2018-07-16 12:49:39.996267] Iteration 17300, train loss = 0.344513, train accuracy = 0.929688\n",
      "[2018-07-16 12:49:43.656261] Iteration 17400, train loss = 0.491973, train accuracy = 0.835938\n",
      "[2018-07-16 12:49:47.298804] Iteration 17500, train loss = 0.355613, train accuracy = 0.898438\n",
      "[2018-07-16 12:49:50.933895] Iteration 17600, train loss = 0.403452, train accuracy = 0.875000\n",
      "[2018-07-16 12:49:54.565114] Iteration 17700, train loss = 0.443842, train accuracy = 0.851562\n",
      "[2018-07-16 12:49:58.200351] Iteration 17800, train loss = 0.328301, train accuracy = 0.914062\n",
      "[2018-07-16 12:50:01.843236] Iteration 17900, train loss = 0.501182, train accuracy = 0.859375\n",
      "[2018-07-16 12:50:05.480411] Iteration 18000, train loss = 0.415340, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.825700\n",
      "[2018-07-16 12:50:10.270633] Iteration 18100, train loss = 0.492410, train accuracy = 0.851562\n",
      "[2018-07-16 12:50:13.959033] Iteration 18200, train loss = 0.361983, train accuracy = 0.898438\n",
      "[2018-07-16 12:50:17.589132] Iteration 18300, train loss = 0.383473, train accuracy = 0.906250\n",
      "[2018-07-16 12:50:21.238420] Iteration 18400, train loss = 0.338164, train accuracy = 0.898438\n",
      "[2018-07-16 12:50:24.888550] Iteration 18500, train loss = 0.416832, train accuracy = 0.882812\n",
      "[2018-07-16 12:50:28.549831] Iteration 18600, train loss = 0.464821, train accuracy = 0.867188\n",
      "[2018-07-16 12:50:32.197684] Iteration 18700, train loss = 0.389767, train accuracy = 0.882812\n",
      "[2018-07-16 12:50:35.827467] Iteration 18800, train loss = 0.419556, train accuracy = 0.890625\n",
      "[2018-07-16 12:50:39.452227] Iteration 18900, train loss = 0.310818, train accuracy = 0.921875\n",
      "[2018-07-16 12:50:43.076169] Iteration 19000, train loss = 0.251227, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.833200\n",
      "[2018-07-16 12:50:47.834189] Iteration 19100, train loss = 0.484850, train accuracy = 0.867188\n",
      "[2018-07-16 12:50:51.463002] Iteration 19200, train loss = 0.364120, train accuracy = 0.890625\n",
      "[2018-07-16 12:50:55.149654] Iteration 19300, train loss = 0.367251, train accuracy = 0.898438\n",
      "[2018-07-16 12:50:58.776073] Iteration 19400, train loss = 0.473639, train accuracy = 0.859375\n",
      "[2018-07-16 12:51:02.421105] Iteration 19500, train loss = 0.378026, train accuracy = 0.867188\n",
      "[2018-07-16 12:51:06.074280] Iteration 19600, train loss = 0.386990, train accuracy = 0.875000\n",
      "[2018-07-16 12:51:09.714229] Iteration 19700, train loss = 0.427499, train accuracy = 0.906250\n",
      "[2018-07-16 12:51:13.351337] Iteration 19800, train loss = 0.281654, train accuracy = 0.914062\n",
      "[2018-07-16 12:51:16.998888] Iteration 19900, train loss = 0.312931, train accuracy = 0.914062\n",
      "[2018-07-16 12:51:20.630343] Iteration 20000, train loss = 0.236797, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.847600\n",
      "[2018-07-16 12:51:25.403939] Iteration 20100, train loss = 0.383892, train accuracy = 0.851562\n",
      "[2018-07-16 12:51:29.054439] Iteration 20200, train loss = 0.389441, train accuracy = 0.867188\n",
      "[2018-07-16 12:51:32.673189] Iteration 20300, train loss = 0.376818, train accuracy = 0.890625\n",
      "[2018-07-16 12:51:36.384889] Iteration 20400, train loss = 0.420190, train accuracy = 0.898438\n",
      "[2018-07-16 12:51:40.015354] Iteration 20500, train loss = 0.455576, train accuracy = 0.867188\n",
      "[2018-07-16 12:51:43.647200] Iteration 20600, train loss = 0.349372, train accuracy = 0.906250\n",
      "[2018-07-16 12:51:47.284240] Iteration 20700, train loss = 0.345158, train accuracy = 0.906250\n",
      "[2018-07-16 12:51:50.916744] Iteration 20800, train loss = 0.351000, train accuracy = 0.898438\n",
      "[2018-07-16 12:51:54.547684] Iteration 20900, train loss = 0.347632, train accuracy = 0.914062\n",
      "[2018-07-16 12:51:58.194978] Iteration 21000, train loss = 0.352421, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.840300\n",
      "[2018-07-16 12:52:02.951836] Iteration 21100, train loss = 0.375871, train accuracy = 0.882812\n",
      "[2018-07-16 12:52:06.593358] Iteration 21200, train loss = 0.445446, train accuracy = 0.890625\n",
      "[2018-07-16 12:52:10.229183] Iteration 21300, train loss = 0.407204, train accuracy = 0.890625\n",
      "[2018-07-16 12:52:13.868663] Iteration 21400, train loss = 0.459040, train accuracy = 0.890625\n",
      "[2018-07-16 12:52:17.590158] Iteration 21500, train loss = 0.376154, train accuracy = 0.890625\n",
      "[2018-07-16 12:52:21.226352] Iteration 21600, train loss = 0.301074, train accuracy = 0.937500\n",
      "[2018-07-16 12:52:24.844113] Iteration 21700, train loss = 0.442950, train accuracy = 0.867188\n",
      "[2018-07-16 12:52:28.465979] Iteration 21800, train loss = 0.375906, train accuracy = 0.898438\n",
      "[2018-07-16 12:52:32.107836] Iteration 21900, train loss = 0.321689, train accuracy = 0.906250\n",
      "[2018-07-16 12:52:35.737877] Iteration 22000, train loss = 0.441177, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.847300\n",
      "[2018-07-16 12:52:40.482104] Iteration 22100, train loss = 0.378756, train accuracy = 0.882812\n",
      "[2018-07-16 12:52:44.131909] Iteration 22200, train loss = 0.330198, train accuracy = 0.906250\n",
      "[2018-07-16 12:52:47.763866] Iteration 22300, train loss = 0.329846, train accuracy = 0.906250\n",
      "[2018-07-16 12:52:51.411345] Iteration 22400, train loss = 0.328745, train accuracy = 0.906250\n",
      "[2018-07-16 12:52:55.051231] Iteration 22500, train loss = 0.333477, train accuracy = 0.914062\n",
      "[2018-07-16 12:52:58.732634] Iteration 22600, train loss = 0.296018, train accuracy = 0.921875\n",
      "[2018-07-16 12:53:02.398524] Iteration 22700, train loss = 0.400768, train accuracy = 0.906250\n",
      "[2018-07-16 12:53:06.052506] Iteration 22800, train loss = 0.333052, train accuracy = 0.914062\n",
      "[2018-07-16 12:53:09.681934] Iteration 22900, train loss = 0.432085, train accuracy = 0.843750\n",
      "[2018-07-16 12:53:13.312265] Iteration 23000, train loss = 0.500203, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.836900\n",
      "[2018-07-16 12:53:18.086624] Iteration 23100, train loss = 0.348050, train accuracy = 0.906250\n",
      "[2018-07-16 12:53:21.719968] Iteration 23200, train loss = 0.321060, train accuracy = 0.945312\n",
      "[2018-07-16 12:53:25.357458] Iteration 23300, train loss = 0.349228, train accuracy = 0.914062\n",
      "[2018-07-16 12:53:29.006624] Iteration 23400, train loss = 0.368195, train accuracy = 0.898438\n",
      "[2018-07-16 12:53:32.649163] Iteration 23500, train loss = 0.297888, train accuracy = 0.929688\n",
      "[2018-07-16 12:53:36.300007] Iteration 23600, train loss = 0.366235, train accuracy = 0.875000\n",
      "[2018-07-16 12:53:39.960256] Iteration 23700, train loss = 0.430443, train accuracy = 0.867188\n",
      "[2018-07-16 12:53:43.668392] Iteration 23800, train loss = 0.290589, train accuracy = 0.937500\n",
      "[2018-07-16 12:53:47.294455] Iteration 23900, train loss = 0.412608, train accuracy = 0.867188\n",
      "[2018-07-16 12:53:50.934279] Iteration 24000, train loss = 0.274706, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.845500\n",
      "[2018-07-16 12:53:55.691513] Iteration 24100, train loss = 0.351813, train accuracy = 0.921875\n",
      "[2018-07-16 12:53:59.323125] Iteration 24200, train loss = 0.256776, train accuracy = 0.953125\n",
      "[2018-07-16 12:54:02.965766] Iteration 24300, train loss = 0.304152, train accuracy = 0.937500\n",
      "[2018-07-16 12:54:06.593086] Iteration 24400, train loss = 0.450885, train accuracy = 0.859375\n",
      "[2018-07-16 12:54:10.223884] Iteration 24500, train loss = 0.379105, train accuracy = 0.867188\n",
      "[2018-07-16 12:54:13.855056] Iteration 24600, train loss = 0.363318, train accuracy = 0.875000\n",
      "[2018-07-16 12:54:17.504489] Iteration 24700, train loss = 0.297946, train accuracy = 0.906250\n",
      "[2018-07-16 12:54:21.146173] Iteration 24800, train loss = 0.359657, train accuracy = 0.875000\n",
      "[2018-07-16 12:54:24.872171] Iteration 24900, train loss = 0.324871, train accuracy = 0.914062\n",
      "[2018-07-16 12:54:28.519421] Iteration 25000, train loss = 0.477410, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.835600\n",
      "[2018-07-16 12:54:33.287318] Iteration 25100, train loss = 0.446758, train accuracy = 0.875000\n",
      "[2018-07-16 12:54:36.920106] Iteration 25200, train loss = 0.330930, train accuracy = 0.914062\n",
      "[2018-07-16 12:54:40.577707] Iteration 25300, train loss = 0.314786, train accuracy = 0.945312\n",
      "[2018-07-16 12:54:44.204753] Iteration 25400, train loss = 0.343295, train accuracy = 0.890625\n",
      "[2018-07-16 12:54:47.830095] Iteration 25500, train loss = 0.327825, train accuracy = 0.929688\n",
      "[2018-07-16 12:54:51.465424] Iteration 25600, train loss = 0.367495, train accuracy = 0.890625\n",
      "[2018-07-16 12:54:55.103297] Iteration 25700, train loss = 0.273094, train accuracy = 0.953125\n",
      "[2018-07-16 12:54:58.732614] Iteration 25800, train loss = 0.283156, train accuracy = 0.929688\n",
      "[2018-07-16 12:55:02.380443] Iteration 25900, train loss = 0.344907, train accuracy = 0.929688\n",
      "[2018-07-16 12:55:06.106356] Iteration 26000, train loss = 0.430455, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.854600\n",
      "[2018-07-16 12:55:10.857283] Iteration 26100, train loss = 0.348240, train accuracy = 0.890625\n",
      "[2018-07-16 12:55:14.494404] Iteration 26200, train loss = 0.358912, train accuracy = 0.906250\n",
      "[2018-07-16 12:55:18.138147] Iteration 26300, train loss = 0.306690, train accuracy = 0.914062\n",
      "[2018-07-16 12:55:21.768854] Iteration 26400, train loss = 0.334604, train accuracy = 0.906250\n",
      "[2018-07-16 12:55:25.411073] Iteration 26500, train loss = 0.265052, train accuracy = 0.945312\n",
      "[2018-07-16 12:55:29.052151] Iteration 26600, train loss = 0.357186, train accuracy = 0.921875\n",
      "[2018-07-16 12:55:32.683668] Iteration 26700, train loss = 0.314709, train accuracy = 0.906250\n",
      "[2018-07-16 12:55:36.302760] Iteration 26800, train loss = 0.448508, train accuracy = 0.867188\n",
      "[2018-07-16 12:55:39.930011] Iteration 26900, train loss = 0.286233, train accuracy = 0.937500\n",
      "[2018-07-16 12:55:43.561061] Iteration 27000, train loss = 0.332871, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.854000\n",
      "[2018-07-16 12:55:48.379594] Iteration 27100, train loss = 0.298780, train accuracy = 0.906250\n",
      "[2018-07-16 12:55:52.012552] Iteration 27200, train loss = 0.345698, train accuracy = 0.890625\n",
      "[2018-07-16 12:55:55.661258] Iteration 27300, train loss = 0.303260, train accuracy = 0.929688\n",
      "[2018-07-16 12:55:59.315065] Iteration 27400, train loss = 0.322247, train accuracy = 0.945312\n",
      "[2018-07-16 12:56:02.950759] Iteration 27500, train loss = 0.296816, train accuracy = 0.906250\n",
      "[2018-07-16 12:56:06.590524] Iteration 27600, train loss = 0.320003, train accuracy = 0.914062\n",
      "[2018-07-16 12:56:10.220313] Iteration 27700, train loss = 0.312841, train accuracy = 0.937500\n",
      "[2018-07-16 12:56:13.888996] Iteration 27800, train loss = 0.365699, train accuracy = 0.914062\n",
      "[2018-07-16 12:56:17.523105] Iteration 27900, train loss = 0.350227, train accuracy = 0.898438\n",
      "[2018-07-16 12:56:21.158887] Iteration 28000, train loss = 0.335121, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.855500\n",
      "[2018-07-16 12:56:25.895766] Iteration 28100, train loss = 0.374767, train accuracy = 0.867188\n",
      "[2018-07-16 12:56:29.605350] Iteration 28200, train loss = 0.371616, train accuracy = 0.882812\n",
      "[2018-07-16 12:56:33.227467] Iteration 28300, train loss = 0.406438, train accuracy = 0.867188\n",
      "[2018-07-16 12:56:36.872420] Iteration 28400, train loss = 0.395341, train accuracy = 0.859375\n",
      "[2018-07-16 12:56:40.517032] Iteration 28500, train loss = 0.272951, train accuracy = 0.921875\n",
      "[2018-07-16 12:56:44.162766] Iteration 28600, train loss = 0.296328, train accuracy = 0.929688\n",
      "[2018-07-16 12:56:47.806457] Iteration 28700, train loss = 0.357616, train accuracy = 0.898438\n",
      "[2018-07-16 12:56:51.440256] Iteration 28800, train loss = 0.355724, train accuracy = 0.906250\n",
      "[2018-07-16 12:56:55.071613] Iteration 28900, train loss = 0.384735, train accuracy = 0.882812\n",
      "[2018-07-16 12:56:58.707761] Iteration 29000, train loss = 0.403319, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.849000\n",
      "[2018-07-16 12:57:03.479030] Iteration 29100, train loss = 0.314089, train accuracy = 0.921875\n",
      "[2018-07-16 12:57:07.123951] Iteration 29200, train loss = 0.321146, train accuracy = 0.898438\n",
      "[2018-07-16 12:57:10.801631] Iteration 29300, train loss = 0.239276, train accuracy = 0.945312\n",
      "[2018-07-16 12:57:14.452609] Iteration 29400, train loss = 0.402759, train accuracy = 0.867188\n",
      "[2018-07-16 12:57:18.090446] Iteration 29500, train loss = 0.351353, train accuracy = 0.898438\n",
      "[2018-07-16 12:57:21.724536] Iteration 29600, train loss = 0.258241, train accuracy = 0.953125\n",
      "[2018-07-16 12:57:25.365312] Iteration 29700, train loss = 0.258523, train accuracy = 0.929688\n",
      "[2018-07-16 12:57:29.008655] Iteration 29800, train loss = 0.390710, train accuracy = 0.890625\n",
      "[2018-07-16 12:57:32.659643] Iteration 29900, train loss = 0.303939, train accuracy = 0.921875\n",
      "[2018-07-16 12:57:36.309162] Iteration 30000, train loss = 0.336001, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.807500\n",
      "[2018-07-16 12:57:41.106520] Iteration 30100, train loss = 0.333781, train accuracy = 0.914062\n",
      "[2018-07-16 12:57:44.787517] Iteration 30200, train loss = 0.512796, train accuracy = 0.882812\n",
      "[2018-07-16 12:57:48.423475] Iteration 30300, train loss = 0.336252, train accuracy = 0.921875\n",
      "[2018-07-16 12:57:52.077371] Iteration 30400, train loss = 0.341847, train accuracy = 0.937500\n",
      "[2018-07-16 12:57:55.798799] Iteration 30500, train loss = 0.403794, train accuracy = 0.898438\n",
      "[2018-07-16 12:57:59.416482] Iteration 30600, train loss = 0.318353, train accuracy = 0.945312\n",
      "[2018-07-16 12:58:03.051675] Iteration 30700, train loss = 0.300468, train accuracy = 0.898438\n",
      "[2018-07-16 12:58:06.678157] Iteration 30800, train loss = 0.443653, train accuracy = 0.875000\n",
      "[2018-07-16 12:58:10.314323] Iteration 30900, train loss = 0.469903, train accuracy = 0.867188\n",
      "[2018-07-16 12:58:13.969516] Iteration 31000, train loss = 0.306922, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.847500\n",
      "[2018-07-16 12:58:18.747676] Iteration 31100, train loss = 0.296722, train accuracy = 0.929688\n",
      "[2018-07-16 12:58:22.405555] Iteration 31200, train loss = 0.260975, train accuracy = 0.945312\n",
      "[2018-07-16 12:58:26.053989] Iteration 31300, train loss = 0.321499, train accuracy = 0.921875\n",
      "[2018-07-16 12:58:29.688114] Iteration 31400, train loss = 0.421633, train accuracy = 0.843750\n",
      "[2018-07-16 12:58:33.342114] Iteration 31500, train loss = 0.250828, train accuracy = 0.953125\n",
      "[2018-07-16 12:58:37.082215] Iteration 31600, train loss = 0.332343, train accuracy = 0.898438\n",
      "[2018-07-16 12:58:40.718515] Iteration 31700, train loss = 0.296483, train accuracy = 0.921875\n",
      "[2018-07-16 12:58:44.356249] Iteration 31800, train loss = 0.314579, train accuracy = 0.921875\n",
      "[2018-07-16 12:58:47.982037] Iteration 31900, train loss = 0.343420, train accuracy = 0.906250\n",
      "[2018-07-16 12:58:51.621273] Iteration 32000, train loss = 0.281570, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.836000\n",
      "[2018-07-16 12:58:56.372564] Iteration 32100, train loss = 0.383654, train accuracy = 0.929688\n",
      "[2018-07-16 12:58:59.990799] Iteration 32200, train loss = 0.393455, train accuracy = 0.890625\n",
      "[2018-07-16 12:59:03.623388] Iteration 32300, train loss = 0.430891, train accuracy = 0.851562\n",
      "[2018-07-16 12:59:07.258437] Iteration 32400, train loss = 0.280093, train accuracy = 0.953125\n",
      "[2018-07-16 12:59:10.907443] Iteration 32500, train loss = 0.261702, train accuracy = 0.945312\n",
      "[2018-07-16 12:59:14.549200] Iteration 32600, train loss = 0.391680, train accuracy = 0.882812\n",
      "[2018-07-16 12:59:18.274067] Iteration 32700, train loss = 0.366874, train accuracy = 0.898438\n",
      "[2018-07-16 12:59:21.922035] Iteration 32800, train loss = 0.348436, train accuracy = 0.921875\n",
      "[2018-07-16 12:59:25.572947] Iteration 32900, train loss = 0.233102, train accuracy = 0.968750\n",
      "[2018-07-16 12:59:29.210402] Iteration 33000, train loss = 0.268764, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.844800\n",
      "[2018-07-16 12:59:33.962764] Iteration 33100, train loss = 0.334194, train accuracy = 0.898438\n",
      "[2018-07-16 12:59:37.611126] Iteration 33200, train loss = 0.296413, train accuracy = 0.937500\n",
      "[2018-07-16 12:59:41.247478] Iteration 33300, train loss = 0.296962, train accuracy = 0.921875\n",
      "[2018-07-16 12:59:44.878630] Iteration 33400, train loss = 0.316444, train accuracy = 0.929688\n",
      "[2018-07-16 12:59:48.517034] Iteration 33500, train loss = 0.327832, train accuracy = 0.929688\n",
      "[2018-07-16 12:59:52.158707] Iteration 33600, train loss = 0.367689, train accuracy = 0.867188\n",
      "[2018-07-16 12:59:55.804884] Iteration 33700, train loss = 0.375927, train accuracy = 0.898438\n",
      "[2018-07-16 12:59:59.523708] Iteration 33800, train loss = 0.361745, train accuracy = 0.906250\n",
      "[2018-07-16 13:00:03.161564] Iteration 33900, train loss = 0.339361, train accuracy = 0.890625\n",
      "[2018-07-16 13:00:06.792595] Iteration 34000, train loss = 0.400831, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.854800\n",
      "[2018-07-16 13:00:11.582601] Iteration 34100, train loss = 0.379283, train accuracy = 0.882812\n",
      "[2018-07-16 13:00:15.228802] Iteration 34200, train loss = 0.332451, train accuracy = 0.906250\n",
      "[2018-07-16 13:00:18.887354] Iteration 34300, train loss = 0.335544, train accuracy = 0.882812\n",
      "[2018-07-16 13:00:22.518222] Iteration 34400, train loss = 0.301886, train accuracy = 0.921875\n",
      "[2018-07-16 13:00:26.157639] Iteration 34500, train loss = 0.366652, train accuracy = 0.921875\n",
      "[2018-07-16 13:00:29.796535] Iteration 34600, train loss = 0.356721, train accuracy = 0.890625\n",
      "[2018-07-16 13:00:33.427718] Iteration 34700, train loss = 0.326600, train accuracy = 0.914062\n",
      "[2018-07-16 13:00:37.047195] Iteration 34800, train loss = 0.324905, train accuracy = 0.898438\n",
      "[2018-07-16 13:00:40.749899] Iteration 34900, train loss = 0.339026, train accuracy = 0.906250\n",
      "[2018-07-16 13:00:44.435727] Iteration 35000, train loss = 0.390628, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.836500\n",
      "[2018-07-16 13:00:49.199471] Iteration 35100, train loss = 0.381592, train accuracy = 0.882812\n",
      "[2018-07-16 13:00:52.851254] Iteration 35200, train loss = 0.345587, train accuracy = 0.906250\n",
      "[2018-07-16 13:00:56.501178] Iteration 35300, train loss = 0.325889, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:00.130908] Iteration 35400, train loss = 0.358811, train accuracy = 0.890625\n",
      "[2018-07-16 13:01:03.772298] Iteration 35500, train loss = 0.353653, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:07.434452] Iteration 35600, train loss = 0.286581, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:11.061300] Iteration 35700, train loss = 0.316397, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:14.692643] Iteration 35800, train loss = 0.347829, train accuracy = 0.875000\n",
      "[2018-07-16 13:01:18.328806] Iteration 35900, train loss = 0.388375, train accuracy = 0.898438\n",
      "[2018-07-16 13:01:21.959213] Iteration 36000, train loss = 0.353681, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.850700\n",
      "[2018-07-16 13:01:26.874216] Iteration 36100, train loss = 0.440226, train accuracy = 0.875000\n",
      "[2018-07-16 13:01:30.520097] Iteration 36200, train loss = 0.378649, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:34.155895] Iteration 36300, train loss = 0.339620, train accuracy = 0.914062\n",
      "[2018-07-16 13:01:37.815415] Iteration 36400, train loss = 0.429353, train accuracy = 0.867188\n",
      "[2018-07-16 13:01:41.450559] Iteration 36500, train loss = 0.287346, train accuracy = 0.945312\n",
      "[2018-07-16 13:01:45.081376] Iteration 36600, train loss = 0.326485, train accuracy = 0.890625\n",
      "[2018-07-16 13:01:48.705228] Iteration 36700, train loss = 0.334422, train accuracy = 0.921875\n",
      "[2018-07-16 13:01:52.349325] Iteration 36800, train loss = 0.242739, train accuracy = 0.929688\n",
      "[2018-07-16 13:01:55.971344] Iteration 36900, train loss = 0.333452, train accuracy = 0.906250\n",
      "[2018-07-16 13:01:59.593308] Iteration 37000, train loss = 0.293884, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.852200\n",
      "[2018-07-16 13:02:04.347128] Iteration 37100, train loss = 0.349365, train accuracy = 0.906250\n",
      "[2018-07-16 13:02:08.037036] Iteration 37200, train loss = 0.236349, train accuracy = 0.953125\n",
      "[2018-07-16 13:02:11.665196] Iteration 37300, train loss = 0.420284, train accuracy = 0.875000\n",
      "[2018-07-16 13:02:15.306997] Iteration 37400, train loss = 0.341238, train accuracy = 0.890625\n",
      "[2018-07-16 13:02:18.943140] Iteration 37500, train loss = 0.276131, train accuracy = 0.929688\n",
      "[2018-07-16 13:02:22.578123] Iteration 37600, train loss = 0.330193, train accuracy = 0.914062\n",
      "[2018-07-16 13:02:26.237100] Iteration 37700, train loss = 0.263504, train accuracy = 0.960938\n",
      "[2018-07-16 13:02:29.890815] Iteration 37800, train loss = 0.249115, train accuracy = 0.945312\n",
      "[2018-07-16 13:02:33.537530] Iteration 37900, train loss = 0.235643, train accuracy = 0.929688\n",
      "[2018-07-16 13:02:37.202350] Iteration 38000, train loss = 0.218826, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.856600\n",
      "[2018-07-16 13:02:41.978812] Iteration 38100, train loss = 0.310965, train accuracy = 0.921875\n",
      "[2018-07-16 13:02:45.610424] Iteration 38200, train loss = 0.382963, train accuracy = 0.906250\n",
      "[2018-07-16 13:02:49.328398] Iteration 38300, train loss = 0.362327, train accuracy = 0.898438\n",
      "[2018-07-16 13:02:52.954249] Iteration 38400, train loss = 0.258309, train accuracy = 0.937500\n",
      "[2018-07-16 13:02:56.599287] Iteration 38500, train loss = 0.363745, train accuracy = 0.906250\n",
      "[2018-07-16 13:03:00.229252] Iteration 38600, train loss = 0.314722, train accuracy = 0.906250\n",
      "[2018-07-16 13:03:03.870685] Iteration 38700, train loss = 0.435253, train accuracy = 0.867188\n",
      "[2018-07-16 13:03:07.500307] Iteration 38800, train loss = 0.224228, train accuracy = 0.945312\n",
      "[2018-07-16 13:03:11.174771] Iteration 38900, train loss = 0.343041, train accuracy = 0.898438\n",
      "[2018-07-16 13:03:14.812805] Iteration 39000, train loss = 0.329200, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.847100\n",
      "[2018-07-16 13:03:19.582453] Iteration 39100, train loss = 0.358333, train accuracy = 0.906250\n",
      "[2018-07-16 13:03:23.237318] Iteration 39200, train loss = 0.335325, train accuracy = 0.921875\n",
      "[2018-07-16 13:03:26.872523] Iteration 39300, train loss = 0.310503, train accuracy = 0.906250\n",
      "[2018-07-16 13:03:30.576143] Iteration 39400, train loss = 0.322384, train accuracy = 0.921875\n",
      "[2018-07-16 13:03:34.206149] Iteration 39500, train loss = 0.272706, train accuracy = 0.937500\n",
      "[2018-07-16 13:03:37.844116] Iteration 39600, train loss = 0.475745, train accuracy = 0.859375\n",
      "[2018-07-16 13:03:41.483331] Iteration 39700, train loss = 0.319465, train accuracy = 0.890625\n",
      "[2018-07-16 13:03:45.109484] Iteration 39800, train loss = 0.349431, train accuracy = 0.875000\n",
      "[2018-07-16 13:03:48.741782] Iteration 39900, train loss = 0.304509, train accuracy = 0.906250\n",
      "[2018-07-16 13:03:52.400458] Iteration 40000, train loss = 0.289569, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.850600\n",
      "[2018-07-16 13:03:57.170892] Iteration 40100, train loss = 0.285617, train accuracy = 0.929688\n",
      "[2018-07-16 13:04:00.806866] Iteration 40200, train loss = 0.311414, train accuracy = 0.929688\n",
      "[2018-07-16 13:04:04.444844] Iteration 40300, train loss = 0.302737, train accuracy = 0.929688\n",
      "[2018-07-16 13:04:08.077861] Iteration 40400, train loss = 0.245440, train accuracy = 0.937500\n",
      "[2018-07-16 13:04:11.797769] Iteration 40500, train loss = 0.323106, train accuracy = 0.898438\n",
      "[2018-07-16 13:04:15.438459] Iteration 40600, train loss = 0.340198, train accuracy = 0.898438\n",
      "[2018-07-16 13:04:19.077588] Iteration 40700, train loss = 0.299231, train accuracy = 0.937500\n",
      "[2018-07-16 13:04:22.712391] Iteration 40800, train loss = 0.250202, train accuracy = 0.937500\n",
      "[2018-07-16 13:04:26.353262] Iteration 40900, train loss = 0.278860, train accuracy = 0.945312\n",
      "[2018-07-16 13:04:29.984869] Iteration 41000, train loss = 0.297539, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.831100\n",
      "[2018-07-16 13:04:34.738687] Iteration 41100, train loss = 0.281015, train accuracy = 0.937500\n",
      "[2018-07-16 13:04:38.377114] Iteration 41200, train loss = 0.302076, train accuracy = 0.929688\n",
      "[2018-07-16 13:04:41.997979] Iteration 41300, train loss = 0.281646, train accuracy = 0.953125\n",
      "[2018-07-16 13:04:45.635739] Iteration 41400, train loss = 0.305718, train accuracy = 0.898438\n",
      "[2018-07-16 13:04:49.280349] Iteration 41500, train loss = 0.344054, train accuracy = 0.898438\n",
      "[2018-07-16 13:04:52.976797] Iteration 41600, train loss = 0.390824, train accuracy = 0.914062\n",
      "[2018-07-16 13:04:56.647899] Iteration 41700, train loss = 0.373474, train accuracy = 0.882812\n",
      "[2018-07-16 13:05:00.279562] Iteration 41800, train loss = 0.341572, train accuracy = 0.890625\n",
      "[2018-07-16 13:05:03.909268] Iteration 41900, train loss = 0.305446, train accuracy = 0.921875\n",
      "[2018-07-16 13:05:07.550872] Iteration 42000, train loss = 0.198503, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.860400\n",
      "[2018-07-16 13:05:12.295629] Iteration 42100, train loss = 0.419771, train accuracy = 0.859375\n",
      "[2018-07-16 13:05:15.921627] Iteration 42200, train loss = 0.269010, train accuracy = 0.945312\n",
      "[2018-07-16 13:05:19.555662] Iteration 42300, train loss = 0.308223, train accuracy = 0.906250\n",
      "[2018-07-16 13:05:23.181410] Iteration 42400, train loss = 0.268199, train accuracy = 0.945312\n",
      "[2018-07-16 13:05:26.837789] Iteration 42500, train loss = 0.287699, train accuracy = 0.921875\n",
      "[2018-07-16 13:05:30.470903] Iteration 42600, train loss = 0.406259, train accuracy = 0.843750\n",
      "[2018-07-16 13:05:34.104810] Iteration 42700, train loss = 0.289619, train accuracy = 0.921875\n",
      "[2018-07-16 13:05:37.856434] Iteration 42800, train loss = 0.285757, train accuracy = 0.929688\n",
      "[2018-07-16 13:05:41.488794] Iteration 42900, train loss = 0.324582, train accuracy = 0.914062\n",
      "[2018-07-16 13:05:45.104955] Iteration 43000, train loss = 0.421437, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.853800\n",
      "[2018-07-16 13:05:49.871151] Iteration 43100, train loss = 0.404797, train accuracy = 0.882812\n",
      "[2018-07-16 13:05:53.506062] Iteration 43200, train loss = 0.350351, train accuracy = 0.906250\n",
      "[2018-07-16 13:05:57.138761] Iteration 43300, train loss = 0.240816, train accuracy = 0.960938\n",
      "[2018-07-16 13:06:00.770215] Iteration 43400, train loss = 0.292716, train accuracy = 0.921875\n",
      "[2018-07-16 13:06:04.423963] Iteration 43500, train loss = 0.335212, train accuracy = 0.906250\n",
      "[2018-07-16 13:06:08.047255] Iteration 43600, train loss = 0.367716, train accuracy = 0.875000\n",
      "[2018-07-16 13:06:11.680749] Iteration 43700, train loss = 0.274544, train accuracy = 0.929688\n",
      "[2018-07-16 13:06:15.327182] Iteration 43800, train loss = 0.384612, train accuracy = 0.882812\n",
      "[2018-07-16 13:06:19.029515] Iteration 43900, train loss = 0.280616, train accuracy = 0.937500\n",
      "[2018-07-16 13:06:22.664910] Iteration 44000, train loss = 0.349568, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.854700\n",
      "[2018-07-16 13:06:27.436597] Iteration 44100, train loss = 0.332080, train accuracy = 0.945312\n",
      "[2018-07-16 13:06:31.078645] Iteration 44200, train loss = 0.308450, train accuracy = 0.906250\n",
      "[2018-07-16 13:06:34.713858] Iteration 44300, train loss = 0.319152, train accuracy = 0.906250\n",
      "[2018-07-16 13:06:38.343938] Iteration 44400, train loss = 0.285466, train accuracy = 0.937500\n",
      "[2018-07-16 13:06:41.971992] Iteration 44500, train loss = 0.417832, train accuracy = 0.882812\n",
      "[2018-07-16 13:06:45.602349] Iteration 44600, train loss = 0.313592, train accuracy = 0.937500\n",
      "[2018-07-16 13:06:49.221214] Iteration 44700, train loss = 0.363474, train accuracy = 0.890625\n",
      "[2018-07-16 13:06:52.858459] Iteration 44800, train loss = 0.417847, train accuracy = 0.882812\n",
      "[2018-07-16 13:06:56.474520] Iteration 44900, train loss = 0.238249, train accuracy = 0.960938\n",
      "[2018-07-16 13:07:00.172673] Iteration 45000, train loss = 0.277517, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.856300\n",
      "[2018-07-16 13:07:04.946654] Iteration 45100, train loss = 0.410374, train accuracy = 0.890625\n",
      "[2018-07-16 13:07:08.576940] Iteration 45200, train loss = 0.342833, train accuracy = 0.929688\n",
      "[2018-07-16 13:07:12.216569] Iteration 45300, train loss = 0.346039, train accuracy = 0.921875\n",
      "[2018-07-16 13:07:15.853740] Iteration 45400, train loss = 0.400655, train accuracy = 0.890625\n",
      "[2018-07-16 13:07:19.484112] Iteration 45500, train loss = 0.332916, train accuracy = 0.906250\n",
      "[2018-07-16 13:07:23.120507] Iteration 45600, train loss = 0.394732, train accuracy = 0.914062\n",
      "[2018-07-16 13:07:26.774744] Iteration 45700, train loss = 0.282904, train accuracy = 0.921875\n",
      "[2018-07-16 13:07:30.407735] Iteration 45800, train loss = 0.305380, train accuracy = 0.937500\n",
      "[2018-07-16 13:07:34.031002] Iteration 45900, train loss = 0.354807, train accuracy = 0.906250\n",
      "[2018-07-16 13:07:37.667287] Iteration 46000, train loss = 0.317209, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.841100\n",
      "[2018-07-16 13:07:42.518966] Iteration 46100, train loss = 0.297198, train accuracy = 0.937500\n",
      "[2018-07-16 13:07:46.158645] Iteration 46200, train loss = 0.357429, train accuracy = 0.898438\n",
      "[2018-07-16 13:07:49.795604] Iteration 46300, train loss = 0.251120, train accuracy = 0.921875\n",
      "[2018-07-16 13:07:53.441359] Iteration 46400, train loss = 0.312972, train accuracy = 0.921875\n",
      "[2018-07-16 13:07:57.067142] Iteration 46500, train loss = 0.290020, train accuracy = 0.953125\n",
      "[2018-07-16 13:08:00.728293] Iteration 46600, train loss = 0.413273, train accuracy = 0.882812\n",
      "[2018-07-16 13:08:04.363412] Iteration 46700, train loss = 0.329018, train accuracy = 0.929688\n",
      "[2018-07-16 13:08:08.029185] Iteration 46800, train loss = 0.503378, train accuracy = 0.835938\n",
      "[2018-07-16 13:08:11.663819] Iteration 46900, train loss = 0.388780, train accuracy = 0.890625\n",
      "[2018-07-16 13:08:15.306918] Iteration 47000, train loss = 0.313369, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.857900\n",
      "[2018-07-16 13:08:20.077774] Iteration 47100, train loss = 0.261577, train accuracy = 0.937500\n",
      "[2018-07-16 13:08:23.769675] Iteration 47200, train loss = 0.314801, train accuracy = 0.921875\n",
      "[2018-07-16 13:08:27.390252] Iteration 47300, train loss = 0.314248, train accuracy = 0.921875\n",
      "[2018-07-16 13:08:31.023326] Iteration 47400, train loss = 0.276854, train accuracy = 0.929688\n",
      "[2018-07-16 13:08:34.644978] Iteration 47500, train loss = 0.291725, train accuracy = 0.929688\n",
      "[2018-07-16 13:08:38.282955] Iteration 47600, train loss = 0.309765, train accuracy = 0.906250\n",
      "[2018-07-16 13:08:41.950571] Iteration 47700, train loss = 0.297906, train accuracy = 0.921875\n",
      "[2018-07-16 13:08:45.586191] Iteration 47800, train loss = 0.348760, train accuracy = 0.906250\n",
      "[2018-07-16 13:08:49.212551] Iteration 47900, train loss = 0.312195, train accuracy = 0.937500\n",
      "[2018-07-16 13:08:52.870565] Iteration 48000, train loss = 0.288098, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.820100\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 13:08:57.610308] Iteration 48100, train loss = 0.327192, train accuracy = 0.921875\n",
      "[2018-07-16 13:09:01.259171] Iteration 48200, train loss = 0.318858, train accuracy = 0.914062\n",
      "[2018-07-16 13:09:04.940336] Iteration 48300, train loss = 0.291903, train accuracy = 0.929688\n",
      "[2018-07-16 13:09:08.616722] Iteration 48400, train loss = 0.249592, train accuracy = 0.914062\n",
      "[2018-07-16 13:09:12.258124] Iteration 48500, train loss = 0.313075, train accuracy = 0.921875\n",
      "[2018-07-16 13:09:15.881889] Iteration 48600, train loss = 0.225444, train accuracy = 0.953125\n",
      "[2018-07-16 13:09:19.520582] Iteration 48700, train loss = 0.299342, train accuracy = 0.921875\n",
      "[2018-07-16 13:09:23.156931] Iteration 48800, train loss = 0.264149, train accuracy = 0.929688\n",
      "[2018-07-16 13:09:26.786469] Iteration 48900, train loss = 0.269140, train accuracy = 0.945312\n",
      "[2018-07-16 13:09:30.416178] Iteration 49000, train loss = 0.220935, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.881600\n",
      "[2018-07-16 13:09:35.209897] Iteration 49100, train loss = 0.287969, train accuracy = 0.953125\n",
      "[2018-07-16 13:09:38.853318] Iteration 49200, train loss = 0.325018, train accuracy = 0.906250\n",
      "[2018-07-16 13:09:42.489609] Iteration 49300, train loss = 0.240359, train accuracy = 0.953125\n",
      "[2018-07-16 13:09:46.139611] Iteration 49400, train loss = 0.228558, train accuracy = 0.953125\n",
      "[2018-07-16 13:09:49.869119] Iteration 49500, train loss = 0.261614, train accuracy = 0.953125\n",
      "[2018-07-16 13:09:53.524118] Iteration 49600, train loss = 0.297840, train accuracy = 0.937500\n",
      "[2018-07-16 13:09:57.150756] Iteration 49700, train loss = 0.198702, train accuracy = 0.960938\n",
      "[2018-07-16 13:10:00.769718] Iteration 49800, train loss = 0.276341, train accuracy = 0.921875\n",
      "[2018-07-16 13:10:04.406705] Iteration 49900, train loss = 0.291502, train accuracy = 0.937500\n",
      "[2018-07-16 13:10:08.038751] Iteration 50000, train loss = 0.288376, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.879500\n",
      "[2018-07-16 13:10:12.807580] Iteration 50100, train loss = 0.220870, train accuracy = 0.945312\n",
      "[2018-07-16 13:10:16.452955] Iteration 50200, train loss = 0.396396, train accuracy = 0.890625\n",
      "[2018-07-16 13:10:20.109615] Iteration 50300, train loss = 0.423106, train accuracy = 0.875000\n",
      "[2018-07-16 13:10:23.758306] Iteration 50400, train loss = 0.294852, train accuracy = 0.914062\n",
      "[2018-07-16 13:10:27.426637] Iteration 50500, train loss = 0.244815, train accuracy = 0.953125\n",
      "[2018-07-16 13:10:31.131980] Iteration 50600, train loss = 0.389050, train accuracy = 0.906250\n",
      "[2018-07-16 13:10:34.777877] Iteration 50700, train loss = 0.225245, train accuracy = 0.945312\n",
      "[2018-07-16 13:10:38.408310] Iteration 50800, train loss = 0.199055, train accuracy = 0.960938\n",
      "[2018-07-16 13:10:42.057392] Iteration 50900, train loss = 0.312962, train accuracy = 0.921875\n",
      "[2018-07-16 13:10:45.677408] Iteration 51000, train loss = 0.261610, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.883500\n",
      "[2018-07-16 13:10:50.452442] Iteration 51100, train loss = 0.267527, train accuracy = 0.921875\n",
      "[2018-07-16 13:10:54.094162] Iteration 51200, train loss = 0.241370, train accuracy = 0.953125\n",
      "[2018-07-16 13:10:57.736710] Iteration 51300, train loss = 0.251883, train accuracy = 0.953125\n",
      "[2018-07-16 13:11:01.382603] Iteration 51400, train loss = 0.240874, train accuracy = 0.945312\n",
      "[2018-07-16 13:11:05.035728] Iteration 51500, train loss = 0.298287, train accuracy = 0.937500\n",
      "[2018-07-16 13:11:08.671971] Iteration 51600, train loss = 0.247749, train accuracy = 0.945312\n",
      "[2018-07-16 13:11:12.396531] Iteration 51700, train loss = 0.271448, train accuracy = 0.937500\n",
      "[2018-07-16 13:11:16.031967] Iteration 51800, train loss = 0.208523, train accuracy = 0.953125\n",
      "[2018-07-16 13:11:19.665171] Iteration 51900, train loss = 0.295333, train accuracy = 0.929688\n",
      "[2018-07-16 13:11:23.306298] Iteration 52000, train loss = 0.248167, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.880900\n",
      "[2018-07-16 13:11:28.056690] Iteration 52100, train loss = 0.363541, train accuracy = 0.898438\n",
      "[2018-07-16 13:11:31.694707] Iteration 52200, train loss = 0.228906, train accuracy = 0.937500\n",
      "[2018-07-16 13:11:35.325736] Iteration 52300, train loss = 0.244867, train accuracy = 0.953125\n",
      "[2018-07-16 13:11:38.952512] Iteration 52400, train loss = 0.206898, train accuracy = 0.953125\n",
      "[2018-07-16 13:11:42.583086] Iteration 52500, train loss = 0.316599, train accuracy = 0.921875\n",
      "[2018-07-16 13:11:46.218663] Iteration 52600, train loss = 0.271507, train accuracy = 0.937500\n",
      "[2018-07-16 13:11:49.874083] Iteration 52700, train loss = 0.298025, train accuracy = 0.906250\n",
      "[2018-07-16 13:11:53.600399] Iteration 52800, train loss = 0.242625, train accuracy = 0.945312\n",
      "[2018-07-16 13:11:57.249177] Iteration 52900, train loss = 0.331526, train accuracy = 0.921875\n",
      "[2018-07-16 13:12:00.881659] Iteration 53000, train loss = 0.252125, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.879500\n",
      "[2018-07-16 13:12:05.683774] Iteration 53100, train loss = 0.296035, train accuracy = 0.921875\n",
      "[2018-07-16 13:12:09.319190] Iteration 53200, train loss = 0.255453, train accuracy = 0.953125\n",
      "[2018-07-16 13:12:12.960739] Iteration 53300, train loss = 0.223984, train accuracy = 0.960938\n",
      "[2018-07-16 13:12:16.605072] Iteration 53400, train loss = 0.309247, train accuracy = 0.921875\n",
      "[2018-07-16 13:12:20.235943] Iteration 53500, train loss = 0.313138, train accuracy = 0.921875\n",
      "[2018-07-16 13:12:23.872268] Iteration 53600, train loss = 0.358662, train accuracy = 0.914062\n",
      "[2018-07-16 13:12:27.505317] Iteration 53700, train loss = 0.275343, train accuracy = 0.929688\n",
      "[2018-07-16 13:12:31.133585] Iteration 53800, train loss = 0.273598, train accuracy = 0.937500\n",
      "[2018-07-16 13:12:34.800713] Iteration 53900, train loss = 0.243038, train accuracy = 0.921875\n",
      "[2018-07-16 13:12:38.502455] Iteration 54000, train loss = 0.241552, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.880100\n",
      "[2018-07-16 13:12:43.267541] Iteration 54100, train loss = 0.201607, train accuracy = 0.968750\n",
      "[2018-07-16 13:12:46.919015] Iteration 54200, train loss = 0.230482, train accuracy = 0.937500\n",
      "[2018-07-16 13:12:50.568648] Iteration 54300, train loss = 0.285043, train accuracy = 0.929688\n",
      "[2018-07-16 13:12:54.205597] Iteration 54400, train loss = 0.254321, train accuracy = 0.937500\n",
      "[2018-07-16 13:12:57.842866] Iteration 54500, train loss = 0.209607, train accuracy = 0.968750\n",
      "[2018-07-16 13:13:01.476831] Iteration 54600, train loss = 0.386827, train accuracy = 0.867188\n",
      "[2018-07-16 13:13:05.126884] Iteration 54700, train loss = 0.332094, train accuracy = 0.898438\n",
      "[2018-07-16 13:13:08.762396] Iteration 54800, train loss = 0.289771, train accuracy = 0.945312\n",
      "[2018-07-16 13:13:12.388425] Iteration 54900, train loss = 0.283746, train accuracy = 0.914062\n",
      "[2018-07-16 13:13:16.014699] Iteration 55000, train loss = 0.311607, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.879300\n",
      "[2018-07-16 13:13:20.924452] Iteration 55100, train loss = 0.245008, train accuracy = 0.953125\n",
      "[2018-07-16 13:13:24.547082] Iteration 55200, train loss = 0.277791, train accuracy = 0.937500\n",
      "[2018-07-16 13:13:28.203011] Iteration 55300, train loss = 0.333811, train accuracy = 0.929688\n",
      "[2018-07-16 13:13:31.845863] Iteration 55400, train loss = 0.289799, train accuracy = 0.929688\n",
      "[2018-07-16 13:13:35.502104] Iteration 55500, train loss = 0.221296, train accuracy = 0.945312\n",
      "[2018-07-16 13:13:39.139571] Iteration 55600, train loss = 0.312289, train accuracy = 0.921875\n",
      "[2018-07-16 13:13:42.773666] Iteration 55700, train loss = 0.227472, train accuracy = 0.945312\n",
      "[2018-07-16 13:13:46.412363] Iteration 55800, train loss = 0.245536, train accuracy = 0.945312\n",
      "[2018-07-16 13:13:50.055686] Iteration 55900, train loss = 0.199544, train accuracy = 0.968750\n",
      "[2018-07-16 13:13:53.696595] Iteration 56000, train loss = 0.288245, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.882200\n",
      "[2018-07-16 13:13:58.443588] Iteration 56100, train loss = 0.259860, train accuracy = 0.929688\n",
      "[2018-07-16 13:14:02.142215] Iteration 56200, train loss = 0.327662, train accuracy = 0.906250\n",
      "[2018-07-16 13:14:05.775073] Iteration 56300, train loss = 0.289423, train accuracy = 0.945312\n",
      "[2018-07-16 13:14:09.390952] Iteration 56400, train loss = 0.310079, train accuracy = 0.914062\n",
      "[2018-07-16 13:14:13.036158] Iteration 56500, train loss = 0.266254, train accuracy = 0.937500\n",
      "[2018-07-16 13:14:16.699879] Iteration 56600, train loss = 0.279367, train accuracy = 0.937500\n",
      "[2018-07-16 13:14:20.351055] Iteration 56700, train loss = 0.270060, train accuracy = 0.914062\n",
      "[2018-07-16 13:14:23.976525] Iteration 56800, train loss = 0.379925, train accuracy = 0.906250\n",
      "[2018-07-16 13:14:27.616222] Iteration 56900, train loss = 0.228916, train accuracy = 0.968750\n",
      "[2018-07-16 13:14:31.256021] Iteration 57000, train loss = 0.240229, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.881700\n",
      "[2018-07-16 13:14:36.053907] Iteration 57100, train loss = 0.260276, train accuracy = 0.929688\n",
      "[2018-07-16 13:14:39.693737] Iteration 57200, train loss = 0.273998, train accuracy = 0.937500\n",
      "[2018-07-16 13:14:43.392951] Iteration 57300, train loss = 0.304199, train accuracy = 0.906250\n",
      "[2018-07-16 13:14:47.019819] Iteration 57400, train loss = 0.304646, train accuracy = 0.906250\n",
      "[2018-07-16 13:14:50.657808] Iteration 57500, train loss = 0.259636, train accuracy = 0.945312\n",
      "[2018-07-16 13:14:54.285999] Iteration 57600, train loss = 0.213841, train accuracy = 0.945312\n",
      "[2018-07-16 13:14:57.905132] Iteration 57700, train loss = 0.251692, train accuracy = 0.937500\n",
      "[2018-07-16 13:15:01.544199] Iteration 57800, train loss = 0.325887, train accuracy = 0.914062\n",
      "[2018-07-16 13:15:05.184373] Iteration 57900, train loss = 0.319275, train accuracy = 0.906250\n",
      "[2018-07-16 13:15:08.835647] Iteration 58000, train loss = 0.255436, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.880400\n",
      "[2018-07-16 13:15:13.595681] Iteration 58100, train loss = 0.301290, train accuracy = 0.898438\n",
      "[2018-07-16 13:15:17.253810] Iteration 58200, train loss = 0.381193, train accuracy = 0.875000\n",
      "[2018-07-16 13:15:20.885948] Iteration 58300, train loss = 0.256716, train accuracy = 0.960938\n",
      "[2018-07-16 13:15:24.614415] Iteration 58400, train loss = 0.193838, train accuracy = 0.968750\n",
      "[2018-07-16 13:15:28.251738] Iteration 58500, train loss = 0.262271, train accuracy = 0.929688\n",
      "[2018-07-16 13:15:31.893479] Iteration 58600, train loss = 0.255664, train accuracy = 0.945312\n",
      "[2018-07-16 13:15:35.536677] Iteration 58700, train loss = 0.376986, train accuracy = 0.898438\n",
      "[2018-07-16 13:15:39.166539] Iteration 58800, train loss = 0.315102, train accuracy = 0.914062\n",
      "[2018-07-16 13:15:42.809833] Iteration 58900, train loss = 0.331416, train accuracy = 0.898438\n",
      "[2018-07-16 13:15:46.433577] Iteration 59000, train loss = 0.291762, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.879900\n",
      "[2018-07-16 13:15:51.195022] Iteration 59100, train loss = 0.266949, train accuracy = 0.937500\n",
      "[2018-07-16 13:15:54.837036] Iteration 59200, train loss = 0.272950, train accuracy = 0.921875\n",
      "[2018-07-16 13:15:58.477434] Iteration 59300, train loss = 0.196248, train accuracy = 0.976562\n",
      "[2018-07-16 13:16:02.116202] Iteration 59400, train loss = 0.282239, train accuracy = 0.937500\n",
      "[2018-07-16 13:16:05.838097] Iteration 59500, train loss = 0.226043, train accuracy = 0.976562\n",
      "[2018-07-16 13:16:09.481218] Iteration 59600, train loss = 0.318231, train accuracy = 0.929688\n",
      "[2018-07-16 13:16:13.124610] Iteration 59700, train loss = 0.280588, train accuracy = 0.945312\n",
      "[2018-07-16 13:16:16.759959] Iteration 59800, train loss = 0.262377, train accuracy = 0.953125\n",
      "[2018-07-16 13:16:20.404111] Iteration 59900, train loss = 0.263951, train accuracy = 0.921875\n",
      "[2018-07-16 13:16:24.032224] Iteration 60000, train loss = 0.233676, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.878900\n",
      "[2018-07-16 13:16:28.788802] Iteration 60100, train loss = 0.226657, train accuracy = 0.968750\n",
      "[2018-07-16 13:16:32.438036] Iteration 60200, train loss = 0.362399, train accuracy = 0.914062\n",
      "[2018-07-16 13:16:36.075196] Iteration 60300, train loss = 0.240536, train accuracy = 0.945312\n",
      "[2018-07-16 13:16:39.717132] Iteration 60400, train loss = 0.299316, train accuracy = 0.929688\n",
      "[2018-07-16 13:16:43.374012] Iteration 60500, train loss = 0.224055, train accuracy = 0.945312\n",
      "[2018-07-16 13:16:47.050269] Iteration 60600, train loss = 0.205382, train accuracy = 0.960938\n",
      "[2018-07-16 13:16:50.746165] Iteration 60700, train loss = 0.224671, train accuracy = 0.953125\n",
      "[2018-07-16 13:16:54.389164] Iteration 60800, train loss = 0.250085, train accuracy = 0.960938\n",
      "[2018-07-16 13:16:58.030334] Iteration 60900, train loss = 0.250082, train accuracy = 0.937500\n",
      "[2018-07-16 13:17:01.690559] Iteration 61000, train loss = 0.305476, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.880800\n",
      "[2018-07-16 13:17:06.477991] Iteration 61100, train loss = 0.273568, train accuracy = 0.929688\n",
      "[2018-07-16 13:17:10.122773] Iteration 61200, train loss = 0.319961, train accuracy = 0.921875\n",
      "[2018-07-16 13:17:13.763577] Iteration 61300, train loss = 0.302112, train accuracy = 0.945312\n",
      "[2018-07-16 13:17:17.414245] Iteration 61400, train loss = 0.335441, train accuracy = 0.929688\n",
      "[2018-07-16 13:17:21.067667] Iteration 61500, train loss = 0.302348, train accuracy = 0.921875\n",
      "[2018-07-16 13:17:24.715792] Iteration 61600, train loss = 0.317542, train accuracy = 0.921875\n",
      "[2018-07-16 13:17:28.364422] Iteration 61700, train loss = 0.293954, train accuracy = 0.890625\n",
      "[2018-07-16 13:17:32.095796] Iteration 61800, train loss = 0.241141, train accuracy = 0.937500\n",
      "[2018-07-16 13:17:35.751671] Iteration 61900, train loss = 0.227465, train accuracy = 0.953125\n",
      "[2018-07-16 13:17:39.407027] Iteration 62000, train loss = 0.227885, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.878800\n",
      "[2018-07-16 13:17:44.211326] Iteration 62100, train loss = 0.315847, train accuracy = 0.914062\n",
      "[2018-07-16 13:17:47.863973] Iteration 62200, train loss = 0.290118, train accuracy = 0.929688\n",
      "[2018-07-16 13:17:51.509129] Iteration 62300, train loss = 0.241056, train accuracy = 0.960938\n",
      "[2018-07-16 13:17:55.149159] Iteration 62400, train loss = 0.239817, train accuracy = 0.953125\n",
      "[2018-07-16 13:17:58.798531] Iteration 62500, train loss = 0.340679, train accuracy = 0.914062\n",
      "[2018-07-16 13:18:02.430874] Iteration 62600, train loss = 0.271959, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:06.057399] Iteration 62700, train loss = 0.260814, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:09.688720] Iteration 62800, train loss = 0.270211, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:13.406051] Iteration 62900, train loss = 0.205648, train accuracy = 0.960938\n",
      "[2018-07-16 13:18:17.059136] Iteration 63000, train loss = 0.197544, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.881000\n",
      "[2018-07-16 13:18:21.859919] Iteration 63100, train loss = 0.264934, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:25.525837] Iteration 63200, train loss = 0.238563, train accuracy = 0.968750\n",
      "[2018-07-16 13:18:29.182647] Iteration 63300, train loss = 0.397105, train accuracy = 0.890625\n",
      "[2018-07-16 13:18:32.824903] Iteration 63400, train loss = 0.278896, train accuracy = 0.914062\n",
      "[2018-07-16 13:18:36.473942] Iteration 63500, train loss = 0.338133, train accuracy = 0.921875\n",
      "[2018-07-16 13:18:40.115411] Iteration 63600, train loss = 0.299891, train accuracy = 0.921875\n",
      "[2018-07-16 13:18:43.765059] Iteration 63700, train loss = 0.264704, train accuracy = 0.953125\n",
      "[2018-07-16 13:18:47.408674] Iteration 63800, train loss = 0.266134, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:51.046525] Iteration 63900, train loss = 0.238068, train accuracy = 0.937500\n",
      "[2018-07-16 13:18:54.764586] Iteration 64000, train loss = 0.249868, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.880300\n",
      "[2018-07-16 13:18:59.533910] Iteration 64100, train loss = 0.296672, train accuracy = 0.921875\n",
      "[2018-07-16 13:19:03.173054] Iteration 64200, train loss = 0.282047, train accuracy = 0.937500\n",
      "[2018-07-16 13:19:06.840100] Iteration 64300, train loss = 0.253130, train accuracy = 0.945312\n",
      "[2018-07-16 13:19:10.485662] Iteration 64400, train loss = 0.201786, train accuracy = 0.976562\n",
      "[2018-07-16 13:19:14.127753] Iteration 64500, train loss = 0.228974, train accuracy = 0.937500\n",
      "[2018-07-16 13:19:17.771226] Iteration 64600, train loss = 0.382790, train accuracy = 0.898438\n",
      "[2018-07-16 13:19:21.408137] Iteration 64700, train loss = 0.242804, train accuracy = 0.953125\n",
      "[2018-07-16 13:19:25.058807] Iteration 64800, train loss = 0.356039, train accuracy = 0.921875\n",
      "[2018-07-16 13:19:28.714326] Iteration 64900, train loss = 0.335158, train accuracy = 0.914062\n",
      "[2018-07-16 13:19:32.361698] Iteration 65000, train loss = 0.193860, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.881700\n",
      "[2018-07-16 13:19:37.174480] Iteration 65100, train loss = 0.331336, train accuracy = 0.937500\n",
      "[2018-07-16 13:19:40.824033] Iteration 65200, train loss = 0.309438, train accuracy = 0.945312\n",
      "[2018-07-16 13:19:44.455489] Iteration 65300, train loss = 0.313746, train accuracy = 0.921875\n",
      "[2018-07-16 13:19:48.086254] Iteration 65400, train loss = 0.266823, train accuracy = 0.945312\n",
      "[2018-07-16 13:19:51.733346] Iteration 65500, train loss = 0.189098, train accuracy = 0.960938\n",
      "[2018-07-16 13:19:55.372557] Iteration 65600, train loss = 0.253669, train accuracy = 0.945312\n",
      "[2018-07-16 13:19:59.030712] Iteration 65700, train loss = 0.308345, train accuracy = 0.929688\n",
      "[2018-07-16 13:20:02.673329] Iteration 65800, train loss = 0.299282, train accuracy = 0.914062\n",
      "[2018-07-16 13:20:06.304843] Iteration 65900, train loss = 0.220464, train accuracy = 0.953125\n",
      "[2018-07-16 13:20:09.962604] Iteration 66000, train loss = 0.233100, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.879800\n",
      "[2018-07-16 13:20:14.760254] Iteration 66100, train loss = 0.189836, train accuracy = 0.960938\n",
      "[2018-07-16 13:20:18.466627] Iteration 66200, train loss = 0.233063, train accuracy = 0.937500\n",
      "[2018-07-16 13:20:22.094378] Iteration 66300, train loss = 0.205526, train accuracy = 0.976562\n",
      "[2018-07-16 13:20:25.733356] Iteration 66400, train loss = 0.263361, train accuracy = 0.945312\n",
      "[2018-07-16 13:20:29.374747] Iteration 66500, train loss = 0.268471, train accuracy = 0.953125\n",
      "[2018-07-16 13:20:33.032002] Iteration 66600, train loss = 0.267321, train accuracy = 0.945312\n",
      "[2018-07-16 13:20:36.666308] Iteration 66700, train loss = 0.203287, train accuracy = 0.953125\n",
      "[2018-07-16 13:20:40.340441] Iteration 66800, train loss = 0.234640, train accuracy = 0.937500\n",
      "[2018-07-16 13:20:43.982444] Iteration 66900, train loss = 0.333893, train accuracy = 0.906250\n",
      "[2018-07-16 13:20:47.617230] Iteration 67000, train loss = 0.369349, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.879600\n",
      "[2018-07-16 13:20:52.363334] Iteration 67100, train loss = 0.247492, train accuracy = 0.937500\n",
      "[2018-07-16 13:20:56.013165] Iteration 67200, train loss = 0.245975, train accuracy = 0.953125\n",
      "[2018-07-16 13:20:59.697317] Iteration 67300, train loss = 0.231800, train accuracy = 0.953125\n",
      "[2018-07-16 13:21:03.387954] Iteration 67400, train loss = 0.257675, train accuracy = 0.937500\n",
      "[2018-07-16 13:21:07.029344] Iteration 67500, train loss = 0.236561, train accuracy = 0.953125\n",
      "[2018-07-16 13:21:10.671271] Iteration 67600, train loss = 0.218544, train accuracy = 0.960938\n",
      "[2018-07-16 13:21:14.305346] Iteration 67700, train loss = 0.231485, train accuracy = 0.953125\n",
      "[2018-07-16 13:21:17.944672] Iteration 67800, train loss = 0.238123, train accuracy = 0.945312\n",
      "[2018-07-16 13:21:21.587874] Iteration 67900, train loss = 0.314646, train accuracy = 0.937500\n",
      "[2018-07-16 13:21:25.215038] Iteration 68000, train loss = 0.273124, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.880200\n",
      "[2018-07-16 13:21:30.015912] Iteration 68100, train loss = 0.169688, train accuracy = 0.992188\n",
      "[2018-07-16 13:21:33.644642] Iteration 68200, train loss = 0.288395, train accuracy = 0.929688\n",
      "[2018-07-16 13:21:37.288152] Iteration 68300, train loss = 0.290227, train accuracy = 0.921875\n",
      "[2018-07-16 13:21:40.948078] Iteration 68400, train loss = 0.359529, train accuracy = 0.898438\n",
      "[2018-07-16 13:21:44.644884] Iteration 68500, train loss = 0.217465, train accuracy = 0.953125\n",
      "[2018-07-16 13:21:48.290335] Iteration 68600, train loss = 0.336866, train accuracy = 0.898438\n",
      "[2018-07-16 13:21:51.948632] Iteration 68700, train loss = 0.282969, train accuracy = 0.929688\n",
      "[2018-07-16 13:21:55.602627] Iteration 68800, train loss = 0.306039, train accuracy = 0.906250\n",
      "[2018-07-16 13:21:59.251264] Iteration 68900, train loss = 0.355817, train accuracy = 0.882812\n",
      "[2018-07-16 13:22:02.895705] Iteration 69000, train loss = 0.306130, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.878700\n",
      "[2018-07-16 13:22:07.639396] Iteration 69100, train loss = 0.315215, train accuracy = 0.929688\n",
      "[2018-07-16 13:22:11.293428] Iteration 69200, train loss = 0.374710, train accuracy = 0.898438\n",
      "[2018-07-16 13:22:14.942516] Iteration 69300, train loss = 0.211471, train accuracy = 0.960938\n",
      "[2018-07-16 13:22:18.602766] Iteration 69400, train loss = 0.298858, train accuracy = 0.898438\n",
      "[2018-07-16 13:22:22.240756] Iteration 69500, train loss = 0.216822, train accuracy = 0.953125\n",
      "[2018-07-16 13:22:25.958829] Iteration 69600, train loss = 0.311569, train accuracy = 0.929688\n",
      "[2018-07-16 13:22:29.604118] Iteration 69700, train loss = 0.208628, train accuracy = 0.953125\n",
      "[2018-07-16 13:22:33.241735] Iteration 69800, train loss = 0.273753, train accuracy = 0.929688\n",
      "[2018-07-16 13:22:36.900484] Iteration 69900, train loss = 0.240657, train accuracy = 0.945312\n",
      "[2018-07-16 13:22:40.548978] Iteration 70000, train loss = 0.198542, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.880000\n",
      "[2018-07-16 13:22:45.328464] Iteration 70100, train loss = 0.252039, train accuracy = 0.937500\n",
      "[2018-07-16 13:22:48.978400] Iteration 70200, train loss = 0.350077, train accuracy = 0.914062\n",
      "[2018-07-16 13:22:52.615662] Iteration 70300, train loss = 0.310021, train accuracy = 0.914062\n",
      "[2018-07-16 13:22:56.257152] Iteration 70400, train loss = 0.294858, train accuracy = 0.953125\n",
      "[2018-07-16 13:22:59.899242] Iteration 70500, train loss = 0.322717, train accuracy = 0.898438\n",
      "[2018-07-16 13:23:03.541931] Iteration 70600, train loss = 0.230788, train accuracy = 0.945312\n",
      "[2018-07-16 13:23:07.252012] Iteration 70700, train loss = 0.288784, train accuracy = 0.937500\n",
      "[2018-07-16 13:23:10.903035] Iteration 70800, train loss = 0.263902, train accuracy = 0.945312\n",
      "[2018-07-16 13:23:14.560929] Iteration 70900, train loss = 0.209846, train accuracy = 0.968750\n",
      "[2018-07-16 13:23:18.249118] Iteration 71000, train loss = 0.257405, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.877600\n",
      "[2018-07-16 13:23:23.001273] Iteration 71100, train loss = 0.244659, train accuracy = 0.945312\n",
      "[2018-07-16 13:23:26.654058] Iteration 71200, train loss = 0.301618, train accuracy = 0.921875\n",
      "[2018-07-16 13:23:30.304275] Iteration 71300, train loss = 0.308666, train accuracy = 0.914062\n",
      "[2018-07-16 13:23:33.946611] Iteration 71400, train loss = 0.294071, train accuracy = 0.898438\n",
      "[2018-07-16 13:23:37.592631] Iteration 71500, train loss = 0.200117, train accuracy = 0.968750\n",
      "[2018-07-16 13:23:41.238371] Iteration 71600, train loss = 0.223430, train accuracy = 0.968750\n",
      "[2018-07-16 13:23:44.878916] Iteration 71700, train loss = 0.225388, train accuracy = 0.945312\n",
      "[2018-07-16 13:23:48.588830] Iteration 71800, train loss = 0.307855, train accuracy = 0.906250\n",
      "[2018-07-16 13:23:52.235081] Iteration 71900, train loss = 0.274188, train accuracy = 0.921875\n",
      "[2018-07-16 13:23:55.889025] Iteration 72000, train loss = 0.277062, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.877500\n",
      "[2018-07-16 13:24:00.665253] Iteration 72100, train loss = 0.265094, train accuracy = 0.914062\n",
      "[2018-07-16 13:24:04.319020] Iteration 72200, train loss = 0.261981, train accuracy = 0.921875\n",
      "[2018-07-16 13:24:07.982799] Iteration 72300, train loss = 0.246606, train accuracy = 0.929688\n",
      "[2018-07-16 13:24:11.621041] Iteration 72400, train loss = 0.247544, train accuracy = 0.960938\n",
      "[2018-07-16 13:24:15.268767] Iteration 72500, train loss = 0.296874, train accuracy = 0.929688\n",
      "[2018-07-16 13:24:18.907534] Iteration 72600, train loss = 0.263587, train accuracy = 0.937500\n",
      "[2018-07-16 13:24:22.540688] Iteration 72700, train loss = 0.248168, train accuracy = 0.960938\n",
      "[2018-07-16 13:24:26.192711] Iteration 72800, train loss = 0.331370, train accuracy = 0.875000\n",
      "[2018-07-16 13:24:29.887603] Iteration 72900, train loss = 0.231693, train accuracy = 0.953125\n",
      "[2018-07-16 13:24:33.517202] Iteration 73000, train loss = 0.206518, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.877900\n",
      "[2018-07-16 13:24:38.320918] Iteration 73100, train loss = 0.297546, train accuracy = 0.906250\n",
      "[2018-07-16 13:24:41.959187] Iteration 73200, train loss = 0.282723, train accuracy = 0.953125\n",
      "[2018-07-16 13:24:45.618951] Iteration 73300, train loss = 0.212484, train accuracy = 0.968750\n",
      "[2018-07-16 13:24:49.271334] Iteration 73400, train loss = 0.207069, train accuracy = 0.945312\n",
      "[2018-07-16 13:24:52.919300] Iteration 73500, train loss = 0.267753, train accuracy = 0.945312\n",
      "[2018-07-16 13:24:56.558706] Iteration 73600, train loss = 0.303071, train accuracy = 0.945312\n",
      "[2018-07-16 13:25:00.201349] Iteration 73700, train loss = 0.274722, train accuracy = 0.929688\n",
      "[2018-07-16 13:25:03.857110] Iteration 73800, train loss = 0.208027, train accuracy = 0.937500\n",
      "[2018-07-16 13:25:07.519601] Iteration 73900, train loss = 0.245731, train accuracy = 0.929688\n",
      "[2018-07-16 13:25:11.173114] Iteration 74000, train loss = 0.238033, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.878600\n",
      "[2018-07-16 13:25:16.030171] Iteration 74100, train loss = 0.271999, train accuracy = 0.921875\n",
      "[2018-07-16 13:25:19.663181] Iteration 74200, train loss = 0.297736, train accuracy = 0.937500\n",
      "[2018-07-16 13:25:23.297454] Iteration 74300, train loss = 0.190130, train accuracy = 0.960938\n",
      "[2018-07-16 13:25:26.943794] Iteration 74400, train loss = 0.248700, train accuracy = 0.914062\n",
      "[2018-07-16 13:25:30.596607] Iteration 74500, train loss = 0.243304, train accuracy = 0.945312\n",
      "[2018-07-16 13:25:34.238377] Iteration 74600, train loss = 0.216469, train accuracy = 0.953125\n",
      "[2018-07-16 13:25:37.875599] Iteration 74700, train loss = 0.343075, train accuracy = 0.929688\n",
      "[2018-07-16 13:25:41.521189] Iteration 74800, train loss = 0.230930, train accuracy = 0.945312\n",
      "[2018-07-16 13:25:45.186390] Iteration 74900, train loss = 0.209489, train accuracy = 0.976562\n",
      "[2018-07-16 13:25:48.851308] Iteration 75000, train loss = 0.289741, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.878900\n",
      "[2018-07-16 13:25:53.688208] Iteration 75100, train loss = 0.244791, train accuracy = 0.945312\n",
      "[2018-07-16 13:25:57.347843] Iteration 75200, train loss = 0.328149, train accuracy = 0.898438\n",
      "[2018-07-16 13:26:00.997251] Iteration 75300, train loss = 0.220881, train accuracy = 0.937500\n",
      "[2018-07-16 13:26:04.642111] Iteration 75400, train loss = 0.272631, train accuracy = 0.937500\n",
      "[2018-07-16 13:26:08.287597] Iteration 75500, train loss = 0.226547, train accuracy = 0.937500\n",
      "[2018-07-16 13:26:11.939387] Iteration 75600, train loss = 0.197271, train accuracy = 0.976562\n",
      "[2018-07-16 13:26:15.597567] Iteration 75700, train loss = 0.240542, train accuracy = 0.945312\n",
      "[2018-07-16 13:26:19.242443] Iteration 75800, train loss = 0.188693, train accuracy = 0.968750\n",
      "[2018-07-16 13:26:22.896231] Iteration 75900, train loss = 0.244404, train accuracy = 0.953125\n",
      "[2018-07-16 13:26:26.543467] Iteration 76000, train loss = 0.323382, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.879400\n",
      "[2018-07-16 13:26:31.300569] Iteration 76100, train loss = 0.266450, train accuracy = 0.937500\n",
      "[2018-07-16 13:26:34.950028] Iteration 76200, train loss = 0.246151, train accuracy = 0.929688\n",
      "[2018-07-16 13:26:38.648166] Iteration 76300, train loss = 0.255415, train accuracy = 0.929688\n",
      "[2018-07-16 13:26:42.310157] Iteration 76400, train loss = 0.232188, train accuracy = 0.953125\n",
      "[2018-07-16 13:26:45.979085] Iteration 76500, train loss = 0.323980, train accuracy = 0.882812\n",
      "[2018-07-16 13:26:49.615716] Iteration 76600, train loss = 0.272102, train accuracy = 0.953125\n",
      "[2018-07-16 13:26:53.265405] Iteration 76700, train loss = 0.446306, train accuracy = 0.859375\n",
      "[2018-07-16 13:26:56.906839] Iteration 76800, train loss = 0.239415, train accuracy = 0.945312\n",
      "[2018-07-16 13:27:00.539492] Iteration 76900, train loss = 0.271603, train accuracy = 0.937500\n",
      "[2018-07-16 13:27:04.188770] Iteration 77000, train loss = 0.264064, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.880700\n",
      "[2018-07-16 13:27:08.989865] Iteration 77100, train loss = 0.209241, train accuracy = 0.976562\n",
      "[2018-07-16 13:27:12.642657] Iteration 77200, train loss = 0.274306, train accuracy = 0.929688\n",
      "[2018-07-16 13:27:16.299889] Iteration 77300, train loss = 0.206521, train accuracy = 0.968750\n",
      "[2018-07-16 13:27:20.031625] Iteration 77400, train loss = 0.229836, train accuracy = 0.945312\n",
      "[2018-07-16 13:27:23.713718] Iteration 77500, train loss = 0.235291, train accuracy = 0.953125\n",
      "[2018-07-16 13:27:27.344473] Iteration 77600, train loss = 0.304402, train accuracy = 0.921875\n",
      "[2018-07-16 13:27:30.999301] Iteration 77700, train loss = 0.279693, train accuracy = 0.929688\n",
      "[2018-07-16 13:27:34.656531] Iteration 77800, train loss = 0.306918, train accuracy = 0.921875\n",
      "[2018-07-16 13:27:38.302967] Iteration 77900, train loss = 0.279006, train accuracy = 0.921875\n",
      "[2018-07-16 13:27:41.943511] Iteration 78000, train loss = 0.259511, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.878600\n",
      "[2018-07-16 13:27:46.713651] Iteration 78100, train loss = 0.265361, train accuracy = 0.945312\n",
      "[2018-07-16 13:27:50.360600] Iteration 78200, train loss = 0.280234, train accuracy = 0.914062\n",
      "[2018-07-16 13:27:54.002530] Iteration 78300, train loss = 0.264387, train accuracy = 0.937500\n",
      "[2018-07-16 13:27:57.644467] Iteration 78400, train loss = 0.292264, train accuracy = 0.921875\n",
      "[2018-07-16 13:28:01.350017] Iteration 78500, train loss = 0.242298, train accuracy = 0.937500\n",
      "[2018-07-16 13:28:05.015454] Iteration 78600, train loss = 0.262322, train accuracy = 0.937500\n",
      "[2018-07-16 13:28:08.662910] Iteration 78700, train loss = 0.221878, train accuracy = 0.945312\n",
      "[2018-07-16 13:28:12.294719] Iteration 78800, train loss = 0.273136, train accuracy = 0.937500\n",
      "[2018-07-16 13:28:15.947127] Iteration 78900, train loss = 0.366786, train accuracy = 0.882812\n",
      "[2018-07-16 13:28:19.594337] Iteration 79000, train loss = 0.309920, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.878600\n",
      "[2018-07-16 13:28:24.371168] Iteration 79100, train loss = 0.231168, train accuracy = 0.937500\n",
      "[2018-07-16 13:28:28.008048] Iteration 79200, train loss = 0.333892, train accuracy = 0.921875\n",
      "[2018-07-16 13:28:31.655270] Iteration 79300, train loss = 0.228997, train accuracy = 0.976562\n",
      "[2018-07-16 13:28:35.298958] Iteration 79400, train loss = 0.207547, train accuracy = 0.945312\n",
      "[2018-07-16 13:28:38.949042] Iteration 79500, train loss = 0.212536, train accuracy = 0.984375\n",
      "[2018-07-16 13:28:42.642656] Iteration 79600, train loss = 0.313214, train accuracy = 0.929688\n",
      "[2018-07-16 13:28:46.293191] Iteration 79700, train loss = 0.293312, train accuracy = 0.921875\n",
      "[2018-07-16 13:28:49.949795] Iteration 79800, train loss = 0.242010, train accuracy = 0.960938\n",
      "[2018-07-16 13:28:53.615606] Iteration 79900, train loss = 0.285731, train accuracy = 0.945312\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三轮  量化\n",
    "prune_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.878700\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.13804926  0.125       0.          0.\n",
      "  0.125      -0.21148889 -0.07291692 -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.85)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 13:29:33.616586] Iteration 100, train loss = 1.576172, train accuracy = 0.476562\n",
      "[2018-07-16 13:29:37.264876] Iteration 200, train loss = 1.348989, train accuracy = 0.531250\n",
      "[2018-07-16 13:29:40.917643] Iteration 300, train loss = 1.144901, train accuracy = 0.640625\n",
      "[2018-07-16 13:29:44.554743] Iteration 400, train loss = 1.145012, train accuracy = 0.601562\n",
      "[2018-07-16 13:29:48.198293] Iteration 500, train loss = 1.032695, train accuracy = 0.656250\n",
      "[2018-07-16 13:29:51.836661] Iteration 600, train loss = 1.076178, train accuracy = 0.593750\n",
      "[2018-07-16 13:29:55.473342] Iteration 700, train loss = 1.087583, train accuracy = 0.656250\n",
      "[2018-07-16 13:29:59.116019] Iteration 800, train loss = 0.867399, train accuracy = 0.726562\n",
      "[2018-07-16 13:30:02.738971] Iteration 900, train loss = 0.980832, train accuracy = 0.695312\n",
      "[2018-07-16 13:30:06.430871] Iteration 1000, train loss = 0.817248, train accuracy = 0.726562\n",
      "Evaluating...\n",
      "Test accuracy = 0.651000\n",
      "[2018-07-16 13:30:11.176262] Iteration 1100, train loss = 0.938671, train accuracy = 0.679688\n",
      "[2018-07-16 13:30:14.821469] Iteration 1200, train loss = 0.940964, train accuracy = 0.687500\n",
      "[2018-07-16 13:30:18.467063] Iteration 1300, train loss = 0.932145, train accuracy = 0.718750\n",
      "[2018-07-16 13:30:22.113533] Iteration 1400, train loss = 0.756204, train accuracy = 0.757812\n",
      "[2018-07-16 13:30:25.756692] Iteration 1500, train loss = 0.777842, train accuracy = 0.757812\n",
      "[2018-07-16 13:30:29.405361] Iteration 1600, train loss = 0.877023, train accuracy = 0.718750\n",
      "[2018-07-16 13:30:33.049260] Iteration 1700, train loss = 1.002397, train accuracy = 0.695312\n",
      "[2018-07-16 13:30:36.690448] Iteration 1800, train loss = 0.627712, train accuracy = 0.828125\n",
      "[2018-07-16 13:30:40.318654] Iteration 1900, train loss = 0.718918, train accuracy = 0.750000\n",
      "[2018-07-16 13:30:43.972093] Iteration 2000, train loss = 0.653003, train accuracy = 0.773438\n",
      "Evaluating...\n",
      "Test accuracy = 0.735000\n",
      "[2018-07-16 13:30:48.792949] Iteration 2100, train loss = 0.864112, train accuracy = 0.757812\n",
      "[2018-07-16 13:30:52.435198] Iteration 2200, train loss = 0.688192, train accuracy = 0.765625\n",
      "[2018-07-16 13:30:56.060281] Iteration 2300, train loss = 0.752580, train accuracy = 0.773438\n",
      "[2018-07-16 13:30:59.689688] Iteration 2400, train loss = 0.728828, train accuracy = 0.742188\n",
      "[2018-07-16 13:31:03.333885] Iteration 2500, train loss = 0.801940, train accuracy = 0.773438\n",
      "[2018-07-16 13:31:06.969273] Iteration 2600, train loss = 0.675995, train accuracy = 0.804688\n",
      "[2018-07-16 13:31:10.615152] Iteration 2700, train loss = 0.737979, train accuracy = 0.765625\n",
      "[2018-07-16 13:31:14.246805] Iteration 2800, train loss = 0.777562, train accuracy = 0.773438\n",
      "[2018-07-16 13:31:17.891038] Iteration 2900, train loss = 0.683770, train accuracy = 0.796875\n",
      "[2018-07-16 13:31:21.527205] Iteration 3000, train loss = 0.820650, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.739500\n",
      "[2018-07-16 13:31:26.317063] Iteration 3100, train loss = 0.699179, train accuracy = 0.796875\n",
      "[2018-07-16 13:31:30.041167] Iteration 3200, train loss = 0.811226, train accuracy = 0.726562\n",
      "[2018-07-16 13:31:33.681681] Iteration 3300, train loss = 0.659021, train accuracy = 0.804688\n",
      "[2018-07-16 13:31:37.316878] Iteration 3400, train loss = 0.676893, train accuracy = 0.773438\n",
      "[2018-07-16 13:31:40.940459] Iteration 3500, train loss = 0.568019, train accuracy = 0.851562\n",
      "[2018-07-16 13:31:44.570232] Iteration 3600, train loss = 0.682101, train accuracy = 0.757812\n",
      "[2018-07-16 13:31:48.208111] Iteration 3700, train loss = 0.531624, train accuracy = 0.820312\n",
      "[2018-07-16 13:31:51.865379] Iteration 3800, train loss = 0.662323, train accuracy = 0.828125\n",
      "[2018-07-16 13:31:55.530791] Iteration 3900, train loss = 0.627123, train accuracy = 0.781250\n",
      "[2018-07-16 13:31:59.180324] Iteration 4000, train loss = 0.497486, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.763900\n",
      "[2018-07-16 13:32:03.938411] Iteration 4100, train loss = 0.637656, train accuracy = 0.820312\n",
      "[2018-07-16 13:32:07.571569] Iteration 4200, train loss = 0.609813, train accuracy = 0.828125\n",
      "[2018-07-16 13:32:11.245243] Iteration 4300, train loss = 0.662006, train accuracy = 0.820312\n",
      "[2018-07-16 13:32:14.948866] Iteration 4400, train loss = 0.616974, train accuracy = 0.812500\n",
      "[2018-07-16 13:32:18.585145] Iteration 4500, train loss = 0.635356, train accuracy = 0.828125\n",
      "[2018-07-16 13:32:22.235968] Iteration 4600, train loss = 0.614151, train accuracy = 0.773438\n",
      "[2018-07-16 13:32:25.881970] Iteration 4700, train loss = 0.636325, train accuracy = 0.796875\n",
      "[2018-07-16 13:32:29.527405] Iteration 4800, train loss = 0.472998, train accuracy = 0.875000\n",
      "[2018-07-16 13:32:33.163243] Iteration 4900, train loss = 0.565206, train accuracy = 0.796875\n",
      "[2018-07-16 13:32:36.818407] Iteration 5000, train loss = 0.486168, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.764000\n",
      "[2018-07-16 13:32:41.595245] Iteration 5100, train loss = 0.625411, train accuracy = 0.820312\n",
      "[2018-07-16 13:32:45.242161] Iteration 5200, train loss = 0.682066, train accuracy = 0.812500\n",
      "[2018-07-16 13:32:48.877728] Iteration 5300, train loss = 0.691921, train accuracy = 0.804688\n",
      "[2018-07-16 13:32:52.537773] Iteration 5400, train loss = 0.655131, train accuracy = 0.820312\n",
      "[2018-07-16 13:32:56.251439] Iteration 5500, train loss = 0.690083, train accuracy = 0.804688\n",
      "[2018-07-16 13:32:59.891222] Iteration 5600, train loss = 0.580152, train accuracy = 0.820312\n",
      "[2018-07-16 13:33:03.527922] Iteration 5700, train loss = 0.539958, train accuracy = 0.890625\n",
      "[2018-07-16 13:33:07.162604] Iteration 5800, train loss = 0.532910, train accuracy = 0.843750\n",
      "[2018-07-16 13:33:10.803273] Iteration 5900, train loss = 0.527628, train accuracy = 0.828125\n",
      "[2018-07-16 13:33:14.435576] Iteration 6000, train loss = 0.571785, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.767800\n",
      "[2018-07-16 13:33:19.185501] Iteration 6100, train loss = 0.615698, train accuracy = 0.820312\n",
      "[2018-07-16 13:33:22.816520] Iteration 6200, train loss = 0.571736, train accuracy = 0.859375\n",
      "[2018-07-16 13:33:26.468213] Iteration 6300, train loss = 0.482660, train accuracy = 0.859375\n",
      "[2018-07-16 13:33:30.116261] Iteration 6400, train loss = 0.575438, train accuracy = 0.843750\n",
      "[2018-07-16 13:33:33.759404] Iteration 6500, train loss = 0.416149, train accuracy = 0.906250\n",
      "[2018-07-16 13:33:37.494755] Iteration 6600, train loss = 0.683457, train accuracy = 0.773438\n",
      "[2018-07-16 13:33:41.157348] Iteration 6700, train loss = 0.551550, train accuracy = 0.843750\n",
      "[2018-07-16 13:33:44.809367] Iteration 6800, train loss = 0.553504, train accuracy = 0.812500\n",
      "[2018-07-16 13:33:48.467088] Iteration 6900, train loss = 0.696128, train accuracy = 0.804688\n",
      "[2018-07-16 13:33:52.107897] Iteration 7000, train loss = 0.550676, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.764900\n",
      "[2018-07-16 13:33:56.878749] Iteration 7100, train loss = 0.619992, train accuracy = 0.804688\n",
      "[2018-07-16 13:34:00.527716] Iteration 7200, train loss = 0.522123, train accuracy = 0.843750\n",
      "[2018-07-16 13:34:04.165724] Iteration 7300, train loss = 0.563554, train accuracy = 0.820312\n",
      "[2018-07-16 13:34:07.806996] Iteration 7400, train loss = 0.550029, train accuracy = 0.820312\n",
      "[2018-07-16 13:34:11.449533] Iteration 7500, train loss = 0.678895, train accuracy = 0.773438\n",
      "[2018-07-16 13:34:15.095334] Iteration 7600, train loss = 0.548580, train accuracy = 0.843750\n",
      "[2018-07-16 13:34:18.807559] Iteration 7700, train loss = 0.698249, train accuracy = 0.789062\n",
      "[2018-07-16 13:34:22.451068] Iteration 7800, train loss = 0.502786, train accuracy = 0.828125\n",
      "[2018-07-16 13:34:26.119028] Iteration 7900, train loss = 0.548128, train accuracy = 0.828125\n",
      "[2018-07-16 13:34:29.783657] Iteration 8000, train loss = 0.615151, train accuracy = 0.812500\n",
      "Evaluating...\n",
      "Test accuracy = 0.781100\n",
      "[2018-07-16 13:34:34.539300] Iteration 8100, train loss = 0.573066, train accuracy = 0.843750\n",
      "[2018-07-16 13:34:38.183361] Iteration 8200, train loss = 0.526488, train accuracy = 0.843750\n",
      "[2018-07-16 13:34:41.835828] Iteration 8300, train loss = 0.657363, train accuracy = 0.796875\n",
      "[2018-07-16 13:34:45.489648] Iteration 8400, train loss = 0.492693, train accuracy = 0.882812\n",
      "[2018-07-16 13:34:49.136667] Iteration 8500, train loss = 0.656782, train accuracy = 0.812500\n",
      "[2018-07-16 13:34:52.783917] Iteration 8600, train loss = 0.470489, train accuracy = 0.867188\n",
      "[2018-07-16 13:34:56.421908] Iteration 8700, train loss = 0.582210, train accuracy = 0.789062\n",
      "[2018-07-16 13:35:00.125496] Iteration 8800, train loss = 0.577891, train accuracy = 0.820312\n",
      "[2018-07-16 13:35:03.801042] Iteration 8900, train loss = 0.687288, train accuracy = 0.781250\n",
      "[2018-07-16 13:35:07.463760] Iteration 9000, train loss = 0.665786, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.785300\n",
      "[2018-07-16 13:35:12.244865] Iteration 9100, train loss = 0.546101, train accuracy = 0.851562\n",
      "[2018-07-16 13:35:15.879222] Iteration 9200, train loss = 0.468417, train accuracy = 0.843750\n",
      "[2018-07-16 13:35:19.535092] Iteration 9300, train loss = 0.526252, train accuracy = 0.843750\n",
      "[2018-07-16 13:35:23.195461] Iteration 9400, train loss = 0.473641, train accuracy = 0.898438\n",
      "[2018-07-16 13:35:26.837529] Iteration 9500, train loss = 0.535864, train accuracy = 0.835938\n",
      "[2018-07-16 13:35:30.465668] Iteration 9600, train loss = 0.703363, train accuracy = 0.765625\n",
      "[2018-07-16 13:35:34.096291] Iteration 9700, train loss = 0.601619, train accuracy = 0.812500\n",
      "[2018-07-16 13:35:37.752896] Iteration 9800, train loss = 0.660144, train accuracy = 0.781250\n",
      "[2018-07-16 13:35:41.404561] Iteration 9900, train loss = 0.547980, train accuracy = 0.843750\n",
      "[2018-07-16 13:35:45.075131] Iteration 10000, train loss = 0.611041, train accuracy = 0.812500\n",
      "Evaluating...\n",
      "Test accuracy = 0.787600\n",
      "[2018-07-16 13:35:49.757880] Iteration 10100, train loss = 0.581208, train accuracy = 0.828125\n",
      "[2018-07-16 13:35:53.378413] Iteration 10200, train loss = 0.746573, train accuracy = 0.726562\n",
      "[2018-07-16 13:35:56.974028] Iteration 10300, train loss = 0.596235, train accuracy = 0.812500\n",
      "[2018-07-16 13:36:00.582969] Iteration 10400, train loss = 0.446199, train accuracy = 0.867188\n",
      "[2018-07-16 13:36:04.197382] Iteration 10500, train loss = 0.502513, train accuracy = 0.843750\n",
      "[2018-07-16 13:36:07.819001] Iteration 10600, train loss = 0.622649, train accuracy = 0.820312\n",
      "[2018-07-16 13:36:11.429478] Iteration 10700, train loss = 0.593100, train accuracy = 0.828125\n",
      "[2018-07-16 13:36:15.045166] Iteration 10800, train loss = 0.627834, train accuracy = 0.796875\n",
      "[2018-07-16 13:36:18.689026] Iteration 10900, train loss = 0.505484, train accuracy = 0.867188\n",
      "[2018-07-16 13:36:22.340051] Iteration 11000, train loss = 0.548650, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.795400\n",
      "[2018-07-16 13:36:27.119508] Iteration 11100, train loss = 0.591791, train accuracy = 0.835938\n",
      "[2018-07-16 13:36:30.760600] Iteration 11200, train loss = 0.558631, train accuracy = 0.835938\n",
      "[2018-07-16 13:36:34.422851] Iteration 11300, train loss = 0.530898, train accuracy = 0.820312\n",
      "[2018-07-16 13:36:38.097973] Iteration 11400, train loss = 0.521261, train accuracy = 0.851562\n",
      "[2018-07-16 13:36:41.755911] Iteration 11500, train loss = 0.559996, train accuracy = 0.843750\n",
      "[2018-07-16 13:36:45.406642] Iteration 11600, train loss = 0.461482, train accuracy = 0.882812\n",
      "[2018-07-16 13:36:49.070339] Iteration 11700, train loss = 0.559144, train accuracy = 0.828125\n",
      "[2018-07-16 13:36:52.747106] Iteration 11800, train loss = 0.590714, train accuracy = 0.820312\n",
      "[2018-07-16 13:36:56.494871] Iteration 11900, train loss = 0.588696, train accuracy = 0.828125\n",
      "[2018-07-16 13:37:00.141838] Iteration 12000, train loss = 0.400986, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.812700\n",
      "[2018-07-16 13:37:04.942781] Iteration 12100, train loss = 0.589889, train accuracy = 0.828125\n",
      "[2018-07-16 13:37:08.605250] Iteration 12200, train loss = 0.465761, train accuracy = 0.875000\n",
      "[2018-07-16 13:37:12.233353] Iteration 12300, train loss = 0.501163, train accuracy = 0.828125\n",
      "[2018-07-16 13:37:15.871848] Iteration 12400, train loss = 0.599338, train accuracy = 0.835938\n",
      "[2018-07-16 13:37:19.511858] Iteration 12500, train loss = 0.581051, train accuracy = 0.820312\n",
      "[2018-07-16 13:37:23.146143] Iteration 12600, train loss = 0.508809, train accuracy = 0.851562\n",
      "[2018-07-16 13:37:26.799608] Iteration 12700, train loss = 0.530511, train accuracy = 0.843750\n",
      "[2018-07-16 13:37:30.473365] Iteration 12800, train loss = 0.523090, train accuracy = 0.851562\n",
      "[2018-07-16 13:37:34.138979] Iteration 12900, train loss = 0.531133, train accuracy = 0.843750\n",
      "[2018-07-16 13:37:37.870655] Iteration 13000, train loss = 0.612660, train accuracy = 0.804688\n",
      "Evaluating...\n",
      "Test accuracy = 0.794900\n",
      "[2018-07-16 13:37:42.649381] Iteration 13100, train loss = 0.606246, train accuracy = 0.828125\n",
      "[2018-07-16 13:37:46.291866] Iteration 13200, train loss = 0.529688, train accuracy = 0.828125\n",
      "[2018-07-16 13:37:49.945334] Iteration 13300, train loss = 0.583140, train accuracy = 0.804688\n",
      "[2018-07-16 13:37:53.601443] Iteration 13400, train loss = 0.618068, train accuracy = 0.812500\n",
      "[2018-07-16 13:37:57.244595] Iteration 13500, train loss = 0.549892, train accuracy = 0.835938\n",
      "[2018-07-16 13:38:00.870707] Iteration 13600, train loss = 0.498802, train accuracy = 0.828125\n",
      "[2018-07-16 13:38:04.539577] Iteration 13700, train loss = 0.608513, train accuracy = 0.796875\n",
      "[2018-07-16 13:38:08.190255] Iteration 13800, train loss = 0.454811, train accuracy = 0.882812\n",
      "[2018-07-16 13:38:11.845762] Iteration 13900, train loss = 0.506285, train accuracy = 0.851562\n",
      "[2018-07-16 13:38:15.478566] Iteration 14000, train loss = 0.546709, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.779100\n",
      "[2018-07-16 13:38:20.339910] Iteration 14100, train loss = 0.441869, train accuracy = 0.898438\n",
      "[2018-07-16 13:38:23.996751] Iteration 14200, train loss = 0.517513, train accuracy = 0.851562\n",
      "[2018-07-16 13:38:27.650836] Iteration 14300, train loss = 0.593665, train accuracy = 0.781250\n",
      "[2018-07-16 13:38:31.300815] Iteration 14400, train loss = 0.475889, train accuracy = 0.875000\n",
      "[2018-07-16 13:38:34.949254] Iteration 14500, train loss = 0.443270, train accuracy = 0.898438\n",
      "[2018-07-16 13:38:38.586077] Iteration 14600, train loss = 0.496632, train accuracy = 0.875000\n",
      "[2018-07-16 13:38:42.231930] Iteration 14700, train loss = 0.579316, train accuracy = 0.843750\n",
      "[2018-07-16 13:38:45.867461] Iteration 14800, train loss = 0.514328, train accuracy = 0.820312\n",
      "[2018-07-16 13:38:49.500666] Iteration 14900, train loss = 0.453532, train accuracy = 0.890625\n",
      "[2018-07-16 13:38:53.145103] Iteration 15000, train loss = 0.590281, train accuracy = 0.804688\n",
      "Evaluating...\n",
      "Test accuracy = 0.792600\n",
      "[2018-07-16 13:38:57.902990] Iteration 15100, train loss = 0.661162, train accuracy = 0.796875\n",
      "[2018-07-16 13:39:01.627472] Iteration 15200, train loss = 0.563649, train accuracy = 0.859375\n",
      "[2018-07-16 13:39:05.288658] Iteration 15300, train loss = 0.476129, train accuracy = 0.851562\n",
      "[2018-07-16 13:39:08.946154] Iteration 15400, train loss = 0.474121, train accuracy = 0.890625\n",
      "[2018-07-16 13:39:12.603622] Iteration 15500, train loss = 0.515942, train accuracy = 0.882812\n",
      "[2018-07-16 13:39:16.253044] Iteration 15600, train loss = 0.514765, train accuracy = 0.812500\n",
      "[2018-07-16 13:39:19.908537] Iteration 15700, train loss = 0.508860, train accuracy = 0.875000\n",
      "[2018-07-16 13:39:23.564250] Iteration 15800, train loss = 0.647507, train accuracy = 0.796875\n",
      "[2018-07-16 13:39:27.208588] Iteration 15900, train loss = 0.737284, train accuracy = 0.765625\n",
      "[2018-07-16 13:39:30.849605] Iteration 16000, train loss = 0.562337, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.815200\n",
      "[2018-07-16 13:39:35.621074] Iteration 16100, train loss = 0.594843, train accuracy = 0.796875\n",
      "[2018-07-16 13:39:39.250629] Iteration 16200, train loss = 0.517647, train accuracy = 0.812500\n",
      "[2018-07-16 13:39:42.950281] Iteration 16300, train loss = 0.530876, train accuracy = 0.843750\n",
      "[2018-07-16 13:39:46.588300] Iteration 16400, train loss = 0.549752, train accuracy = 0.851562\n",
      "[2018-07-16 13:39:50.237757] Iteration 16500, train loss = 0.474145, train accuracy = 0.867188\n",
      "[2018-07-16 13:39:53.888982] Iteration 16600, train loss = 0.535152, train accuracy = 0.835938\n",
      "[2018-07-16 13:39:57.529767] Iteration 16700, train loss = 0.541837, train accuracy = 0.875000\n",
      "[2018-07-16 13:40:01.177119] Iteration 16800, train loss = 0.494863, train accuracy = 0.851562\n",
      "[2018-07-16 13:40:04.835383] Iteration 16900, train loss = 0.599469, train accuracy = 0.812500\n",
      "[2018-07-16 13:40:08.497383] Iteration 17000, train loss = 0.521122, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.812700\n",
      "[2018-07-16 13:40:13.280673] Iteration 17100, train loss = 0.510723, train accuracy = 0.828125\n",
      "[2018-07-16 13:40:16.947244] Iteration 17200, train loss = 0.386538, train accuracy = 0.890625\n",
      "[2018-07-16 13:40:20.581801] Iteration 17300, train loss = 0.546442, train accuracy = 0.828125\n",
      "[2018-07-16 13:40:24.305505] Iteration 17400, train loss = 0.439858, train accuracy = 0.867188\n",
      "[2018-07-16 13:40:27.935615] Iteration 17500, train loss = 0.590914, train accuracy = 0.789062\n",
      "[2018-07-16 13:40:31.560255] Iteration 17600, train loss = 0.586932, train accuracy = 0.835938\n",
      "[2018-07-16 13:40:35.214782] Iteration 17700, train loss = 0.442698, train accuracy = 0.843750\n",
      "[2018-07-16 13:40:38.873537] Iteration 17800, train loss = 0.405303, train accuracy = 0.867188\n",
      "[2018-07-16 13:40:42.529622] Iteration 17900, train loss = 0.410335, train accuracy = 0.890625\n",
      "[2018-07-16 13:40:46.171356] Iteration 18000, train loss = 0.531177, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.813700\n",
      "[2018-07-16 13:40:50.993382] Iteration 18100, train loss = 0.616339, train accuracy = 0.773438\n",
      "[2018-07-16 13:40:54.662709] Iteration 18200, train loss = 0.483061, train accuracy = 0.843750\n",
      "[2018-07-16 13:40:58.319483] Iteration 18300, train loss = 0.450734, train accuracy = 0.867188\n",
      "[2018-07-16 13:41:01.981261] Iteration 18400, train loss = 0.496552, train accuracy = 0.859375\n",
      "[2018-07-16 13:41:05.672558] Iteration 18500, train loss = 0.439327, train accuracy = 0.882812\n",
      "[2018-07-16 13:41:09.351933] Iteration 18600, train loss = 0.546354, train accuracy = 0.843750\n",
      "[2018-07-16 13:41:12.986015] Iteration 18700, train loss = 0.520932, train accuracy = 0.835938\n",
      "[2018-07-16 13:41:16.626501] Iteration 18800, train loss = 0.485658, train accuracy = 0.882812\n",
      "[2018-07-16 13:41:20.273494] Iteration 18900, train loss = 0.602705, train accuracy = 0.820312\n",
      "[2018-07-16 13:41:23.909240] Iteration 19000, train loss = 0.448994, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.818900\n",
      "[2018-07-16 13:41:28.683853] Iteration 19100, train loss = 0.468617, train accuracy = 0.875000\n",
      "[2018-07-16 13:41:32.330081] Iteration 19200, train loss = 0.541430, train accuracy = 0.859375\n",
      "[2018-07-16 13:41:35.974383] Iteration 19300, train loss = 0.524019, train accuracy = 0.828125\n",
      "[2018-07-16 13:41:39.605829] Iteration 19400, train loss = 0.589373, train accuracy = 0.851562\n",
      "[2018-07-16 13:41:43.244391] Iteration 19500, train loss = 0.520133, train accuracy = 0.835938\n",
      "[2018-07-16 13:41:46.887687] Iteration 19600, train loss = 0.494273, train accuracy = 0.828125\n",
      "[2018-07-16 13:41:50.622887] Iteration 19700, train loss = 0.407424, train accuracy = 0.906250\n",
      "[2018-07-16 13:41:54.260161] Iteration 19800, train loss = 0.554726, train accuracy = 0.820312\n",
      "[2018-07-16 13:41:57.912023] Iteration 19900, train loss = 0.586273, train accuracy = 0.812500\n",
      "[2018-07-16 13:42:01.567736] Iteration 20000, train loss = 0.536226, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.806700\n",
      "[2018-07-16 13:42:06.334352] Iteration 20100, train loss = 0.609920, train accuracy = 0.820312\n",
      "[2018-07-16 13:42:09.970183] Iteration 20200, train loss = 0.453536, train accuracy = 0.851562\n",
      "[2018-07-16 13:42:13.613483] Iteration 20300, train loss = 0.522121, train accuracy = 0.859375\n",
      "[2018-07-16 13:42:17.259196] Iteration 20400, train loss = 0.467597, train accuracy = 0.859375\n",
      "[2018-07-16 13:42:20.920974] Iteration 20500, train loss = 0.444354, train accuracy = 0.867188\n",
      "[2018-07-16 13:42:24.552723] Iteration 20600, train loss = 0.568756, train accuracy = 0.843750\n",
      "[2018-07-16 13:42:28.195714] Iteration 20700, train loss = 0.668520, train accuracy = 0.812500\n",
      "[2018-07-16 13:42:31.926884] Iteration 20800, train loss = 0.558953, train accuracy = 0.820312\n",
      "[2018-07-16 13:42:35.577080] Iteration 20900, train loss = 0.504952, train accuracy = 0.835938\n",
      "[2018-07-16 13:42:39.240584] Iteration 21000, train loss = 0.536845, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.796700\n",
      "[2018-07-16 13:42:44.006434] Iteration 21100, train loss = 0.543849, train accuracy = 0.828125\n",
      "[2018-07-16 13:42:47.637888] Iteration 21200, train loss = 0.404708, train accuracy = 0.882812\n",
      "[2018-07-16 13:42:51.276916] Iteration 21300, train loss = 0.431724, train accuracy = 0.867188\n",
      "[2018-07-16 13:42:54.920436] Iteration 21400, train loss = 0.532890, train accuracy = 0.820312\n",
      "[2018-07-16 13:42:58.575651] Iteration 21500, train loss = 0.521365, train accuracy = 0.859375\n",
      "[2018-07-16 13:43:02.256061] Iteration 21600, train loss = 0.494458, train accuracy = 0.851562\n",
      "[2018-07-16 13:43:05.898120] Iteration 21700, train loss = 0.407420, train accuracy = 0.906250\n",
      "[2018-07-16 13:43:09.541463] Iteration 21800, train loss = 0.573955, train accuracy = 0.804688\n",
      "[2018-07-16 13:43:13.262754] Iteration 21900, train loss = 0.696845, train accuracy = 0.812500\n",
      "[2018-07-16 13:43:16.919056] Iteration 22000, train loss = 0.534217, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.813700\n",
      "[2018-07-16 13:43:21.690563] Iteration 22100, train loss = 0.580817, train accuracy = 0.835938\n",
      "[2018-07-16 13:43:25.344133] Iteration 22200, train loss = 0.588269, train accuracy = 0.781250\n",
      "[2018-07-16 13:43:29.004598] Iteration 22300, train loss = 0.598570, train accuracy = 0.804688\n",
      "[2018-07-16 13:43:32.660194] Iteration 22400, train loss = 0.498449, train accuracy = 0.843750\n",
      "[2018-07-16 13:43:36.313726] Iteration 22500, train loss = 0.589806, train accuracy = 0.851562\n",
      "[2018-07-16 13:43:39.940764] Iteration 22600, train loss = 0.494851, train accuracy = 0.843750\n",
      "[2018-07-16 13:43:43.579480] Iteration 22700, train loss = 0.384862, train accuracy = 0.898438\n",
      "[2018-07-16 13:43:47.223442] Iteration 22800, train loss = 0.486010, train accuracy = 0.875000\n",
      "[2018-07-16 13:43:50.871403] Iteration 22900, train loss = 0.588907, train accuracy = 0.789062\n",
      "[2018-07-16 13:43:54.591035] Iteration 23000, train loss = 0.362648, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.821500\n",
      "[2018-07-16 13:43:59.397157] Iteration 23100, train loss = 0.405167, train accuracy = 0.882812\n",
      "[2018-07-16 13:44:03.041164] Iteration 23200, train loss = 0.457807, train accuracy = 0.882812\n",
      "[2018-07-16 13:44:06.698436] Iteration 23300, train loss = 0.532806, train accuracy = 0.828125\n",
      "[2018-07-16 13:44:10.334331] Iteration 23400, train loss = 0.461719, train accuracy = 0.875000\n",
      "[2018-07-16 13:44:13.979141] Iteration 23500, train loss = 0.457640, train accuracy = 0.906250\n",
      "[2018-07-16 13:44:17.606074] Iteration 23600, train loss = 0.465609, train accuracy = 0.867188\n",
      "[2018-07-16 13:44:21.243308] Iteration 23700, train loss = 0.439853, train accuracy = 0.882812\n",
      "[2018-07-16 13:44:24.881619] Iteration 23800, train loss = 0.417842, train accuracy = 0.882812\n",
      "[2018-07-16 13:44:28.531849] Iteration 23900, train loss = 0.655404, train accuracy = 0.835938\n",
      "[2018-07-16 13:44:32.186516] Iteration 24000, train loss = 0.415485, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.822600\n",
      "[2018-07-16 13:44:37.001665] Iteration 24100, train loss = 0.495859, train accuracy = 0.851562\n",
      "[2018-07-16 13:44:40.664503] Iteration 24200, train loss = 0.508178, train accuracy = 0.867188\n",
      "[2018-07-16 13:44:44.336628] Iteration 24300, train loss = 0.483349, train accuracy = 0.851562\n",
      "[2018-07-16 13:44:47.986435] Iteration 24400, train loss = 0.522490, train accuracy = 0.820312\n",
      "[2018-07-16 13:44:51.632562] Iteration 24500, train loss = 0.622833, train accuracy = 0.820312\n",
      "[2018-07-16 13:44:55.280550] Iteration 24600, train loss = 0.458370, train accuracy = 0.882812\n",
      "[2018-07-16 13:44:58.928391] Iteration 24700, train loss = 0.463638, train accuracy = 0.882812\n",
      "[2018-07-16 13:45:02.578771] Iteration 24800, train loss = 0.457191, train accuracy = 0.867188\n",
      "[2018-07-16 13:45:06.221900] Iteration 24900, train loss = 0.578282, train accuracy = 0.828125\n",
      "[2018-07-16 13:45:09.868266] Iteration 25000, train loss = 0.518055, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.800200\n",
      "[2018-07-16 13:45:14.640085] Iteration 25100, train loss = 0.487181, train accuracy = 0.867188\n",
      "[2018-07-16 13:45:18.325771] Iteration 25200, train loss = 0.431784, train accuracy = 0.882812\n",
      "[2018-07-16 13:45:21.984211] Iteration 25300, train loss = 0.439459, train accuracy = 0.882812\n",
      "[2018-07-16 13:45:25.644260] Iteration 25400, train loss = 0.426608, train accuracy = 0.875000\n",
      "[2018-07-16 13:45:29.295436] Iteration 25500, train loss = 0.619708, train accuracy = 0.796875\n",
      "[2018-07-16 13:45:32.935375] Iteration 25600, train loss = 0.449667, train accuracy = 0.859375\n",
      "[2018-07-16 13:45:36.583680] Iteration 25700, train loss = 0.430557, train accuracy = 0.882812\n",
      "[2018-07-16 13:45:40.225142] Iteration 25800, train loss = 0.483700, train accuracy = 0.835938\n",
      "[2018-07-16 13:45:43.874987] Iteration 25900, train loss = 0.356083, train accuracy = 0.914062\n",
      "[2018-07-16 13:45:47.525346] Iteration 26000, train loss = 0.433214, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.824900\n",
      "[2018-07-16 13:45:52.300944] Iteration 26100, train loss = 0.467287, train accuracy = 0.859375\n",
      "[2018-07-16 13:45:55.963142] Iteration 26200, train loss = 0.560081, train accuracy = 0.820312\n",
      "[2018-07-16 13:45:59.611598] Iteration 26300, train loss = 0.434496, train accuracy = 0.906250\n",
      "[2018-07-16 13:46:03.313643] Iteration 26400, train loss = 0.533315, train accuracy = 0.843750\n",
      "[2018-07-16 13:46:06.940622] Iteration 26500, train loss = 0.471002, train accuracy = 0.843750\n",
      "[2018-07-16 13:46:10.583319] Iteration 26600, train loss = 0.449492, train accuracy = 0.875000\n",
      "[2018-07-16 13:46:14.246172] Iteration 26700, train loss = 0.508079, train accuracy = 0.851562\n",
      "[2018-07-16 13:46:17.905083] Iteration 26800, train loss = 0.430282, train accuracy = 0.882812\n",
      "[2018-07-16 13:46:21.585303] Iteration 26900, train loss = 0.632742, train accuracy = 0.812500\n",
      "[2018-07-16 13:46:25.259982] Iteration 27000, train loss = 0.520032, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.817000\n",
      "[2018-07-16 13:46:30.077519] Iteration 27100, train loss = 0.587846, train accuracy = 0.828125\n",
      "[2018-07-16 13:46:33.738347] Iteration 27200, train loss = 0.461807, train accuracy = 0.851562\n",
      "[2018-07-16 13:46:37.433181] Iteration 27300, train loss = 0.470148, train accuracy = 0.882812\n",
      "[2018-07-16 13:46:41.114144] Iteration 27400, train loss = 0.549243, train accuracy = 0.796875\n",
      "[2018-07-16 13:46:44.819994] Iteration 27500, train loss = 0.479990, train accuracy = 0.859375\n",
      "[2018-07-16 13:46:48.459996] Iteration 27600, train loss = 0.574164, train accuracy = 0.828125\n",
      "[2018-07-16 13:46:52.093822] Iteration 27700, train loss = 0.458318, train accuracy = 0.882812\n",
      "[2018-07-16 13:46:55.726973] Iteration 27800, train loss = 0.415644, train accuracy = 0.882812\n",
      "[2018-07-16 13:46:59.358994] Iteration 27900, train loss = 0.432236, train accuracy = 0.898438\n",
      "[2018-07-16 13:47:03.005603] Iteration 28000, train loss = 0.563023, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.824600\n",
      "[2018-07-16 13:47:07.787743] Iteration 28100, train loss = 0.438586, train accuracy = 0.867188\n",
      "[2018-07-16 13:47:11.449486] Iteration 28200, train loss = 0.409487, train accuracy = 0.890625\n",
      "[2018-07-16 13:47:15.094979] Iteration 28300, train loss = 0.450171, train accuracy = 0.851562\n",
      "[2018-07-16 13:47:18.743998] Iteration 28400, train loss = 0.432041, train accuracy = 0.859375\n",
      "[2018-07-16 13:47:22.405853] Iteration 28500, train loss = 0.455670, train accuracy = 0.875000\n",
      "[2018-07-16 13:47:26.106912] Iteration 28600, train loss = 0.534398, train accuracy = 0.851562\n",
      "[2018-07-16 13:47:29.777675] Iteration 28700, train loss = 0.525370, train accuracy = 0.843750\n",
      "[2018-07-16 13:47:33.408945] Iteration 28800, train loss = 0.503034, train accuracy = 0.875000\n",
      "[2018-07-16 13:47:37.058945] Iteration 28900, train loss = 0.474104, train accuracy = 0.867188\n",
      "[2018-07-16 13:47:40.713112] Iteration 29000, train loss = 0.546063, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.824900\n",
      "[2018-07-16 13:47:45.473587] Iteration 29100, train loss = 0.563433, train accuracy = 0.828125\n",
      "[2018-07-16 13:47:49.112293] Iteration 29200, train loss = 0.506251, train accuracy = 0.828125\n",
      "[2018-07-16 13:47:52.748526] Iteration 29300, train loss = 0.527083, train accuracy = 0.875000\n",
      "[2018-07-16 13:47:56.407848] Iteration 29400, train loss = 0.480803, train accuracy = 0.875000\n",
      "[2018-07-16 13:48:00.046824] Iteration 29500, train loss = 0.539405, train accuracy = 0.867188\n",
      "[2018-07-16 13:48:03.681930] Iteration 29600, train loss = 0.562642, train accuracy = 0.828125\n",
      "[2018-07-16 13:48:07.394313] Iteration 29700, train loss = 0.496062, train accuracy = 0.867188\n",
      "[2018-07-16 13:48:11.063000] Iteration 29800, train loss = 0.588138, train accuracy = 0.820312\n",
      "[2018-07-16 13:48:14.733933] Iteration 29900, train loss = 0.404487, train accuracy = 0.867188\n",
      "[2018-07-16 13:48:18.388844] Iteration 30000, train loss = 0.457344, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.829200\n",
      "[2018-07-16 13:48:23.159379] Iteration 30100, train loss = 0.605494, train accuracy = 0.781250\n",
      "[2018-07-16 13:48:26.804483] Iteration 30200, train loss = 0.551800, train accuracy = 0.875000\n",
      "[2018-07-16 13:48:30.457293] Iteration 30300, train loss = 0.482994, train accuracy = 0.843750\n",
      "[2018-07-16 13:48:34.098065] Iteration 30400, train loss = 0.617344, train accuracy = 0.820312\n",
      "[2018-07-16 13:48:37.747297] Iteration 30500, train loss = 0.534461, train accuracy = 0.875000\n",
      "[2018-07-16 13:48:41.396013] Iteration 30600, train loss = 0.456846, train accuracy = 0.867188\n",
      "[2018-07-16 13:48:45.038651] Iteration 30700, train loss = 0.460274, train accuracy = 0.867188\n",
      "[2018-07-16 13:48:48.760300] Iteration 30800, train loss = 0.739428, train accuracy = 0.796875\n",
      "[2018-07-16 13:48:52.410591] Iteration 30900, train loss = 0.458484, train accuracy = 0.882812\n",
      "[2018-07-16 13:48:56.060070] Iteration 31000, train loss = 0.457569, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.819300\n",
      "[2018-07-16 13:49:00.825748] Iteration 31100, train loss = 0.504645, train accuracy = 0.851562\n",
      "[2018-07-16 13:49:04.469425] Iteration 31200, train loss = 0.399615, train accuracy = 0.882812\n",
      "[2018-07-16 13:49:08.107576] Iteration 31300, train loss = 0.471298, train accuracy = 0.875000\n",
      "[2018-07-16 13:49:11.740646] Iteration 31400, train loss = 0.649944, train accuracy = 0.820312\n",
      "[2018-07-16 13:49:15.364903] Iteration 31500, train loss = 0.496808, train accuracy = 0.882812\n",
      "[2018-07-16 13:49:18.997091] Iteration 31600, train loss = 0.468590, train accuracy = 0.851562\n",
      "[2018-07-16 13:49:22.641175] Iteration 31700, train loss = 0.467640, train accuracy = 0.867188\n",
      "[2018-07-16 13:49:26.301872] Iteration 31800, train loss = 0.442173, train accuracy = 0.882812\n",
      "[2018-07-16 13:49:30.052802] Iteration 31900, train loss = 0.591458, train accuracy = 0.796875\n",
      "[2018-07-16 13:49:33.705632] Iteration 32000, train loss = 0.561230, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.810300\n",
      "[2018-07-16 13:49:38.487536] Iteration 32100, train loss = 0.391962, train accuracy = 0.882812\n",
      "[2018-07-16 13:49:42.138302] Iteration 32200, train loss = 0.458335, train accuracy = 0.859375\n",
      "[2018-07-16 13:49:45.781657] Iteration 32300, train loss = 0.456992, train accuracy = 0.859375\n",
      "[2018-07-16 13:49:49.426240] Iteration 32400, train loss = 0.464005, train accuracy = 0.843750\n",
      "[2018-07-16 13:49:53.095745] Iteration 32500, train loss = 0.438310, train accuracy = 0.859375\n",
      "[2018-07-16 13:49:56.739224] Iteration 32600, train loss = 0.475217, train accuracy = 0.882812\n",
      "[2018-07-16 13:50:00.380713] Iteration 32700, train loss = 0.424661, train accuracy = 0.875000\n",
      "[2018-07-16 13:50:04.026313] Iteration 32800, train loss = 0.627967, train accuracy = 0.820312\n",
      "[2018-07-16 13:50:07.677863] Iteration 32900, train loss = 0.500574, train accuracy = 0.835938\n",
      "[2018-07-16 13:50:11.354741] Iteration 33000, train loss = 0.463592, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.828500\n",
      "[2018-07-16 13:50:16.205395] Iteration 33100, train loss = 0.481393, train accuracy = 0.867188\n",
      "[2018-07-16 13:50:19.855553] Iteration 33200, train loss = 0.496931, train accuracy = 0.843750\n",
      "[2018-07-16 13:50:23.503052] Iteration 33300, train loss = 0.458264, train accuracy = 0.875000\n",
      "[2018-07-16 13:50:27.157873] Iteration 33400, train loss = 0.442557, train accuracy = 0.875000\n",
      "[2018-07-16 13:50:30.804129] Iteration 33500, train loss = 0.575471, train accuracy = 0.820312\n",
      "[2018-07-16 13:50:34.457990] Iteration 33600, train loss = 0.424298, train accuracy = 0.859375\n",
      "[2018-07-16 13:50:38.107481] Iteration 33700, train loss = 0.477867, train accuracy = 0.867188\n",
      "[2018-07-16 13:50:41.748963] Iteration 33800, train loss = 0.479765, train accuracy = 0.835938\n",
      "[2018-07-16 13:50:45.408283] Iteration 33900, train loss = 0.505699, train accuracy = 0.820312\n",
      "[2018-07-16 13:50:49.059624] Iteration 34000, train loss = 0.388331, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.812700\n",
      "[2018-07-16 13:50:53.889818] Iteration 34100, train loss = 0.448393, train accuracy = 0.898438\n",
      "[2018-07-16 13:50:57.537945] Iteration 34200, train loss = 0.450876, train accuracy = 0.859375\n",
      "[2018-07-16 13:51:01.187027] Iteration 34300, train loss = 0.523670, train accuracy = 0.867188\n",
      "[2018-07-16 13:51:04.830945] Iteration 34400, train loss = 0.400171, train accuracy = 0.906250\n",
      "[2018-07-16 13:51:08.498699] Iteration 34500, train loss = 0.524592, train accuracy = 0.828125\n",
      "[2018-07-16 13:51:12.145748] Iteration 34600, train loss = 0.495429, train accuracy = 0.875000\n",
      "[2018-07-16 13:51:15.806661] Iteration 34700, train loss = 0.549458, train accuracy = 0.859375\n",
      "[2018-07-16 13:51:19.453959] Iteration 34800, train loss = 0.436455, train accuracy = 0.875000\n",
      "[2018-07-16 13:51:23.106878] Iteration 34900, train loss = 0.519419, train accuracy = 0.828125\n",
      "[2018-07-16 13:51:26.752402] Iteration 35000, train loss = 0.413999, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.822300\n",
      "[2018-07-16 13:51:31.563151] Iteration 35100, train loss = 0.431237, train accuracy = 0.898438\n",
      "[2018-07-16 13:51:35.225921] Iteration 35200, train loss = 0.387563, train accuracy = 0.906250\n",
      "[2018-07-16 13:51:38.902026] Iteration 35300, train loss = 0.547202, train accuracy = 0.835938\n",
      "[2018-07-16 13:51:42.539939] Iteration 35400, train loss = 0.568613, train accuracy = 0.820312\n",
      "[2018-07-16 13:51:46.167304] Iteration 35500, train loss = 0.439171, train accuracy = 0.859375\n",
      "[2018-07-16 13:51:49.816067] Iteration 35600, train loss = 0.383039, train accuracy = 0.890625\n",
      "[2018-07-16 13:51:53.473971] Iteration 35700, train loss = 0.451549, train accuracy = 0.890625\n",
      "[2018-07-16 13:51:57.132652] Iteration 35800, train loss = 0.550632, train accuracy = 0.851562\n",
      "[2018-07-16 13:52:00.766463] Iteration 35900, train loss = 0.322498, train accuracy = 0.921875\n",
      "[2018-07-16 13:52:04.411699] Iteration 36000, train loss = 0.503355, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.808400\n",
      "[2018-07-16 13:52:09.225871] Iteration 36100, train loss = 0.519525, train accuracy = 0.859375\n",
      "[2018-07-16 13:52:12.862477] Iteration 36200, train loss = 0.482892, train accuracy = 0.851562\n",
      "[2018-07-16 13:52:16.509305] Iteration 36300, train loss = 0.324754, train accuracy = 0.914062\n",
      "[2018-07-16 13:52:20.225592] Iteration 36400, train loss = 0.425215, train accuracy = 0.859375\n",
      "[2018-07-16 13:52:23.859459] Iteration 36500, train loss = 0.569115, train accuracy = 0.820312\n",
      "[2018-07-16 13:52:27.486728] Iteration 36600, train loss = 0.438112, train accuracy = 0.890625\n",
      "[2018-07-16 13:52:31.126860] Iteration 36700, train loss = 0.533809, train accuracy = 0.875000\n",
      "[2018-07-16 13:52:34.759880] Iteration 36800, train loss = 0.586021, train accuracy = 0.851562\n",
      "[2018-07-16 13:52:38.417379] Iteration 36900, train loss = 0.392131, train accuracy = 0.906250\n",
      "[2018-07-16 13:52:42.066094] Iteration 37000, train loss = 0.499383, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.801400\n",
      "[2018-07-16 13:52:46.845391] Iteration 37100, train loss = 0.294378, train accuracy = 0.921875\n",
      "[2018-07-16 13:52:50.500953] Iteration 37200, train loss = 0.490008, train accuracy = 0.851562\n",
      "[2018-07-16 13:52:54.143093] Iteration 37300, train loss = 0.510870, train accuracy = 0.851562\n",
      "[2018-07-16 13:52:57.780209] Iteration 37400, train loss = 0.458859, train accuracy = 0.882812\n",
      "[2018-07-16 13:53:01.497047] Iteration 37500, train loss = 0.458576, train accuracy = 0.898438\n",
      "[2018-07-16 13:53:05.158881] Iteration 37600, train loss = 0.467046, train accuracy = 0.875000\n",
      "[2018-07-16 13:53:08.796272] Iteration 37700, train loss = 0.402219, train accuracy = 0.867188\n",
      "[2018-07-16 13:53:12.422486] Iteration 37800, train loss = 0.537444, train accuracy = 0.828125\n",
      "[2018-07-16 13:53:16.066263] Iteration 37900, train loss = 0.359149, train accuracy = 0.890625\n",
      "[2018-07-16 13:53:19.702102] Iteration 38000, train loss = 0.418094, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.815600\n",
      "[2018-07-16 13:53:24.465212] Iteration 38100, train loss = 0.468446, train accuracy = 0.890625\n",
      "[2018-07-16 13:53:28.112636] Iteration 38200, train loss = 0.434369, train accuracy = 0.859375\n",
      "[2018-07-16 13:53:31.771824] Iteration 38300, train loss = 0.592070, train accuracy = 0.835938\n",
      "[2018-07-16 13:53:35.431181] Iteration 38400, train loss = 0.342911, train accuracy = 0.914062\n",
      "[2018-07-16 13:53:39.078533] Iteration 38500, train loss = 0.509814, train accuracy = 0.843750\n",
      "[2018-07-16 13:53:42.783073] Iteration 38600, train loss = 0.435112, train accuracy = 0.851562\n",
      "[2018-07-16 13:53:46.430180] Iteration 38700, train loss = 0.408449, train accuracy = 0.890625\n",
      "[2018-07-16 13:53:50.078940] Iteration 38800, train loss = 0.314797, train accuracy = 0.906250\n",
      "[2018-07-16 13:53:53.741052] Iteration 38900, train loss = 0.503791, train accuracy = 0.843750\n",
      "[2018-07-16 13:53:57.374436] Iteration 39000, train loss = 0.380072, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.792600\n",
      "[2018-07-16 13:54:02.124733] Iteration 39100, train loss = 0.506273, train accuracy = 0.859375\n",
      "[2018-07-16 13:54:05.752459] Iteration 39200, train loss = 0.458846, train accuracy = 0.859375\n",
      "[2018-07-16 13:54:09.389762] Iteration 39300, train loss = 0.491614, train accuracy = 0.851562\n",
      "[2018-07-16 13:54:13.031332] Iteration 39400, train loss = 0.492896, train accuracy = 0.875000\n",
      "[2018-07-16 13:54:16.675359] Iteration 39500, train loss = 0.424353, train accuracy = 0.859375\n",
      "[2018-07-16 13:54:20.333752] Iteration 39600, train loss = 0.431744, train accuracy = 0.898438\n",
      "[2018-07-16 13:54:24.056887] Iteration 39700, train loss = 0.422115, train accuracy = 0.875000\n",
      "[2018-07-16 13:54:27.734995] Iteration 39800, train loss = 0.484284, train accuracy = 0.875000\n",
      "[2018-07-16 13:54:31.383116] Iteration 39900, train loss = 0.507065, train accuracy = 0.828125\n",
      "[2018-07-16 13:54:35.046226] Iteration 40000, train loss = 0.487895, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.815000\n",
      "[2018-07-16 13:54:39.795408] Iteration 40100, train loss = 0.371845, train accuracy = 0.898438\n",
      "[2018-07-16 13:54:43.449284] Iteration 40200, train loss = 0.355998, train accuracy = 0.906250\n",
      "[2018-07-16 13:54:47.081352] Iteration 40300, train loss = 0.352773, train accuracy = 0.921875\n",
      "[2018-07-16 13:54:50.734518] Iteration 40400, train loss = 0.430479, train accuracy = 0.890625\n",
      "[2018-07-16 13:54:54.367822] Iteration 40500, train loss = 0.393472, train accuracy = 0.890625\n",
      "[2018-07-16 13:54:58.009588] Iteration 40600, train loss = 0.448821, train accuracy = 0.898438\n",
      "[2018-07-16 13:55:01.654018] Iteration 40700, train loss = 0.442419, train accuracy = 0.867188\n",
      "[2018-07-16 13:55:05.339448] Iteration 40800, train loss = 0.473079, train accuracy = 0.828125\n",
      "[2018-07-16 13:55:09.057062] Iteration 40900, train loss = 0.485776, train accuracy = 0.859375\n",
      "[2018-07-16 13:55:12.715338] Iteration 41000, train loss = 0.527299, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.805300\n",
      "[2018-07-16 13:55:17.544914] Iteration 41100, train loss = 0.477323, train accuracy = 0.828125\n",
      "[2018-07-16 13:55:21.191229] Iteration 41200, train loss = 0.612179, train accuracy = 0.820312\n",
      "[2018-07-16 13:55:24.839211] Iteration 41300, train loss = 0.428958, train accuracy = 0.882812\n",
      "[2018-07-16 13:55:28.494182] Iteration 41400, train loss = 0.551093, train accuracy = 0.867188\n",
      "[2018-07-16 13:55:32.147289] Iteration 41500, train loss = 0.465887, train accuracy = 0.851562\n",
      "[2018-07-16 13:55:35.781966] Iteration 41600, train loss = 0.380999, train accuracy = 0.875000\n",
      "[2018-07-16 13:55:39.415715] Iteration 41700, train loss = 0.556714, train accuracy = 0.828125\n",
      "[2018-07-16 13:55:43.057873] Iteration 41800, train loss = 0.417243, train accuracy = 0.875000\n",
      "[2018-07-16 13:55:46.689297] Iteration 41900, train loss = 0.505391, train accuracy = 0.851562\n",
      "[2018-07-16 13:55:50.419407] Iteration 42000, train loss = 0.559122, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.828500\n",
      "[2018-07-16 13:55:55.195084] Iteration 42100, train loss = 0.380972, train accuracy = 0.929688\n",
      "[2018-07-16 13:55:58.835722] Iteration 42200, train loss = 0.455464, train accuracy = 0.875000\n",
      "[2018-07-16 13:56:02.492289] Iteration 42300, train loss = 0.379713, train accuracy = 0.898438\n",
      "[2018-07-16 13:56:06.155272] Iteration 42400, train loss = 0.440580, train accuracy = 0.875000\n",
      "[2018-07-16 13:56:09.808971] Iteration 42500, train loss = 0.473672, train accuracy = 0.859375\n",
      "[2018-07-16 13:56:13.454663] Iteration 42600, train loss = 0.420726, train accuracy = 0.867188\n",
      "[2018-07-16 13:56:17.121585] Iteration 42700, train loss = 0.466220, train accuracy = 0.898438\n",
      "[2018-07-16 13:56:20.755462] Iteration 42800, train loss = 0.510374, train accuracy = 0.867188\n",
      "[2018-07-16 13:56:24.394840] Iteration 42900, train loss = 0.392250, train accuracy = 0.898438\n",
      "[2018-07-16 13:56:28.043610] Iteration 43000, train loss = 0.466747, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.821400\n",
      "[2018-07-16 13:56:32.874793] Iteration 43100, train loss = 0.461030, train accuracy = 0.867188\n",
      "[2018-07-16 13:56:36.521866] Iteration 43200, train loss = 0.554283, train accuracy = 0.820312\n",
      "[2018-07-16 13:56:40.172488] Iteration 43300, train loss = 0.637270, train accuracy = 0.789062\n",
      "[2018-07-16 13:56:43.824002] Iteration 43400, train loss = 0.540994, train accuracy = 0.859375\n",
      "[2018-07-16 13:56:47.477102] Iteration 43500, train loss = 0.603998, train accuracy = 0.835938\n",
      "[2018-07-16 13:56:51.126050] Iteration 43600, train loss = 0.471196, train accuracy = 0.851562\n",
      "[2018-07-16 13:56:54.760475] Iteration 43700, train loss = 0.468181, train accuracy = 0.851562\n",
      "[2018-07-16 13:56:58.409493] Iteration 43800, train loss = 0.441011, train accuracy = 0.890625\n",
      "[2018-07-16 13:57:02.071336] Iteration 43900, train loss = 0.446698, train accuracy = 0.898438\n",
      "[2018-07-16 13:57:05.723983] Iteration 44000, train loss = 0.436364, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.834700\n",
      "[2018-07-16 13:57:10.496813] Iteration 44100, train loss = 0.489226, train accuracy = 0.875000\n",
      "[2018-07-16 13:57:14.226999] Iteration 44200, train loss = 0.496290, train accuracy = 0.835938\n",
      "[2018-07-16 13:57:17.863759] Iteration 44300, train loss = 0.462864, train accuracy = 0.859375\n",
      "[2018-07-16 13:57:21.507107] Iteration 44400, train loss = 0.442858, train accuracy = 0.859375\n",
      "[2018-07-16 13:57:25.156134] Iteration 44500, train loss = 0.553597, train accuracy = 0.820312\n",
      "[2018-07-16 13:57:28.817482] Iteration 44600, train loss = 0.410694, train accuracy = 0.921875\n",
      "[2018-07-16 13:57:32.468553] Iteration 44700, train loss = 0.385084, train accuracy = 0.906250\n",
      "[2018-07-16 13:57:36.117928] Iteration 44800, train loss = 0.496680, train accuracy = 0.820312\n",
      "[2018-07-16 13:57:39.759673] Iteration 44900, train loss = 0.530990, train accuracy = 0.851562\n",
      "[2018-07-16 13:57:43.418055] Iteration 45000, train loss = 0.420944, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.813500\n",
      "[2018-07-16 13:57:48.260136] Iteration 45100, train loss = 0.492366, train accuracy = 0.898438\n",
      "[2018-07-16 13:57:51.917501] Iteration 45200, train loss = 0.455334, train accuracy = 0.882812\n",
      "[2018-07-16 13:57:55.644014] Iteration 45300, train loss = 0.475753, train accuracy = 0.859375\n",
      "[2018-07-16 13:57:59.286136] Iteration 45400, train loss = 0.467335, train accuracy = 0.859375\n",
      "[2018-07-16 13:58:02.933242] Iteration 45500, train loss = 0.441393, train accuracy = 0.867188\n",
      "[2018-07-16 13:58:06.572594] Iteration 45600, train loss = 0.450827, train accuracy = 0.859375\n",
      "[2018-07-16 13:58:10.197245] Iteration 45700, train loss = 0.538149, train accuracy = 0.851562\n",
      "[2018-07-16 13:58:13.859509] Iteration 45800, train loss = 0.381486, train accuracy = 0.914062\n",
      "[2018-07-16 13:58:17.500334] Iteration 45900, train loss = 0.411415, train accuracy = 0.875000\n",
      "[2018-07-16 13:58:21.149476] Iteration 46000, train loss = 0.404191, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.827300\n",
      "[2018-07-16 13:58:25.928559] Iteration 46100, train loss = 0.476354, train accuracy = 0.828125\n",
      "[2018-07-16 13:58:29.581777] Iteration 46200, train loss = 0.549422, train accuracy = 0.828125\n",
      "[2018-07-16 13:58:33.222963] Iteration 46300, train loss = 0.315568, train accuracy = 0.921875\n",
      "[2018-07-16 13:58:36.922714] Iteration 46400, train loss = 0.377742, train accuracy = 0.906250\n",
      "[2018-07-16 13:58:40.571873] Iteration 46500, train loss = 0.540081, train accuracy = 0.843750\n",
      "[2018-07-16 13:58:44.221607] Iteration 46600, train loss = 0.433378, train accuracy = 0.843750\n",
      "[2018-07-16 13:58:47.861006] Iteration 46700, train loss = 0.357096, train accuracy = 0.945312\n",
      "[2018-07-16 13:58:51.501324] Iteration 46800, train loss = 0.484836, train accuracy = 0.875000\n",
      "[2018-07-16 13:58:55.146827] Iteration 46900, train loss = 0.447027, train accuracy = 0.890625\n",
      "[2018-07-16 13:58:58.793403] Iteration 47000, train loss = 0.457830, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.799600\n",
      "[2018-07-16 13:59:03.568276] Iteration 47100, train loss = 0.492318, train accuracy = 0.859375\n",
      "[2018-07-16 13:59:07.227674] Iteration 47200, train loss = 0.441751, train accuracy = 0.867188\n",
      "[2018-07-16 13:59:10.866522] Iteration 47300, train loss = 0.505458, train accuracy = 0.875000\n",
      "[2018-07-16 13:59:14.522811] Iteration 47400, train loss = 0.384223, train accuracy = 0.890625\n",
      "[2018-07-16 13:59:18.219386] Iteration 47500, train loss = 0.440496, train accuracy = 0.875000\n",
      "[2018-07-16 13:59:21.884008] Iteration 47600, train loss = 0.456816, train accuracy = 0.859375\n",
      "[2018-07-16 13:59:25.525623] Iteration 47700, train loss = 0.544139, train accuracy = 0.820312\n",
      "[2018-07-16 13:59:29.165254] Iteration 47800, train loss = 0.540851, train accuracy = 0.828125\n",
      "[2018-07-16 13:59:32.805819] Iteration 47900, train loss = 0.478245, train accuracy = 0.851562\n",
      "[2018-07-16 13:59:36.446203] Iteration 48000, train loss = 0.554052, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.821900\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 13:59:41.212457] Iteration 48100, train loss = 0.467999, train accuracy = 0.867188\n",
      "[2018-07-16 13:59:44.858231] Iteration 48200, train loss = 0.452588, train accuracy = 0.890625\n",
      "[2018-07-16 13:59:48.496787] Iteration 48300, train loss = 0.366774, train accuracy = 0.882812\n",
      "[2018-07-16 13:59:52.150497] Iteration 48400, train loss = 0.366756, train accuracy = 0.898438\n",
      "[2018-07-16 13:59:55.800560] Iteration 48500, train loss = 0.362300, train accuracy = 0.906250\n",
      "[2018-07-16 13:59:59.457416] Iteration 48600, train loss = 0.593485, train accuracy = 0.812500\n",
      "[2018-07-16 14:00:03.160648] Iteration 48700, train loss = 0.474166, train accuracy = 0.882812\n",
      "[2018-07-16 14:00:06.821685] Iteration 48800, train loss = 0.341184, train accuracy = 0.914062\n",
      "[2018-07-16 14:00:10.481087] Iteration 48900, train loss = 0.400699, train accuracy = 0.890625\n",
      "[2018-07-16 14:00:14.138890] Iteration 49000, train loss = 0.493672, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.847400\n",
      "[2018-07-16 14:00:18.942671] Iteration 49100, train loss = 0.377804, train accuracy = 0.875000\n",
      "[2018-07-16 14:00:22.588374] Iteration 49200, train loss = 0.330686, train accuracy = 0.945312\n",
      "[2018-07-16 14:00:26.227967] Iteration 49300, train loss = 0.452928, train accuracy = 0.875000\n",
      "[2018-07-16 14:00:29.864427] Iteration 49400, train loss = 0.552780, train accuracy = 0.835938\n",
      "[2018-07-16 14:00:33.515123] Iteration 49500, train loss = 0.605608, train accuracy = 0.796875\n",
      "[2018-07-16 14:00:37.159980] Iteration 49600, train loss = 0.371402, train accuracy = 0.937500\n",
      "[2018-07-16 14:00:40.823559] Iteration 49700, train loss = 0.525374, train accuracy = 0.851562\n",
      "[2018-07-16 14:00:44.561536] Iteration 49800, train loss = 0.531461, train accuracy = 0.867188\n",
      "[2018-07-16 14:00:48.215862] Iteration 49900, train loss = 0.439356, train accuracy = 0.898438\n",
      "[2018-07-16 14:00:51.870472] Iteration 50000, train loss = 0.401140, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.849900\n",
      "[2018-07-16 14:00:56.680963] Iteration 50100, train loss = 0.400789, train accuracy = 0.898438\n",
      "[2018-07-16 14:01:00.330628] Iteration 50200, train loss = 0.431088, train accuracy = 0.859375\n",
      "[2018-07-16 14:01:03.988248] Iteration 50300, train loss = 0.488256, train accuracy = 0.859375\n",
      "[2018-07-16 14:01:07.633499] Iteration 50400, train loss = 0.589403, train accuracy = 0.820312\n",
      "[2018-07-16 14:01:11.261310] Iteration 50500, train loss = 0.425021, train accuracy = 0.867188\n",
      "[2018-07-16 14:01:14.910992] Iteration 50600, train loss = 0.387955, train accuracy = 0.914062\n",
      "[2018-07-16 14:01:18.552875] Iteration 50700, train loss = 0.373280, train accuracy = 0.875000\n",
      "[2018-07-16 14:01:22.192373] Iteration 50800, train loss = 0.297217, train accuracy = 0.937500\n",
      "[2018-07-16 14:01:25.897057] Iteration 50900, train loss = 0.476067, train accuracy = 0.859375\n",
      "[2018-07-16 14:01:29.542711] Iteration 51000, train loss = 0.569292, train accuracy = 0.828125\n",
      "Evaluating...\n",
      "Test accuracy = 0.850600\n",
      "[2018-07-16 14:01:34.303450] Iteration 51100, train loss = 0.318723, train accuracy = 0.921875\n",
      "[2018-07-16 14:01:37.962458] Iteration 51200, train loss = 0.433533, train accuracy = 0.875000\n",
      "[2018-07-16 14:01:41.616202] Iteration 51300, train loss = 0.341739, train accuracy = 0.898438\n",
      "[2018-07-16 14:01:45.273635] Iteration 51400, train loss = 0.446338, train accuracy = 0.867188\n",
      "[2018-07-16 14:01:48.964596] Iteration 51500, train loss = 0.335997, train accuracy = 0.929688\n",
      "[2018-07-16 14:01:52.632006] Iteration 51600, train loss = 0.455708, train accuracy = 0.875000\n",
      "[2018-07-16 14:01:56.298740] Iteration 51700, train loss = 0.329911, train accuracy = 0.929688\n",
      "[2018-07-16 14:01:59.943760] Iteration 51800, train loss = 0.433513, train accuracy = 0.859375\n",
      "[2018-07-16 14:02:03.574990] Iteration 51900, train loss = 0.344116, train accuracy = 0.921875\n",
      "[2018-07-16 14:02:07.258175] Iteration 52000, train loss = 0.464465, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.849500\n",
      "[2018-07-16 14:02:12.024465] Iteration 52100, train loss = 0.306739, train accuracy = 0.914062\n",
      "[2018-07-16 14:02:15.673165] Iteration 52200, train loss = 0.329781, train accuracy = 0.937500\n",
      "[2018-07-16 14:02:19.325131] Iteration 52300, train loss = 0.475547, train accuracy = 0.851562\n",
      "[2018-07-16 14:02:22.993888] Iteration 52400, train loss = 0.478577, train accuracy = 0.882812\n",
      "[2018-07-16 14:02:26.653798] Iteration 52500, train loss = 0.447772, train accuracy = 0.890625\n",
      "[2018-07-16 14:02:30.298960] Iteration 52600, train loss = 0.362902, train accuracy = 0.921875\n",
      "[2018-07-16 14:02:33.940459] Iteration 52700, train loss = 0.410950, train accuracy = 0.875000\n",
      "[2018-07-16 14:02:37.602851] Iteration 52800, train loss = 0.409496, train accuracy = 0.898438\n",
      "[2018-07-16 14:02:41.273185] Iteration 52900, train loss = 0.373745, train accuracy = 0.921875\n",
      "[2018-07-16 14:02:44.928336] Iteration 53000, train loss = 0.401148, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.848100\n",
      "[2018-07-16 14:02:49.769166] Iteration 53100, train loss = 0.397455, train accuracy = 0.898438\n",
      "[2018-07-16 14:02:53.406340] Iteration 53200, train loss = 0.424036, train accuracy = 0.890625\n",
      "[2018-07-16 14:02:57.037060] Iteration 53300, train loss = 0.493921, train accuracy = 0.851562\n",
      "[2018-07-16 14:03:00.681728] Iteration 53400, train loss = 0.367009, train accuracy = 0.929688\n",
      "[2018-07-16 14:03:04.346778] Iteration 53500, train loss = 0.442081, train accuracy = 0.867188\n",
      "[2018-07-16 14:03:08.014046] Iteration 53600, train loss = 0.342502, train accuracy = 0.921875\n",
      "[2018-07-16 14:03:11.659561] Iteration 53700, train loss = 0.422243, train accuracy = 0.859375\n",
      "[2018-07-16 14:03:15.307881] Iteration 53800, train loss = 0.509792, train accuracy = 0.812500\n",
      "[2018-07-16 14:03:18.958137] Iteration 53900, train loss = 0.460541, train accuracy = 0.867188\n",
      "[2018-07-16 14:03:22.623686] Iteration 54000, train loss = 0.394194, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.850400\n",
      "[2018-07-16 14:03:27.412370] Iteration 54100, train loss = 0.442959, train accuracy = 0.867188\n",
      "[2018-07-16 14:03:31.117362] Iteration 54200, train loss = 0.414207, train accuracy = 0.875000\n",
      "[2018-07-16 14:03:34.767512] Iteration 54300, train loss = 0.302087, train accuracy = 0.953125\n",
      "[2018-07-16 14:03:38.408698] Iteration 54400, train loss = 0.492635, train accuracy = 0.875000\n",
      "[2018-07-16 14:03:42.047874] Iteration 54500, train loss = 0.320367, train accuracy = 0.937500\n",
      "[2018-07-16 14:03:45.704861] Iteration 54600, train loss = 0.463625, train accuracy = 0.867188\n",
      "[2018-07-16 14:03:49.356932] Iteration 54700, train loss = 0.483797, train accuracy = 0.875000\n",
      "[2018-07-16 14:03:53.009171] Iteration 54800, train loss = 0.509540, train accuracy = 0.851562\n",
      "[2018-07-16 14:03:56.673756] Iteration 54900, train loss = 0.464295, train accuracy = 0.890625\n",
      "[2018-07-16 14:04:00.325794] Iteration 55000, train loss = 0.391648, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.851000\n",
      "[2018-07-16 14:04:05.107419] Iteration 55100, train loss = 0.392346, train accuracy = 0.882812\n",
      "[2018-07-16 14:04:08.761999] Iteration 55200, train loss = 0.448223, train accuracy = 0.882812\n",
      "[2018-07-16 14:04:12.455449] Iteration 55300, train loss = 0.450205, train accuracy = 0.882812\n",
      "[2018-07-16 14:04:16.145172] Iteration 55400, train loss = 0.401044, train accuracy = 0.890625\n",
      "[2018-07-16 14:04:19.778306] Iteration 55500, train loss = 0.399664, train accuracy = 0.882812\n",
      "[2018-07-16 14:04:23.420078] Iteration 55600, train loss = 0.530476, train accuracy = 0.828125\n",
      "[2018-07-16 14:04:27.080073] Iteration 55700, train loss = 0.347612, train accuracy = 0.921875\n",
      "[2018-07-16 14:04:30.729677] Iteration 55800, train loss = 0.446173, train accuracy = 0.867188\n",
      "[2018-07-16 14:04:34.371839] Iteration 55900, train loss = 0.407039, train accuracy = 0.914062\n",
      "[2018-07-16 14:04:38.026658] Iteration 56000, train loss = 0.363644, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.850700\n",
      "[2018-07-16 14:04:42.820352] Iteration 56100, train loss = 0.366883, train accuracy = 0.929688\n",
      "[2018-07-16 14:04:46.475468] Iteration 56200, train loss = 0.383482, train accuracy = 0.906250\n",
      "[2018-07-16 14:04:50.138862] Iteration 56300, train loss = 0.321487, train accuracy = 0.921875\n",
      "[2018-07-16 14:04:53.802512] Iteration 56400, train loss = 0.420484, train accuracy = 0.921875\n",
      "[2018-07-16 14:04:57.503562] Iteration 56500, train loss = 0.405298, train accuracy = 0.867188\n",
      "[2018-07-16 14:05:01.159799] Iteration 56600, train loss = 0.480305, train accuracy = 0.851562\n",
      "[2018-07-16 14:05:04.825013] Iteration 56700, train loss = 0.385838, train accuracy = 0.914062\n",
      "[2018-07-16 14:05:08.468032] Iteration 56800, train loss = 0.343324, train accuracy = 0.929688\n",
      "[2018-07-16 14:05:12.119125] Iteration 56900, train loss = 0.415182, train accuracy = 0.875000\n",
      "[2018-07-16 14:05:15.759136] Iteration 57000, train loss = 0.492241, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.849300\n",
      "[2018-07-16 14:05:20.537783] Iteration 57100, train loss = 0.403675, train accuracy = 0.898438\n",
      "[2018-07-16 14:05:24.180922] Iteration 57200, train loss = 0.383759, train accuracy = 0.882812\n",
      "[2018-07-16 14:05:27.821083] Iteration 57300, train loss = 0.466761, train accuracy = 0.906250\n",
      "[2018-07-16 14:05:31.468016] Iteration 57400, train loss = 0.338679, train accuracy = 0.906250\n",
      "[2018-07-16 14:05:35.127289] Iteration 57500, train loss = 0.368643, train accuracy = 0.882812\n",
      "[2018-07-16 14:05:38.838570] Iteration 57600, train loss = 0.333962, train accuracy = 0.937500\n",
      "[2018-07-16 14:05:42.489473] Iteration 57700, train loss = 0.458774, train accuracy = 0.843750\n",
      "[2018-07-16 14:05:46.120573] Iteration 57800, train loss = 0.441154, train accuracy = 0.882812\n",
      "[2018-07-16 14:05:49.800790] Iteration 57900, train loss = 0.459854, train accuracy = 0.867188\n",
      "[2018-07-16 14:05:53.445396] Iteration 58000, train loss = 0.387106, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.850100\n",
      "[2018-07-16 14:05:58.235212] Iteration 58100, train loss = 0.472386, train accuracy = 0.851562\n",
      "[2018-07-16 14:06:01.872545] Iteration 58200, train loss = 0.431337, train accuracy = 0.859375\n",
      "[2018-07-16 14:06:05.501063] Iteration 58300, train loss = 0.398445, train accuracy = 0.898438\n",
      "[2018-07-16 14:06:09.145243] Iteration 58400, train loss = 0.421982, train accuracy = 0.875000\n",
      "[2018-07-16 14:06:12.786804] Iteration 58500, train loss = 0.473454, train accuracy = 0.875000\n",
      "[2018-07-16 14:06:16.436461] Iteration 58600, train loss = 0.366377, train accuracy = 0.914062\n",
      "[2018-07-16 14:06:20.158833] Iteration 58700, train loss = 0.501860, train accuracy = 0.859375\n",
      "[2018-07-16 14:06:23.814102] Iteration 58800, train loss = 0.395388, train accuracy = 0.890625\n",
      "[2018-07-16 14:06:27.458140] Iteration 58900, train loss = 0.409722, train accuracy = 0.921875\n",
      "[2018-07-16 14:06:31.102420] Iteration 59000, train loss = 0.324776, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.850000\n",
      "[2018-07-16 14:06:35.863934] Iteration 59100, train loss = 0.519579, train accuracy = 0.835938\n",
      "[2018-07-16 14:06:39.515179] Iteration 59200, train loss = 0.454455, train accuracy = 0.843750\n",
      "[2018-07-16 14:06:43.169195] Iteration 59300, train loss = 0.574625, train accuracy = 0.828125\n",
      "[2018-07-16 14:06:46.806779] Iteration 59400, train loss = 0.378769, train accuracy = 0.875000\n",
      "[2018-07-16 14:06:50.439874] Iteration 59500, train loss = 0.479783, train accuracy = 0.875000\n",
      "[2018-07-16 14:06:54.065490] Iteration 59600, train loss = 0.371285, train accuracy = 0.890625\n",
      "[2018-07-16 14:06:57.715251] Iteration 59700, train loss = 0.400212, train accuracy = 0.898438\n",
      "[2018-07-16 14:07:01.439357] Iteration 59800, train loss = 0.569702, train accuracy = 0.859375\n",
      "[2018-07-16 14:07:05.096063] Iteration 59900, train loss = 0.458862, train accuracy = 0.882812\n",
      "[2018-07-16 14:07:08.739331] Iteration 60000, train loss = 0.483683, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.849500\n",
      "[2018-07-16 14:07:13.526249] Iteration 60100, train loss = 0.527828, train accuracy = 0.828125\n",
      "[2018-07-16 14:07:17.180865] Iteration 60200, train loss = 0.427078, train accuracy = 0.804688\n",
      "[2018-07-16 14:07:20.824844] Iteration 60300, train loss = 0.343794, train accuracy = 0.921875\n",
      "[2018-07-16 14:07:24.468721] Iteration 60400, train loss = 0.634876, train accuracy = 0.812500\n",
      "[2018-07-16 14:07:28.136667] Iteration 60500, train loss = 0.567867, train accuracy = 0.804688\n",
      "[2018-07-16 14:07:31.787989] Iteration 60600, train loss = 0.435670, train accuracy = 0.875000\n",
      "[2018-07-16 14:07:35.430994] Iteration 60700, train loss = 0.415268, train accuracy = 0.882812\n",
      "[2018-07-16 14:07:39.072561] Iteration 60800, train loss = 0.321531, train accuracy = 0.929688\n",
      "[2018-07-16 14:07:42.748866] Iteration 60900, train loss = 0.475498, train accuracy = 0.859375\n",
      "[2018-07-16 14:07:46.397409] Iteration 61000, train loss = 0.413388, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.850100\n",
      "[2018-07-16 14:07:51.173232] Iteration 61100, train loss = 0.415064, train accuracy = 0.882812\n",
      "[2018-07-16 14:07:54.838406] Iteration 61200, train loss = 0.460278, train accuracy = 0.835938\n",
      "[2018-07-16 14:07:58.490222] Iteration 61300, train loss = 0.462515, train accuracy = 0.867188\n",
      "[2018-07-16 14:08:02.134857] Iteration 61400, train loss = 0.449435, train accuracy = 0.859375\n",
      "[2018-07-16 14:08:05.777732] Iteration 61500, train loss = 0.327472, train accuracy = 0.914062\n",
      "[2018-07-16 14:08:09.441019] Iteration 61600, train loss = 0.379941, train accuracy = 0.882812\n",
      "[2018-07-16 14:08:13.096863] Iteration 61700, train loss = 0.417547, train accuracy = 0.875000\n",
      "[2018-07-16 14:08:16.748814] Iteration 61800, train loss = 0.386628, train accuracy = 0.898438\n",
      "[2018-07-16 14:08:20.408759] Iteration 61900, train loss = 0.393094, train accuracy = 0.867188\n",
      "[2018-07-16 14:08:24.082083] Iteration 62000, train loss = 0.416618, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.849800\n",
      "[2018-07-16 14:08:28.955348] Iteration 62100, train loss = 0.391827, train accuracy = 0.882812\n",
      "[2018-07-16 14:08:32.600944] Iteration 62200, train loss = 0.531876, train accuracy = 0.820312\n",
      "[2018-07-16 14:08:36.241700] Iteration 62300, train loss = 0.326851, train accuracy = 0.921875\n",
      "[2018-07-16 14:08:39.905939] Iteration 62400, train loss = 0.559546, train accuracy = 0.843750\n",
      "[2018-07-16 14:08:43.564826] Iteration 62500, train loss = 0.372263, train accuracy = 0.921875\n",
      "[2018-07-16 14:08:47.223584] Iteration 62600, train loss = 0.353196, train accuracy = 0.921875\n",
      "[2018-07-16 14:08:50.885007] Iteration 62700, train loss = 0.466056, train accuracy = 0.882812\n",
      "[2018-07-16 14:08:54.549414] Iteration 62800, train loss = 0.392141, train accuracy = 0.921875\n",
      "[2018-07-16 14:08:58.191875] Iteration 62900, train loss = 0.440552, train accuracy = 0.867188\n",
      "[2018-07-16 14:09:01.830603] Iteration 63000, train loss = 0.403003, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.851200\n",
      "[2018-07-16 14:09:06.673889] Iteration 63100, train loss = 0.413946, train accuracy = 0.882812\n",
      "[2018-07-16 14:09:10.336446] Iteration 63200, train loss = 0.342149, train accuracy = 0.890625\n",
      "[2018-07-16 14:09:13.960769] Iteration 63300, train loss = 0.365277, train accuracy = 0.898438\n",
      "[2018-07-16 14:09:17.598391] Iteration 63400, train loss = 0.510823, train accuracy = 0.867188\n",
      "[2018-07-16 14:09:21.242067] Iteration 63500, train loss = 0.408056, train accuracy = 0.890625\n",
      "[2018-07-16 14:09:24.892261] Iteration 63600, train loss = 0.396373, train accuracy = 0.890625\n",
      "[2018-07-16 14:09:28.547895] Iteration 63700, train loss = 0.420871, train accuracy = 0.867188\n",
      "[2018-07-16 14:09:32.183881] Iteration 63800, train loss = 0.370716, train accuracy = 0.867188\n",
      "[2018-07-16 14:09:35.841248] Iteration 63900, train loss = 0.479228, train accuracy = 0.851562\n",
      "[2018-07-16 14:09:39.489889] Iteration 64000, train loss = 0.393744, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.851000\n",
      "[2018-07-16 14:09:44.293539] Iteration 64100, train loss = 0.336360, train accuracy = 0.906250\n",
      "[2018-07-16 14:09:47.948660] Iteration 64200, train loss = 0.496381, train accuracy = 0.859375\n",
      "[2018-07-16 14:09:51.657221] Iteration 64300, train loss = 0.400030, train accuracy = 0.859375\n",
      "[2018-07-16 14:09:55.305942] Iteration 64400, train loss = 0.482115, train accuracy = 0.843750\n",
      "[2018-07-16 14:09:58.937882] Iteration 64500, train loss = 0.471041, train accuracy = 0.867188\n",
      "[2018-07-16 14:10:02.571491] Iteration 64600, train loss = 0.433323, train accuracy = 0.882812\n",
      "[2018-07-16 14:10:06.214025] Iteration 64700, train loss = 0.428829, train accuracy = 0.906250\n",
      "[2018-07-16 14:10:09.851495] Iteration 64800, train loss = 0.416601, train accuracy = 0.828125\n",
      "[2018-07-16 14:10:13.501854] Iteration 64900, train loss = 0.467990, train accuracy = 0.898438\n",
      "[2018-07-16 14:10:17.145430] Iteration 65000, train loss = 0.448238, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.852800\n",
      "[2018-07-16 14:10:21.943392] Iteration 65100, train loss = 0.325926, train accuracy = 0.937500\n",
      "[2018-07-16 14:10:25.602998] Iteration 65200, train loss = 0.343053, train accuracy = 0.898438\n",
      "[2018-07-16 14:10:29.268647] Iteration 65300, train loss = 0.364430, train accuracy = 0.890625\n",
      "[2018-07-16 14:10:33.011197] Iteration 65400, train loss = 0.500640, train accuracy = 0.835938\n",
      "[2018-07-16 14:10:36.658945] Iteration 65500, train loss = 0.445921, train accuracy = 0.843750\n",
      "[2018-07-16 14:10:40.325602] Iteration 65600, train loss = 0.457543, train accuracy = 0.875000\n",
      "[2018-07-16 14:10:43.967942] Iteration 65700, train loss = 0.431082, train accuracy = 0.882812\n",
      "[2018-07-16 14:10:47.600709] Iteration 65800, train loss = 0.323809, train accuracy = 0.914062\n",
      "[2018-07-16 14:10:51.242453] Iteration 65900, train loss = 0.365838, train accuracy = 0.906250\n",
      "[2018-07-16 14:10:54.889846] Iteration 66000, train loss = 0.279108, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.849700\n",
      "[2018-07-16 14:10:59.646337] Iteration 66100, train loss = 0.473885, train accuracy = 0.843750\n",
      "[2018-07-16 14:11:03.294088] Iteration 66200, train loss = 0.410554, train accuracy = 0.890625\n",
      "[2018-07-16 14:11:06.942304] Iteration 66300, train loss = 0.404422, train accuracy = 0.898438\n",
      "[2018-07-16 14:11:10.584979] Iteration 66400, train loss = 0.363102, train accuracy = 0.914062\n",
      "[2018-07-16 14:11:14.332888] Iteration 66500, train loss = 0.416871, train accuracy = 0.890625\n",
      "[2018-07-16 14:11:17.985895] Iteration 66600, train loss = 0.354511, train accuracy = 0.914062\n",
      "[2018-07-16 14:11:21.632736] Iteration 66700, train loss = 0.419746, train accuracy = 0.890625\n",
      "[2018-07-16 14:11:25.280437] Iteration 66800, train loss = 0.449301, train accuracy = 0.867188\n",
      "[2018-07-16 14:11:28.925305] Iteration 66900, train loss = 0.336073, train accuracy = 0.929688\n",
      "[2018-07-16 14:11:32.561947] Iteration 67000, train loss = 0.389328, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.849200\n",
      "[2018-07-16 14:11:37.319081] Iteration 67100, train loss = 0.466684, train accuracy = 0.875000\n",
      "[2018-07-16 14:11:40.958297] Iteration 67200, train loss = 0.371266, train accuracy = 0.906250\n",
      "[2018-07-16 14:11:44.604611] Iteration 67300, train loss = 0.334441, train accuracy = 0.929688\n",
      "[2018-07-16 14:11:48.247384] Iteration 67400, train loss = 0.405778, train accuracy = 0.890625\n",
      "[2018-07-16 14:11:51.892039] Iteration 67500, train loss = 0.484324, train accuracy = 0.820312\n",
      "[2018-07-16 14:11:55.600232] Iteration 67600, train loss = 0.352477, train accuracy = 0.906250\n",
      "[2018-07-16 14:11:59.257033] Iteration 67700, train loss = 0.414804, train accuracy = 0.882812\n",
      "[2018-07-16 14:12:02.893810] Iteration 67800, train loss = 0.438819, train accuracy = 0.875000\n",
      "[2018-07-16 14:12:06.556949] Iteration 67900, train loss = 0.372417, train accuracy = 0.921875\n",
      "[2018-07-16 14:12:10.198413] Iteration 68000, train loss = 0.461325, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.849300\n",
      "[2018-07-16 14:12:14.976587] Iteration 68100, train loss = 0.438973, train accuracy = 0.867188\n",
      "[2018-07-16 14:12:18.617592] Iteration 68200, train loss = 0.413289, train accuracy = 0.890625\n",
      "[2018-07-16 14:12:22.262065] Iteration 68300, train loss = 0.475184, train accuracy = 0.859375\n",
      "[2018-07-16 14:12:25.901258] Iteration 68400, train loss = 0.512591, train accuracy = 0.859375\n",
      "[2018-07-16 14:12:29.535360] Iteration 68500, train loss = 0.395821, train accuracy = 0.898438\n",
      "[2018-07-16 14:12:33.190687] Iteration 68600, train loss = 0.358171, train accuracy = 0.921875\n",
      "[2018-07-16 14:12:36.889593] Iteration 68700, train loss = 0.463699, train accuracy = 0.859375\n",
      "[2018-07-16 14:12:40.541518] Iteration 68800, train loss = 0.404512, train accuracy = 0.898438\n",
      "[2018-07-16 14:12:44.204503] Iteration 68900, train loss = 0.443901, train accuracy = 0.890625\n",
      "[2018-07-16 14:12:47.848998] Iteration 69000, train loss = 0.364608, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.849000\n",
      "[2018-07-16 14:12:52.635284] Iteration 69100, train loss = 0.446388, train accuracy = 0.867188\n",
      "[2018-07-16 14:12:56.281401] Iteration 69200, train loss = 0.478603, train accuracy = 0.843750\n",
      "[2018-07-16 14:12:59.928493] Iteration 69300, train loss = 0.418199, train accuracy = 0.882812\n",
      "[2018-07-16 14:13:03.592205] Iteration 69400, train loss = 0.384696, train accuracy = 0.898438\n",
      "[2018-07-16 14:13:07.243148] Iteration 69500, train loss = 0.349809, train accuracy = 0.906250\n",
      "[2018-07-16 14:13:10.878198] Iteration 69600, train loss = 0.471913, train accuracy = 0.835938\n",
      "[2018-07-16 14:13:14.527338] Iteration 69700, train loss = 0.477396, train accuracy = 0.843750\n",
      "[2018-07-16 14:13:18.175006] Iteration 69800, train loss = 0.467256, train accuracy = 0.867188\n",
      "[2018-07-16 14:13:21.862342] Iteration 69900, train loss = 0.507316, train accuracy = 0.882812\n",
      "[2018-07-16 14:13:25.506897] Iteration 70000, train loss = 0.388483, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.849300\n",
      "[2018-07-16 14:13:30.271317] Iteration 70100, train loss = 0.403903, train accuracy = 0.867188\n",
      "[2018-07-16 14:13:33.907642] Iteration 70200, train loss = 0.496781, train accuracy = 0.851562\n",
      "[2018-07-16 14:13:37.571936] Iteration 70300, train loss = 0.461097, train accuracy = 0.835938\n",
      "[2018-07-16 14:13:41.236679] Iteration 70400, train loss = 0.380125, train accuracy = 0.898438\n",
      "[2018-07-16 14:13:44.882148] Iteration 70500, train loss = 0.508971, train accuracy = 0.867188\n",
      "[2018-07-16 14:13:48.535665] Iteration 70600, train loss = 0.389438, train accuracy = 0.890625\n",
      "[2018-07-16 14:13:52.183708] Iteration 70700, train loss = 0.323568, train accuracy = 0.921875\n",
      "[2018-07-16 14:13:55.844407] Iteration 70800, train loss = 0.406741, train accuracy = 0.875000\n",
      "[2018-07-16 14:13:59.491049] Iteration 70900, train loss = 0.604818, train accuracy = 0.828125\n",
      "[2018-07-16 14:14:03.209971] Iteration 71000, train loss = 0.376162, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.851200\n",
      "[2018-07-16 14:14:07.958521] Iteration 71100, train loss = 0.419920, train accuracy = 0.859375\n",
      "[2018-07-16 14:14:11.610366] Iteration 71200, train loss = 0.285059, train accuracy = 0.960938\n",
      "[2018-07-16 14:14:15.271550] Iteration 71300, train loss = 0.527119, train accuracy = 0.859375\n",
      "[2018-07-16 14:14:18.931485] Iteration 71400, train loss = 0.538324, train accuracy = 0.835938\n",
      "[2018-07-16 14:14:22.587524] Iteration 71500, train loss = 0.290190, train accuracy = 0.953125\n",
      "[2018-07-16 14:14:26.241337] Iteration 71600, train loss = 0.432097, train accuracy = 0.867188\n",
      "[2018-07-16 14:14:29.876964] Iteration 71700, train loss = 0.422626, train accuracy = 0.890625\n",
      "[2018-07-16 14:14:33.541073] Iteration 71800, train loss = 0.373724, train accuracy = 0.882812\n",
      "[2018-07-16 14:14:37.223605] Iteration 71900, train loss = 0.497594, train accuracy = 0.843750\n",
      "[2018-07-16 14:14:40.877563] Iteration 72000, train loss = 0.518905, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.848100\n",
      "[2018-07-16 14:14:45.735635] Iteration 72100, train loss = 0.424276, train accuracy = 0.867188\n",
      "[2018-07-16 14:14:49.368451] Iteration 72200, train loss = 0.531617, train accuracy = 0.890625\n",
      "[2018-07-16 14:14:53.011712] Iteration 72300, train loss = 0.436089, train accuracy = 0.890625\n",
      "[2018-07-16 14:14:56.670117] Iteration 72400, train loss = 0.500980, train accuracy = 0.835938\n",
      "[2018-07-16 14:15:00.314760] Iteration 72500, train loss = 0.390159, train accuracy = 0.875000\n",
      "[2018-07-16 14:15:03.968626] Iteration 72600, train loss = 0.486743, train accuracy = 0.890625\n",
      "[2018-07-16 14:15:07.613112] Iteration 72700, train loss = 0.422820, train accuracy = 0.898438\n",
      "[2018-07-16 14:15:11.276679] Iteration 72800, train loss = 0.397355, train accuracy = 0.921875\n",
      "[2018-07-16 14:15:14.941433] Iteration 72900, train loss = 0.463876, train accuracy = 0.851562\n",
      "[2018-07-16 14:15:18.596489] Iteration 73000, train loss = 0.389105, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.849300\n",
      "[2018-07-16 14:15:23.377349] Iteration 73100, train loss = 0.430760, train accuracy = 0.882812\n",
      "[2018-07-16 14:15:27.094504] Iteration 73200, train loss = 0.407448, train accuracy = 0.875000\n",
      "[2018-07-16 14:15:30.737627] Iteration 73300, train loss = 0.363330, train accuracy = 0.890625\n",
      "[2018-07-16 14:15:34.392032] Iteration 73400, train loss = 0.351593, train accuracy = 0.929688\n",
      "[2018-07-16 14:15:38.052311] Iteration 73500, train loss = 0.465315, train accuracy = 0.890625\n",
      "[2018-07-16 14:15:41.709828] Iteration 73600, train loss = 0.397805, train accuracy = 0.898438\n",
      "[2018-07-16 14:15:45.347672] Iteration 73700, train loss = 0.506923, train accuracy = 0.867188\n",
      "[2018-07-16 14:15:48.980560] Iteration 73800, train loss = 0.351573, train accuracy = 0.906250\n",
      "[2018-07-16 14:15:52.628711] Iteration 73900, train loss = 0.353800, train accuracy = 0.906250\n",
      "[2018-07-16 14:15:56.280750] Iteration 74000, train loss = 0.483061, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.849500\n",
      "[2018-07-16 14:16:01.072721] Iteration 74100, train loss = 0.416593, train accuracy = 0.875000\n",
      "[2018-07-16 14:16:04.722634] Iteration 74200, train loss = 0.423049, train accuracy = 0.875000\n",
      "[2018-07-16 14:16:08.427333] Iteration 74300, train loss = 0.409796, train accuracy = 0.890625\n",
      "[2018-07-16 14:16:12.063670] Iteration 74400, train loss = 0.497723, train accuracy = 0.867188\n",
      "[2018-07-16 14:16:15.712588] Iteration 74500, train loss = 0.351411, train accuracy = 0.921875\n",
      "[2018-07-16 14:16:19.347948] Iteration 74600, train loss = 0.475751, train accuracy = 0.875000\n",
      "[2018-07-16 14:16:22.990733] Iteration 74700, train loss = 0.345613, train accuracy = 0.890625\n",
      "[2018-07-16 14:16:26.624758] Iteration 74800, train loss = 0.352066, train accuracy = 0.914062\n",
      "[2018-07-16 14:16:30.264236] Iteration 74900, train loss = 0.327001, train accuracy = 0.945312\n",
      "[2018-07-16 14:16:33.904856] Iteration 75000, train loss = 0.515645, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.850300\n",
      "[2018-07-16 14:16:38.698355] Iteration 75100, train loss = 0.363966, train accuracy = 0.914062\n",
      "[2018-07-16 14:16:42.343790] Iteration 75200, train loss = 0.393753, train accuracy = 0.898438\n",
      "[2018-07-16 14:16:45.996145] Iteration 75300, train loss = 0.452787, train accuracy = 0.828125\n",
      "[2018-07-16 14:16:49.713477] Iteration 75400, train loss = 0.333923, train accuracy = 0.921875\n",
      "[2018-07-16 14:16:53.386511] Iteration 75500, train loss = 0.418727, train accuracy = 0.875000\n",
      "[2018-07-16 14:16:57.042109] Iteration 75600, train loss = 0.539515, train accuracy = 0.859375\n",
      "[2018-07-16 14:17:00.676026] Iteration 75700, train loss = 0.375342, train accuracy = 0.890625\n",
      "[2018-07-16 14:17:04.317792] Iteration 75800, train loss = 0.398082, train accuracy = 0.875000\n",
      "[2018-07-16 14:17:07.966782] Iteration 75900, train loss = 0.397354, train accuracy = 0.843750\n",
      "[2018-07-16 14:17:11.608268] Iteration 76000, train loss = 0.412902, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.850000\n",
      "[2018-07-16 14:17:16.374399] Iteration 76100, train loss = 0.351140, train accuracy = 0.921875\n",
      "[2018-07-16 14:17:19.999878] Iteration 76200, train loss = 0.413226, train accuracy = 0.898438\n",
      "[2018-07-16 14:17:23.655310] Iteration 76300, train loss = 0.289952, train accuracy = 0.960938\n",
      "[2018-07-16 14:17:27.309522] Iteration 76400, train loss = 0.287578, train accuracy = 0.945312\n",
      "[2018-07-16 14:17:30.991810] Iteration 76500, train loss = 0.390339, train accuracy = 0.890625\n",
      "[2018-07-16 14:17:34.665861] Iteration 76600, train loss = 0.335864, train accuracy = 0.921875\n",
      "[2018-07-16 14:17:38.317435] Iteration 76700, train loss = 0.437724, train accuracy = 0.875000\n",
      "[2018-07-16 14:17:41.971835] Iteration 76800, train loss = 0.514179, train accuracy = 0.859375\n",
      "[2018-07-16 14:17:45.623581] Iteration 76900, train loss = 0.395911, train accuracy = 0.898438\n",
      "[2018-07-16 14:17:49.271005] Iteration 77000, train loss = 0.455166, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.849200\n",
      "[2018-07-16 14:17:54.063199] Iteration 77100, train loss = 0.434037, train accuracy = 0.890625\n",
      "[2018-07-16 14:17:57.698504] Iteration 77200, train loss = 0.420101, train accuracy = 0.890625\n",
      "[2018-07-16 14:18:01.346876] Iteration 77300, train loss = 0.562137, train accuracy = 0.820312\n",
      "[2018-07-16 14:18:04.992109] Iteration 77400, train loss = 0.379244, train accuracy = 0.875000\n",
      "[2018-07-16 14:18:08.640838] Iteration 77500, train loss = 0.458483, train accuracy = 0.875000\n",
      "[2018-07-16 14:18:12.300183] Iteration 77600, train loss = 0.498420, train accuracy = 0.843750\n",
      "[2018-07-16 14:18:16.017080] Iteration 77700, train loss = 0.345464, train accuracy = 0.929688\n",
      "[2018-07-16 14:18:19.671177] Iteration 77800, train loss = 0.492741, train accuracy = 0.843750\n",
      "[2018-07-16 14:18:23.319800] Iteration 77900, train loss = 0.483186, train accuracy = 0.859375\n",
      "[2018-07-16 14:18:26.978238] Iteration 78000, train loss = 0.387041, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.849100\n",
      "[2018-07-16 14:18:31.743945] Iteration 78100, train loss = 0.427370, train accuracy = 0.859375\n",
      "[2018-07-16 14:18:35.394374] Iteration 78200, train loss = 0.329351, train accuracy = 0.929688\n",
      "[2018-07-16 14:18:39.038095] Iteration 78300, train loss = 0.546276, train accuracy = 0.828125\n",
      "[2018-07-16 14:18:42.673636] Iteration 78400, train loss = 0.463561, train accuracy = 0.851562\n",
      "[2018-07-16 14:18:46.307191] Iteration 78500, train loss = 0.400665, train accuracy = 0.882812\n",
      "[2018-07-16 14:18:49.955822] Iteration 78600, train loss = 0.376413, train accuracy = 0.898438\n",
      "[2018-07-16 14:18:53.602101] Iteration 78700, train loss = 0.392259, train accuracy = 0.882812\n",
      "[2018-07-16 14:18:57.297909] Iteration 78800, train loss = 0.365740, train accuracy = 0.906250\n",
      "[2018-07-16 14:19:00.954927] Iteration 78900, train loss = 0.379966, train accuracy = 0.898438\n",
      "[2018-07-16 14:19:04.600853] Iteration 79000, train loss = 0.287193, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.848800\n",
      "[2018-07-16 14:19:09.392624] Iteration 79100, train loss = 0.482981, train accuracy = 0.859375\n",
      "[2018-07-16 14:19:13.026610] Iteration 79200, train loss = 0.450871, train accuracy = 0.890625\n",
      "[2018-07-16 14:19:16.683520] Iteration 79300, train loss = 0.413181, train accuracy = 0.898438\n",
      "[2018-07-16 14:19:20.334030] Iteration 79400, train loss = 0.516449, train accuracy = 0.867188\n",
      "[2018-07-16 14:19:23.977050] Iteration 79500, train loss = 0.389476, train accuracy = 0.882812\n",
      "[2018-07-16 14:19:27.634759] Iteration 79600, train loss = 0.527975, train accuracy = 0.867188\n",
      "[2018-07-16 14:19:31.288090] Iteration 79700, train loss = 0.383506, train accuracy = 0.890625\n",
      "[2018-07-16 14:19:34.920418] Iteration 79800, train loss = 0.404952, train accuracy = 0.906250\n",
      "[2018-07-16 14:19:38.633626] Iteration 79900, train loss = 0.283717, train accuracy = 0.914062\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四轮  量化  \n",
    "prune_rate = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.848800\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.          0.          0.125       0.125      -0.05734529  0.04687182\n",
      "  0.125      -0.25        0.         -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,1)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 14:20:27.726334] Iteration 100, train loss = 2.557323, train accuracy = 0.101562\n",
      "[2018-07-16 14:20:30.780439] Iteration 200, train loss = 2.463882, train accuracy = 0.125000\n",
      "[2018-07-16 14:20:33.814336] Iteration 300, train loss = 2.494437, train accuracy = 0.109375\n",
      "[2018-07-16 14:20:36.859007] Iteration 400, train loss = 2.435233, train accuracy = 0.179688\n",
      "[2018-07-16 14:20:39.915505] Iteration 500, train loss = 2.621666, train accuracy = 0.078125\n",
      "[2018-07-16 14:20:42.952243] Iteration 600, train loss = 2.392482, train accuracy = 0.171875\n",
      "[2018-07-16 14:20:46.011884] Iteration 700, train loss = 2.614270, train accuracy = 0.070312\n",
      "[2018-07-16 14:20:49.048064] Iteration 800, train loss = 2.497260, train accuracy = 0.132812\n",
      "[2018-07-16 14:20:52.109603] Iteration 900, train loss = 2.488502, train accuracy = 0.125000\n",
      "[2018-07-16 14:20:55.157959] Iteration 1000, train loss = 2.468275, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:20:59.320777] Iteration 1100, train loss = 2.566859, train accuracy = 0.117188\n",
      "[2018-07-16 14:21:02.453144] Iteration 1200, train loss = 2.515246, train accuracy = 0.093750\n",
      "[2018-07-16 14:21:05.504375] Iteration 1300, train loss = 2.422663, train accuracy = 0.132812\n",
      "[2018-07-16 14:21:08.547855] Iteration 1400, train loss = 2.562704, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:11.580510] Iteration 1500, train loss = 2.484848, train accuracy = 0.171875\n",
      "[2018-07-16 14:21:14.634463] Iteration 1600, train loss = 2.532229, train accuracy = 0.125000\n",
      "[2018-07-16 14:21:17.688213] Iteration 1700, train loss = 2.548163, train accuracy = 0.140625\n",
      "[2018-07-16 14:21:20.727776] Iteration 1800, train loss = 2.458062, train accuracy = 0.132812\n",
      "[2018-07-16 14:21:23.774146] Iteration 1900, train loss = 2.467201, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:26.821777] Iteration 2000, train loss = 2.469595, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:21:30.989178] Iteration 2100, train loss = 2.449440, train accuracy = 0.132812\n",
      "[2018-07-16 14:21:34.056097] Iteration 2200, train loss = 2.466679, train accuracy = 0.148438\n",
      "[2018-07-16 14:21:37.105288] Iteration 2300, train loss = 2.484965, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:40.153582] Iteration 2400, train loss = 2.442082, train accuracy = 0.132812\n",
      "[2018-07-16 14:21:43.275186] Iteration 2500, train loss = 2.541001, train accuracy = 0.148438\n",
      "[2018-07-16 14:21:46.340040] Iteration 2600, train loss = 2.391131, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:49.400251] Iteration 2700, train loss = 2.440660, train accuracy = 0.140625\n",
      "[2018-07-16 14:21:52.435765] Iteration 2800, train loss = 2.525905, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:55.483331] Iteration 2900, train loss = 2.471583, train accuracy = 0.109375\n",
      "[2018-07-16 14:21:58.524070] Iteration 3000, train loss = 2.452290, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:22:02.703506] Iteration 3100, train loss = 2.404729, train accuracy = 0.148438\n",
      "[2018-07-16 14:22:05.772138] Iteration 3200, train loss = 2.471468, train accuracy = 0.164062\n",
      "[2018-07-16 14:22:08.819913] Iteration 3300, train loss = 2.420968, train accuracy = 0.117188\n",
      "[2018-07-16 14:22:11.863567] Iteration 3400, train loss = 2.476331, train accuracy = 0.148438\n",
      "[2018-07-16 14:22:14.908383] Iteration 3500, train loss = 2.521586, train accuracy = 0.109375\n",
      "[2018-07-16 14:22:17.955941] Iteration 3600, train loss = 2.418369, train accuracy = 0.148438\n",
      "[2018-07-16 14:22:21.006973] Iteration 3700, train loss = 2.482059, train accuracy = 0.171875\n",
      "[2018-07-16 14:22:24.050474] Iteration 3800, train loss = 2.536032, train accuracy = 0.125000\n",
      "[2018-07-16 14:22:27.182180] Iteration 3900, train loss = 2.528999, train accuracy = 0.117188\n",
      "[2018-07-16 14:22:30.236604] Iteration 4000, train loss = 2.493158, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:22:34.408516] Iteration 4100, train loss = 2.483369, train accuracy = 0.109375\n",
      "[2018-07-16 14:22:37.462001] Iteration 4200, train loss = 2.391180, train accuracy = 0.171875\n",
      "[2018-07-16 14:22:40.516074] Iteration 4300, train loss = 2.430859, train accuracy = 0.156250\n",
      "[2018-07-16 14:22:43.575192] Iteration 4400, train loss = 2.466452, train accuracy = 0.164062\n",
      "[2018-07-16 14:22:46.625520] Iteration 4500, train loss = 2.479191, train accuracy = 0.156250\n",
      "[2018-07-16 14:22:49.669526] Iteration 4600, train loss = 2.433836, train accuracy = 0.171875\n",
      "[2018-07-16 14:22:52.719761] Iteration 4700, train loss = 2.391975, train accuracy = 0.156250\n",
      "[2018-07-16 14:22:55.760906] Iteration 4800, train loss = 2.331505, train accuracy = 0.140625\n",
      "[2018-07-16 14:22:58.803194] Iteration 4900, train loss = 2.467055, train accuracy = 0.132812\n",
      "[2018-07-16 14:23:01.860023] Iteration 5000, train loss = 2.544045, train accuracy = 0.062500\n",
      "Evaluating...\n",
      "Test accuracy = 0.143100\n",
      "[2018-07-16 14:23:06.062132] Iteration 5100, train loss = 2.603688, train accuracy = 0.140625\n",
      "[2018-07-16 14:23:09.189184] Iteration 5200, train loss = 2.484690, train accuracy = 0.148438\n",
      "[2018-07-16 14:23:12.243802] Iteration 5300, train loss = 2.532972, train accuracy = 0.132812\n",
      "[2018-07-16 14:23:15.295901] Iteration 5400, train loss = 2.512219, train accuracy = 0.093750\n",
      "[2018-07-16 14:23:18.355348] Iteration 5500, train loss = 2.467918, train accuracy = 0.140625\n",
      "[2018-07-16 14:23:21.424941] Iteration 5600, train loss = 2.484286, train accuracy = 0.109375\n",
      "[2018-07-16 14:23:24.483362] Iteration 5700, train loss = 2.472082, train accuracy = 0.101562\n",
      "[2018-07-16 14:23:27.549563] Iteration 5800, train loss = 2.532442, train accuracy = 0.125000\n",
      "[2018-07-16 14:23:30.619804] Iteration 5900, train loss = 2.524257, train accuracy = 0.109375\n",
      "[2018-07-16 14:23:33.678198] Iteration 6000, train loss = 2.509409, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:23:37.852510] Iteration 6100, train loss = 2.379331, train accuracy = 0.171875\n",
      "[2018-07-16 14:23:40.896704] Iteration 6200, train loss = 2.581411, train accuracy = 0.109375\n",
      "[2018-07-16 14:23:43.942155] Iteration 6300, train loss = 2.444487, train accuracy = 0.093750\n",
      "[2018-07-16 14:23:46.979937] Iteration 6400, train loss = 2.370401, train accuracy = 0.125000\n",
      "[2018-07-16 14:23:50.131066] Iteration 6500, train loss = 2.439758, train accuracy = 0.140625\n",
      "[2018-07-16 14:23:53.175174] Iteration 6600, train loss = 2.548845, train accuracy = 0.078125\n",
      "[2018-07-16 14:23:56.234483] Iteration 6700, train loss = 2.587254, train accuracy = 0.101562\n",
      "[2018-07-16 14:23:59.272010] Iteration 6800, train loss = 2.477111, train accuracy = 0.164062\n",
      "[2018-07-16 14:24:02.336569] Iteration 6900, train loss = 2.340638, train accuracy = 0.195312\n",
      "[2018-07-16 14:24:05.378870] Iteration 7000, train loss = 2.553107, train accuracy = 0.070312\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:24:09.550639] Iteration 7100, train loss = 2.502353, train accuracy = 0.109375\n",
      "[2018-07-16 14:24:12.608424] Iteration 7200, train loss = 2.532954, train accuracy = 0.140625\n",
      "[2018-07-16 14:24:15.653109] Iteration 7300, train loss = 2.468891, train accuracy = 0.148438\n",
      "[2018-07-16 14:24:18.695455] Iteration 7400, train loss = 2.366326, train accuracy = 0.117188\n",
      "[2018-07-16 14:24:21.730272] Iteration 7500, train loss = 2.585092, train accuracy = 0.156250\n",
      "[2018-07-16 14:24:24.755313] Iteration 7600, train loss = 2.468444, train accuracy = 0.078125\n",
      "[2018-07-16 14:24:27.795164] Iteration 7700, train loss = 2.546676, train accuracy = 0.171875\n",
      "[2018-07-16 14:24:30.918054] Iteration 7800, train loss = 2.382333, train accuracy = 0.171875\n",
      "[2018-07-16 14:24:33.950566] Iteration 7900, train loss = 2.481737, train accuracy = 0.125000\n",
      "[2018-07-16 14:24:36.985018] Iteration 8000, train loss = 2.537859, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:24:41.175204] Iteration 8100, train loss = 2.500476, train accuracy = 0.195312\n",
      "[2018-07-16 14:24:44.222927] Iteration 8200, train loss = 2.474138, train accuracy = 0.132812\n",
      "[2018-07-16 14:24:47.269945] Iteration 8300, train loss = 2.437364, train accuracy = 0.109375\n",
      "[2018-07-16 14:24:50.313369] Iteration 8400, train loss = 2.475791, train accuracy = 0.171875\n",
      "[2018-07-16 14:24:53.363509] Iteration 8500, train loss = 2.553129, train accuracy = 0.164062\n",
      "[2018-07-16 14:24:56.424411] Iteration 8600, train loss = 2.538304, train accuracy = 0.148438\n",
      "[2018-07-16 14:24:59.476038] Iteration 8700, train loss = 2.386521, train accuracy = 0.117188\n",
      "[2018-07-16 14:25:02.524051] Iteration 8800, train loss = 2.443502, train accuracy = 0.109375\n",
      "[2018-07-16 14:25:05.569813] Iteration 8900, train loss = 2.500806, train accuracy = 0.148438\n",
      "[2018-07-16 14:25:08.606270] Iteration 9000, train loss = 2.458159, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:25:12.844816] Iteration 9100, train loss = 2.633557, train accuracy = 0.148438\n",
      "[2018-07-16 14:25:15.899760] Iteration 9200, train loss = 2.471293, train accuracy = 0.125000\n",
      "[2018-07-16 14:25:18.947545] Iteration 9300, train loss = 2.440131, train accuracy = 0.156250\n",
      "[2018-07-16 14:25:21.979726] Iteration 9400, train loss = 2.518478, train accuracy = 0.164062\n",
      "[2018-07-16 14:25:25.034653] Iteration 9500, train loss = 2.522111, train accuracy = 0.156250\n",
      "[2018-07-16 14:25:28.086295] Iteration 9600, train loss = 2.587854, train accuracy = 0.093750\n",
      "[2018-07-16 14:25:31.153566] Iteration 9700, train loss = 2.443919, train accuracy = 0.148438\n",
      "[2018-07-16 14:25:34.199424] Iteration 9800, train loss = 2.471486, train accuracy = 0.132812\n",
      "[2018-07-16 14:25:37.242394] Iteration 9900, train loss = 2.511893, train accuracy = 0.171875\n",
      "[2018-07-16 14:25:40.298128] Iteration 10000, train loss = 2.464241, train accuracy = 0.109375\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:25:44.515276] Iteration 10100, train loss = 2.472181, train accuracy = 0.085938\n",
      "[2018-07-16 14:25:47.555972] Iteration 10200, train loss = 2.406757, train accuracy = 0.148438\n",
      "[2018-07-16 14:25:50.621618] Iteration 10300, train loss = 2.486151, train accuracy = 0.140625\n",
      "[2018-07-16 14:25:53.677896] Iteration 10400, train loss = 2.522584, train accuracy = 0.101562\n",
      "[2018-07-16 14:25:56.807723] Iteration 10500, train loss = 2.430670, train accuracy = 0.164062\n",
      "[2018-07-16 14:25:59.851092] Iteration 10600, train loss = 2.482340, train accuracy = 0.132812\n",
      "[2018-07-16 14:26:02.884599] Iteration 10700, train loss = 2.643543, train accuracy = 0.132812\n",
      "[2018-07-16 14:26:05.910153] Iteration 10800, train loss = 2.521571, train accuracy = 0.101562\n",
      "[2018-07-16 14:26:08.943869] Iteration 10900, train loss = 2.494717, train accuracy = 0.109375\n",
      "[2018-07-16 14:26:11.981177] Iteration 11000, train loss = 2.422086, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:26:16.157315] Iteration 11100, train loss = 2.491927, train accuracy = 0.140625\n",
      "[2018-07-16 14:26:19.199443] Iteration 11200, train loss = 2.419031, train accuracy = 0.179688\n",
      "[2018-07-16 14:26:22.255916] Iteration 11300, train loss = 2.597596, train accuracy = 0.093750\n",
      "[2018-07-16 14:26:25.307976] Iteration 11400, train loss = 2.636839, train accuracy = 0.117188\n",
      "[2018-07-16 14:26:28.360240] Iteration 11500, train loss = 2.581488, train accuracy = 0.125000\n",
      "[2018-07-16 14:26:31.412272] Iteration 11600, train loss = 2.536966, train accuracy = 0.125000\n",
      "[2018-07-16 14:26:34.478901] Iteration 11700, train loss = 2.607155, train accuracy = 0.109375\n",
      "[2018-07-16 14:26:37.622076] Iteration 11800, train loss = 2.501896, train accuracy = 0.101562\n",
      "[2018-07-16 14:26:40.668467] Iteration 11900, train loss = 2.486440, train accuracy = 0.125000\n",
      "[2018-07-16 14:26:43.720668] Iteration 12000, train loss = 2.506940, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:26:47.901003] Iteration 12100, train loss = 2.554113, train accuracy = 0.070312\n",
      "[2018-07-16 14:26:50.942247] Iteration 12200, train loss = 2.519058, train accuracy = 0.132812\n",
      "[2018-07-16 14:26:53.994073] Iteration 12300, train loss = 2.488312, train accuracy = 0.101562\n",
      "[2018-07-16 14:26:57.030485] Iteration 12400, train loss = 2.419299, train accuracy = 0.164062\n",
      "[2018-07-16 14:27:00.079167] Iteration 12500, train loss = 2.452453, train accuracy = 0.156250\n",
      "[2018-07-16 14:27:03.121917] Iteration 12600, train loss = 2.435522, train accuracy = 0.179688\n",
      "[2018-07-16 14:27:06.168076] Iteration 12700, train loss = 2.421937, train accuracy = 0.132812\n",
      "[2018-07-16 14:27:09.212503] Iteration 12800, train loss = 2.379729, train accuracy = 0.148438\n",
      "[2018-07-16 14:27:12.258194] Iteration 12900, train loss = 2.542100, train accuracy = 0.078125\n",
      "[2018-07-16 14:27:15.310413] Iteration 13000, train loss = 2.443197, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.142700\n",
      "[2018-07-16 14:27:19.544783] Iteration 13100, train loss = 2.364377, train accuracy = 0.171875\n",
      "[2018-07-16 14:27:22.625760] Iteration 13200, train loss = 2.514179, train accuracy = 0.085938\n",
      "[2018-07-16 14:27:25.669238] Iteration 13300, train loss = 2.453677, train accuracy = 0.132812\n",
      "[2018-07-16 14:27:28.736623] Iteration 13400, train loss = 2.377986, train accuracy = 0.125000\n",
      "[2018-07-16 14:27:31.773817] Iteration 13500, train loss = 2.408849, train accuracy = 0.148438\n",
      "[2018-07-16 14:27:34.804496] Iteration 13600, train loss = 2.432687, train accuracy = 0.171875\n",
      "[2018-07-16 14:27:37.835125] Iteration 13700, train loss = 2.391214, train accuracy = 0.171875\n",
      "[2018-07-16 14:27:40.869250] Iteration 13800, train loss = 2.471770, train accuracy = 0.125000\n",
      "[2018-07-16 14:27:43.915690] Iteration 13900, train loss = 2.496760, train accuracy = 0.085938\n",
      "[2018-07-16 14:27:46.965423] Iteration 14000, train loss = 2.421124, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:27:51.163490] Iteration 14100, train loss = 2.493387, train accuracy = 0.101562\n",
      "[2018-07-16 14:27:54.216486] Iteration 14200, train loss = 2.437096, train accuracy = 0.148438\n",
      "[2018-07-16 14:27:57.265162] Iteration 14300, train loss = 2.506619, train accuracy = 0.132812\n",
      "[2018-07-16 14:28:00.321237] Iteration 14400, train loss = 2.454902, train accuracy = 0.117188\n",
      "[2018-07-16 14:28:03.451276] Iteration 14500, train loss = 2.487174, train accuracy = 0.156250\n",
      "[2018-07-16 14:28:06.496876] Iteration 14600, train loss = 2.442395, train accuracy = 0.101562\n",
      "[2018-07-16 14:28:09.546637] Iteration 14700, train loss = 2.517889, train accuracy = 0.179688\n",
      "[2018-07-16 14:28:12.586858] Iteration 14800, train loss = 2.481824, train accuracy = 0.164062\n",
      "[2018-07-16 14:28:15.635055] Iteration 14900, train loss = 2.417750, train accuracy = 0.132812\n",
      "[2018-07-16 14:28:18.689982] Iteration 15000, train loss = 2.470709, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.141400\n",
      "[2018-07-16 14:28:22.866027] Iteration 15100, train loss = 2.442798, train accuracy = 0.117188\n",
      "[2018-07-16 14:28:25.902225] Iteration 15200, train loss = 2.399424, train accuracy = 0.117188\n",
      "[2018-07-16 14:28:28.953850] Iteration 15300, train loss = 2.463458, train accuracy = 0.101562\n",
      "[2018-07-16 14:28:31.987853] Iteration 15400, train loss = 2.438718, train accuracy = 0.093750\n",
      "[2018-07-16 14:28:35.019877] Iteration 15500, train loss = 2.477851, train accuracy = 0.148438\n",
      "[2018-07-16 14:28:38.069794] Iteration 15600, train loss = 2.464573, train accuracy = 0.101562\n",
      "[2018-07-16 14:28:41.115652] Iteration 15700, train loss = 2.412664, train accuracy = 0.164062\n",
      "[2018-07-16 14:28:44.254298] Iteration 15800, train loss = 2.465775, train accuracy = 0.140625\n",
      "[2018-07-16 14:28:47.298090] Iteration 15900, train loss = 2.482178, train accuracy = 0.148438\n",
      "[2018-07-16 14:28:50.335249] Iteration 16000, train loss = 2.591509, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.142400\n",
      "[2018-07-16 14:28:54.523323] Iteration 16100, train loss = 2.407348, train accuracy = 0.140625\n",
      "[2018-07-16 14:28:57.578510] Iteration 16200, train loss = 2.460529, train accuracy = 0.171875\n",
      "[2018-07-16 14:29:00.617499] Iteration 16300, train loss = 2.538089, train accuracy = 0.101562\n",
      "[2018-07-16 14:29:03.673775] Iteration 16400, train loss = 2.465185, train accuracy = 0.101562\n",
      "[2018-07-16 14:29:06.703967] Iteration 16500, train loss = 2.534568, train accuracy = 0.125000\n",
      "[2018-07-16 14:29:09.735797] Iteration 16600, train loss = 2.478071, train accuracy = 0.148438\n",
      "[2018-07-16 14:29:12.782332] Iteration 16700, train loss = 2.357162, train accuracy = 0.132812\n",
      "[2018-07-16 14:29:15.830687] Iteration 16800, train loss = 2.562586, train accuracy = 0.093750\n",
      "[2018-07-16 14:29:18.874314] Iteration 16900, train loss = 2.481063, train accuracy = 0.117188\n",
      "[2018-07-16 14:29:21.913545] Iteration 17000, train loss = 2.424114, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:29:26.198909] Iteration 17100, train loss = 2.491655, train accuracy = 0.148438\n",
      "[2018-07-16 14:29:29.256507] Iteration 17200, train loss = 2.556929, train accuracy = 0.125000\n",
      "[2018-07-16 14:29:32.290292] Iteration 17300, train loss = 2.488242, train accuracy = 0.164062\n",
      "[2018-07-16 14:29:35.329575] Iteration 17400, train loss = 2.424829, train accuracy = 0.156250\n",
      "[2018-07-16 14:29:38.377504] Iteration 17500, train loss = 2.388727, train accuracy = 0.171875\n",
      "[2018-07-16 14:29:41.405379] Iteration 17600, train loss = 2.597466, train accuracy = 0.156250\n",
      "[2018-07-16 14:29:44.434670] Iteration 17700, train loss = 2.434585, train accuracy = 0.125000\n",
      "[2018-07-16 14:29:47.485613] Iteration 17800, train loss = 2.465587, train accuracy = 0.140625\n",
      "[2018-07-16 14:29:50.540234] Iteration 17900, train loss = 2.524993, train accuracy = 0.078125\n",
      "[2018-07-16 14:29:53.591641] Iteration 18000, train loss = 2.418747, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:29:57.767365] Iteration 18100, train loss = 2.589868, train accuracy = 0.085938\n",
      "[2018-07-16 14:30:00.809983] Iteration 18200, train loss = 2.489144, train accuracy = 0.148438\n",
      "[2018-07-16 14:30:03.845373] Iteration 18300, train loss = 2.480918, train accuracy = 0.085938\n",
      "[2018-07-16 14:30:06.890493] Iteration 18400, train loss = 2.371137, train accuracy = 0.187500\n",
      "[2018-07-16 14:30:10.016567] Iteration 18500, train loss = 2.495212, train accuracy = 0.070312\n",
      "[2018-07-16 14:30:13.076325] Iteration 18600, train loss = 2.369047, train accuracy = 0.148438\n",
      "[2018-07-16 14:30:16.135632] Iteration 18700, train loss = 2.364770, train accuracy = 0.164062\n",
      "[2018-07-16 14:30:19.185888] Iteration 18800, train loss = 2.535886, train accuracy = 0.140625\n",
      "[2018-07-16 14:30:22.225452] Iteration 18900, train loss = 2.413610, train accuracy = 0.101562\n",
      "[2018-07-16 14:30:25.286730] Iteration 19000, train loss = 2.506975, train accuracy = 0.070312\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:30:29.481366] Iteration 19100, train loss = 2.535002, train accuracy = 0.140625\n",
      "[2018-07-16 14:30:32.549902] Iteration 19200, train loss = 2.372562, train accuracy = 0.156250\n",
      "[2018-07-16 14:30:35.584731] Iteration 19300, train loss = 2.396250, train accuracy = 0.148438\n",
      "[2018-07-16 14:30:38.628423] Iteration 19400, train loss = 2.497415, train accuracy = 0.156250\n",
      "[2018-07-16 14:30:41.687016] Iteration 19500, train loss = 2.572669, train accuracy = 0.109375\n",
      "[2018-07-16 14:30:44.730858] Iteration 19600, train loss = 2.468261, train accuracy = 0.132812\n",
      "[2018-07-16 14:30:47.784302] Iteration 19700, train loss = 2.472958, train accuracy = 0.117188\n",
      "[2018-07-16 14:30:50.912287] Iteration 19800, train loss = 2.409708, train accuracy = 0.164062\n",
      "[2018-07-16 14:30:53.940465] Iteration 19900, train loss = 2.519475, train accuracy = 0.101562\n",
      "[2018-07-16 14:30:56.972630] Iteration 20000, train loss = 2.489296, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:31:01.128668] Iteration 20100, train loss = 2.580456, train accuracy = 0.109375\n",
      "[2018-07-16 14:31:04.182965] Iteration 20200, train loss = 2.535075, train accuracy = 0.101562\n",
      "[2018-07-16 14:31:07.252549] Iteration 20300, train loss = 2.406637, train accuracy = 0.164062\n",
      "[2018-07-16 14:31:10.299274] Iteration 20400, train loss = 2.389486, train accuracy = 0.148438\n",
      "[2018-07-16 14:31:13.341675] Iteration 20500, train loss = 2.523238, train accuracy = 0.109375\n",
      "[2018-07-16 14:31:16.380177] Iteration 20600, train loss = 2.348009, train accuracy = 0.140625\n",
      "[2018-07-16 14:31:19.415009] Iteration 20700, train loss = 2.432771, train accuracy = 0.179688\n",
      "[2018-07-16 14:31:22.467298] Iteration 20800, train loss = 2.543055, train accuracy = 0.195312\n",
      "[2018-07-16 14:31:25.500225] Iteration 20900, train loss = 2.478116, train accuracy = 0.171875\n",
      "[2018-07-16 14:31:28.544912] Iteration 21000, train loss = 2.501509, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:31:32.851544] Iteration 21100, train loss = 2.362464, train accuracy = 0.140625\n",
      "[2018-07-16 14:31:35.895568] Iteration 21200, train loss = 2.531299, train accuracy = 0.117188\n",
      "[2018-07-16 14:31:38.949913] Iteration 21300, train loss = 2.398178, train accuracy = 0.187500\n",
      "[2018-07-16 14:31:41.981362] Iteration 21400, train loss = 2.413260, train accuracy = 0.132812\n",
      "[2018-07-16 14:31:45.014724] Iteration 21500, train loss = 2.458672, train accuracy = 0.117188\n",
      "[2018-07-16 14:31:48.073844] Iteration 21600, train loss = 2.398212, train accuracy = 0.164062\n",
      "[2018-07-16 14:31:51.132105] Iteration 21700, train loss = 2.527730, train accuracy = 0.125000\n",
      "[2018-07-16 14:31:54.194520] Iteration 21800, train loss = 2.459810, train accuracy = 0.117188\n",
      "[2018-07-16 14:31:57.270660] Iteration 21900, train loss = 2.424131, train accuracy = 0.109375\n",
      "[2018-07-16 14:32:00.317272] Iteration 22000, train loss = 2.555100, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.142400\n",
      "[2018-07-16 14:32:04.489666] Iteration 22100, train loss = 2.628482, train accuracy = 0.101562\n",
      "[2018-07-16 14:32:07.529762] Iteration 22200, train loss = 2.554380, train accuracy = 0.093750\n",
      "[2018-07-16 14:32:10.576879] Iteration 22300, train loss = 2.450320, train accuracy = 0.132812\n",
      "[2018-07-16 14:32:13.628656] Iteration 22400, train loss = 2.463646, train accuracy = 0.085938\n",
      "[2018-07-16 14:32:16.753037] Iteration 22500, train loss = 2.554977, train accuracy = 0.125000\n",
      "[2018-07-16 14:32:19.773460] Iteration 22600, train loss = 2.440175, train accuracy = 0.156250\n",
      "[2018-07-16 14:32:22.793076] Iteration 22700, train loss = 2.407747, train accuracy = 0.125000\n",
      "[2018-07-16 14:32:25.818406] Iteration 22800, train loss = 2.389873, train accuracy = 0.148438\n",
      "[2018-07-16 14:32:28.841400] Iteration 22900, train loss = 2.393285, train accuracy = 0.125000\n",
      "[2018-07-16 14:32:31.863817] Iteration 23000, train loss = 2.473804, train accuracy = 0.101562\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:32:35.927879] Iteration 23100, train loss = 2.406946, train accuracy = 0.148438\n",
      "[2018-07-16 14:32:38.948099] Iteration 23200, train loss = 2.447544, train accuracy = 0.164062\n",
      "[2018-07-16 14:32:41.969668] Iteration 23300, train loss = 2.553849, train accuracy = 0.148438\n",
      "[2018-07-16 14:32:44.995217] Iteration 23400, train loss = 2.517608, train accuracy = 0.164062\n",
      "[2018-07-16 14:32:48.014587] Iteration 23500, train loss = 2.446799, train accuracy = 0.171875\n",
      "[2018-07-16 14:32:51.043718] Iteration 23600, train loss = 2.564132, train accuracy = 0.093750\n",
      "[2018-07-16 14:32:54.068836] Iteration 23700, train loss = 2.513693, train accuracy = 0.132812\n",
      "[2018-07-16 14:32:57.124697] Iteration 23800, train loss = 2.520918, train accuracy = 0.132812\n",
      "[2018-07-16 14:33:00.169583] Iteration 23900, train loss = 2.440847, train accuracy = 0.132812\n",
      "[2018-07-16 14:33:03.232312] Iteration 24000, train loss = 2.423955, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:33:07.460778] Iteration 24100, train loss = 2.593908, train accuracy = 0.078125\n",
      "[2018-07-16 14:33:10.539893] Iteration 24200, train loss = 2.613446, train accuracy = 0.085938\n",
      "[2018-07-16 14:33:13.601690] Iteration 24300, train loss = 2.417378, train accuracy = 0.171875\n",
      "[2018-07-16 14:33:16.678966] Iteration 24400, train loss = 2.428161, train accuracy = 0.156250\n",
      "[2018-07-16 14:33:19.738808] Iteration 24500, train loss = 2.458035, train accuracy = 0.125000\n",
      "[2018-07-16 14:33:22.808522] Iteration 24600, train loss = 2.570987, train accuracy = 0.164062\n",
      "[2018-07-16 14:33:25.877948] Iteration 24700, train loss = 2.400262, train accuracy = 0.140625\n",
      "[2018-07-16 14:33:28.995791] Iteration 24800, train loss = 2.430880, train accuracy = 0.148438\n",
      "[2018-07-16 14:33:32.144573] Iteration 24900, train loss = 2.362731, train accuracy = 0.148438\n",
      "[2018-07-16 14:33:35.242616] Iteration 25000, train loss = 2.507582, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:33:39.451861] Iteration 25100, train loss = 2.445829, train accuracy = 0.125000\n",
      "[2018-07-16 14:33:42.531870] Iteration 25200, train loss = 2.563457, train accuracy = 0.070312\n",
      "[2018-07-16 14:33:45.620907] Iteration 25300, train loss = 2.355399, train accuracy = 0.171875\n",
      "[2018-07-16 14:33:48.686985] Iteration 25400, train loss = 2.550913, train accuracy = 0.070312\n",
      "[2018-07-16 14:33:51.773001] Iteration 25500, train loss = 2.622439, train accuracy = 0.078125\n",
      "[2018-07-16 14:33:54.856068] Iteration 25600, train loss = 2.469586, train accuracy = 0.132812\n",
      "[2018-07-16 14:33:57.925229] Iteration 25700, train loss = 2.476655, train accuracy = 0.156250\n",
      "[2018-07-16 14:34:00.997522] Iteration 25800, train loss = 2.522219, train accuracy = 0.093750\n",
      "[2018-07-16 14:34:04.143522] Iteration 25900, train loss = 2.527126, train accuracy = 0.093750\n",
      "[2018-07-16 14:34:07.213947] Iteration 26000, train loss = 2.412648, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:34:11.421777] Iteration 26100, train loss = 2.503330, train accuracy = 0.125000\n",
      "[2018-07-16 14:34:14.488409] Iteration 26200, train loss = 2.496970, train accuracy = 0.101562\n",
      "[2018-07-16 14:34:17.570429] Iteration 26300, train loss = 2.612114, train accuracy = 0.101562\n",
      "[2018-07-16 14:34:20.637128] Iteration 26400, train loss = 2.450924, train accuracy = 0.132812\n",
      "[2018-07-16 14:34:23.720897] Iteration 26500, train loss = 2.590525, train accuracy = 0.101562\n",
      "[2018-07-16 14:34:26.784483] Iteration 26600, train loss = 2.546628, train accuracy = 0.093750\n",
      "[2018-07-16 14:34:29.883942] Iteration 26700, train loss = 2.595513, train accuracy = 0.093750\n",
      "[2018-07-16 14:34:32.969108] Iteration 26800, train loss = 2.409916, train accuracy = 0.132812\n",
      "[2018-07-16 14:34:36.023881] Iteration 26900, train loss = 2.553212, train accuracy = 0.093750\n",
      "[2018-07-16 14:34:39.166428] Iteration 27000, train loss = 2.619430, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:34:43.374021] Iteration 27100, train loss = 2.416290, train accuracy = 0.156250\n",
      "[2018-07-16 14:34:46.451792] Iteration 27200, train loss = 2.401572, train accuracy = 0.125000\n",
      "[2018-07-16 14:34:49.519616] Iteration 27300, train loss = 2.514301, train accuracy = 0.148438\n",
      "[2018-07-16 14:34:52.583413] Iteration 27400, train loss = 2.598006, train accuracy = 0.101562\n",
      "[2018-07-16 14:34:55.640835] Iteration 27500, train loss = 2.325845, train accuracy = 0.187500\n",
      "[2018-07-16 14:34:58.720702] Iteration 27600, train loss = 2.470963, train accuracy = 0.117188\n",
      "[2018-07-16 14:35:01.810326] Iteration 27700, train loss = 2.332960, train accuracy = 0.187500\n",
      "[2018-07-16 14:35:04.888261] Iteration 27800, train loss = 2.451449, train accuracy = 0.179688\n",
      "[2018-07-16 14:35:07.996572] Iteration 27900, train loss = 2.517311, train accuracy = 0.109375\n",
      "[2018-07-16 14:35:11.057088] Iteration 28000, train loss = 2.514785, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:35:15.366866] Iteration 28100, train loss = 2.589453, train accuracy = 0.117188\n",
      "[2018-07-16 14:35:18.450811] Iteration 28200, train loss = 2.465346, train accuracy = 0.117188\n",
      "[2018-07-16 14:35:21.529290] Iteration 28300, train loss = 2.440905, train accuracy = 0.140625\n",
      "[2018-07-16 14:35:24.618411] Iteration 28400, train loss = 2.579771, train accuracy = 0.109375\n",
      "[2018-07-16 14:35:27.713422] Iteration 28500, train loss = 2.419307, train accuracy = 0.117188\n",
      "[2018-07-16 14:35:30.797869] Iteration 28600, train loss = 2.429203, train accuracy = 0.164062\n",
      "[2018-07-16 14:35:33.874546] Iteration 28700, train loss = 2.531300, train accuracy = 0.117188\n",
      "[2018-07-16 14:35:36.952959] Iteration 28800, train loss = 2.426055, train accuracy = 0.179688\n",
      "[2018-07-16 14:35:40.035987] Iteration 28900, train loss = 2.434539, train accuracy = 0.140625\n",
      "[2018-07-16 14:35:43.104847] Iteration 29000, train loss = 2.382182, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.142200\n",
      "[2018-07-16 14:35:47.309486] Iteration 29100, train loss = 2.482237, train accuracy = 0.109375\n",
      "[2018-07-16 14:35:50.465164] Iteration 29200, train loss = 2.433997, train accuracy = 0.148438\n",
      "[2018-07-16 14:35:53.565715] Iteration 29300, train loss = 2.454475, train accuracy = 0.179688\n",
      "[2018-07-16 14:35:56.631371] Iteration 29400, train loss = 2.457376, train accuracy = 0.132812\n",
      "[2018-07-16 14:35:59.724831] Iteration 29500, train loss = 2.439294, train accuracy = 0.187500\n",
      "[2018-07-16 14:36:02.792716] Iteration 29600, train loss = 2.503666, train accuracy = 0.109375\n",
      "[2018-07-16 14:36:05.857514] Iteration 29700, train loss = 2.479762, train accuracy = 0.140625\n",
      "[2018-07-16 14:36:08.921660] Iteration 29800, train loss = 2.543457, train accuracy = 0.085938\n",
      "[2018-07-16 14:36:11.983776] Iteration 29900, train loss = 2.563738, train accuracy = 0.125000\n",
      "[2018-07-16 14:36:15.056860] Iteration 30000, train loss = 2.436521, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 14:36:19.268928] Iteration 30100, train loss = 2.552552, train accuracy = 0.140625\n",
      "[2018-07-16 14:36:22.323718] Iteration 30200, train loss = 2.590131, train accuracy = 0.117188\n",
      "[2018-07-16 14:36:25.445667] Iteration 30300, train loss = 2.436124, train accuracy = 0.101562\n",
      "[2018-07-16 14:36:28.526677] Iteration 30400, train loss = 2.543408, train accuracy = 0.132812\n",
      "[2018-07-16 14:36:31.586720] Iteration 30500, train loss = 2.378115, train accuracy = 0.156250\n",
      "[2018-07-16 14:36:34.666090] Iteration 30600, train loss = 2.401603, train accuracy = 0.125000\n",
      "[2018-07-16 14:36:37.744568] Iteration 30700, train loss = 2.530139, train accuracy = 0.148438\n",
      "[2018-07-16 14:36:40.846171] Iteration 30800, train loss = 2.434385, train accuracy = 0.125000\n",
      "[2018-07-16 14:36:43.933214] Iteration 30900, train loss = 2.509451, train accuracy = 0.117188\n",
      "[2018-07-16 14:36:47.012280] Iteration 31000, train loss = 2.513902, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:36:51.233557] Iteration 31100, train loss = 2.461759, train accuracy = 0.132812\n",
      "[2018-07-16 14:36:54.306081] Iteration 31200, train loss = 2.392181, train accuracy = 0.164062\n",
      "[2018-07-16 14:36:57.369361] Iteration 31300, train loss = 2.367428, train accuracy = 0.140625\n",
      "[2018-07-16 14:37:00.551863] Iteration 31400, train loss = 2.567703, train accuracy = 0.140625\n",
      "[2018-07-16 14:37:03.618733] Iteration 31500, train loss = 2.533760, train accuracy = 0.078125\n",
      "[2018-07-16 14:37:06.680592] Iteration 31600, train loss = 2.379294, train accuracy = 0.187500\n",
      "[2018-07-16 14:37:09.742471] Iteration 31700, train loss = 2.499322, train accuracy = 0.156250\n",
      "[2018-07-16 14:37:12.804877] Iteration 31800, train loss = 2.515533, train accuracy = 0.117188\n",
      "[2018-07-16 14:37:15.865663] Iteration 31900, train loss = 2.413974, train accuracy = 0.109375\n",
      "[2018-07-16 14:37:18.929187] Iteration 32000, train loss = 2.430726, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:37:23.119115] Iteration 32100, train loss = 2.585819, train accuracy = 0.132812\n",
      "[2018-07-16 14:37:26.191356] Iteration 32200, train loss = 2.421087, train accuracy = 0.148438\n",
      "[2018-07-16 14:37:29.260409] Iteration 32300, train loss = 2.470197, train accuracy = 0.101562\n",
      "[2018-07-16 14:37:32.341982] Iteration 32400, train loss = 2.484405, train accuracy = 0.132812\n",
      "[2018-07-16 14:37:35.488654] Iteration 32500, train loss = 2.494802, train accuracy = 0.132812\n",
      "[2018-07-16 14:37:38.569630] Iteration 32600, train loss = 2.406177, train accuracy = 0.117188\n",
      "[2018-07-16 14:37:41.658150] Iteration 32700, train loss = 2.533911, train accuracy = 0.140625\n",
      "[2018-07-16 14:37:44.728137] Iteration 32800, train loss = 2.398674, train accuracy = 0.117188\n",
      "[2018-07-16 14:37:47.799543] Iteration 32900, train loss = 2.514940, train accuracy = 0.140625\n",
      "[2018-07-16 14:37:50.865093] Iteration 33000, train loss = 2.432991, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:37:55.064005] Iteration 33100, train loss = 2.386428, train accuracy = 0.156250\n",
      "[2018-07-16 14:37:58.133890] Iteration 33200, train loss = 2.592464, train accuracy = 0.132812\n",
      "[2018-07-16 14:38:01.199158] Iteration 33300, train loss = 2.522323, train accuracy = 0.156250\n",
      "[2018-07-16 14:38:04.265279] Iteration 33400, train loss = 2.406159, train accuracy = 0.164062\n",
      "[2018-07-16 14:38:07.333208] Iteration 33500, train loss = 2.381193, train accuracy = 0.187500\n",
      "[2018-07-16 14:38:10.472506] Iteration 33600, train loss = 2.390201, train accuracy = 0.156250\n",
      "[2018-07-16 14:38:13.547716] Iteration 33700, train loss = 2.492058, train accuracy = 0.132812\n",
      "[2018-07-16 14:38:16.634616] Iteration 33800, train loss = 2.439500, train accuracy = 0.117188\n",
      "[2018-07-16 14:38:19.704488] Iteration 33900, train loss = 2.548128, train accuracy = 0.078125\n",
      "[2018-07-16 14:38:22.793718] Iteration 34000, train loss = 2.316584, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.142300\n",
      "[2018-07-16 14:38:27.034113] Iteration 34100, train loss = 2.434748, train accuracy = 0.195312\n",
      "[2018-07-16 14:38:30.095979] Iteration 34200, train loss = 2.559751, train accuracy = 0.117188\n",
      "[2018-07-16 14:38:33.154668] Iteration 34300, train loss = 2.565497, train accuracy = 0.125000\n",
      "[2018-07-16 14:38:36.242257] Iteration 34400, train loss = 2.478926, train accuracy = 0.156250\n",
      "[2018-07-16 14:38:39.319774] Iteration 34500, train loss = 2.402821, train accuracy = 0.171875\n",
      "[2018-07-16 14:38:42.396312] Iteration 34600, train loss = 2.387938, train accuracy = 0.195312\n",
      "[2018-07-16 14:38:45.527995] Iteration 34700, train loss = 2.472835, train accuracy = 0.132812\n",
      "[2018-07-16 14:38:48.586754] Iteration 34800, train loss = 2.490785, train accuracy = 0.117188\n",
      "[2018-07-16 14:38:51.669506] Iteration 34900, train loss = 2.414969, train accuracy = 0.109375\n",
      "[2018-07-16 14:38:54.732069] Iteration 35000, train loss = 2.553946, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:38:58.948686] Iteration 35100, train loss = 2.543429, train accuracy = 0.132812\n",
      "[2018-07-16 14:39:02.021295] Iteration 35200, train loss = 2.509085, train accuracy = 0.132812\n",
      "[2018-07-16 14:39:05.110060] Iteration 35300, train loss = 2.447010, train accuracy = 0.148438\n",
      "[2018-07-16 14:39:08.192525] Iteration 35400, train loss = 2.417227, train accuracy = 0.179688\n",
      "[2018-07-16 14:39:11.269029] Iteration 35500, train loss = 2.427204, train accuracy = 0.179688\n",
      "[2018-07-16 14:39:14.345721] Iteration 35600, train loss = 2.415890, train accuracy = 0.109375\n",
      "[2018-07-16 14:39:17.433573] Iteration 35700, train loss = 2.404382, train accuracy = 0.148438\n",
      "[2018-07-16 14:39:20.585883] Iteration 35800, train loss = 2.456240, train accuracy = 0.125000\n",
      "[2018-07-16 14:39:23.660361] Iteration 35900, train loss = 2.540076, train accuracy = 0.132812\n",
      "[2018-07-16 14:39:26.735287] Iteration 36000, train loss = 2.527876, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.142300\n",
      "[2018-07-16 14:39:30.955995] Iteration 36100, train loss = 2.489691, train accuracy = 0.156250\n",
      "[2018-07-16 14:39:34.051143] Iteration 36200, train loss = 2.437746, train accuracy = 0.140625\n",
      "[2018-07-16 14:39:37.115615] Iteration 36300, train loss = 2.676363, train accuracy = 0.117188\n",
      "[2018-07-16 14:39:40.174807] Iteration 36400, train loss = 2.506473, train accuracy = 0.164062\n",
      "[2018-07-16 14:39:43.245585] Iteration 36500, train loss = 2.489765, train accuracy = 0.179688\n",
      "[2018-07-16 14:39:46.315562] Iteration 36600, train loss = 2.469273, train accuracy = 0.140625\n",
      "[2018-07-16 14:39:49.378577] Iteration 36700, train loss = 2.549016, train accuracy = 0.070312\n",
      "[2018-07-16 14:39:52.455446] Iteration 36800, train loss = 2.474183, train accuracy = 0.117188\n",
      "[2018-07-16 14:39:55.624459] Iteration 36900, train loss = 2.535994, train accuracy = 0.156250\n",
      "[2018-07-16 14:39:58.702073] Iteration 37000, train loss = 2.528442, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:40:02.909342] Iteration 37100, train loss = 2.361995, train accuracy = 0.156250\n",
      "[2018-07-16 14:40:05.982255] Iteration 37200, train loss = 2.667022, train accuracy = 0.117188\n",
      "[2018-07-16 14:40:09.054102] Iteration 37300, train loss = 2.486173, train accuracy = 0.148438\n",
      "[2018-07-16 14:40:12.117657] Iteration 37400, train loss = 2.520899, train accuracy = 0.132812\n",
      "[2018-07-16 14:40:15.214563] Iteration 37500, train loss = 2.385278, train accuracy = 0.164062\n",
      "[2018-07-16 14:40:18.289571] Iteration 37600, train loss = 2.599908, train accuracy = 0.093750\n",
      "[2018-07-16 14:40:21.347884] Iteration 37700, train loss = 2.520715, train accuracy = 0.125000\n",
      "[2018-07-16 14:40:24.433774] Iteration 37800, train loss = 2.432090, train accuracy = 0.195312\n",
      "[2018-07-16 14:40:27.507602] Iteration 37900, train loss = 2.560917, train accuracy = 0.117188\n",
      "[2018-07-16 14:40:30.635535] Iteration 38000, train loss = 2.529675, train accuracy = 0.085938\n",
      "Evaluating...\n",
      "Test accuracy = 0.141000\n",
      "[2018-07-16 14:40:34.850517] Iteration 38100, train loss = 2.446735, train accuracy = 0.109375\n",
      "[2018-07-16 14:40:37.921340] Iteration 38200, train loss = 2.570714, train accuracy = 0.109375\n",
      "[2018-07-16 14:40:40.999838] Iteration 38300, train loss = 2.428513, train accuracy = 0.164062\n",
      "[2018-07-16 14:40:44.075705] Iteration 38400, train loss = 2.489957, train accuracy = 0.125000\n",
      "[2018-07-16 14:40:47.137812] Iteration 38500, train loss = 2.510687, train accuracy = 0.132812\n",
      "[2018-07-16 14:40:50.239789] Iteration 38600, train loss = 2.443048, train accuracy = 0.109375\n",
      "[2018-07-16 14:40:53.321625] Iteration 38700, train loss = 2.496117, train accuracy = 0.078125\n",
      "[2018-07-16 14:40:56.409493] Iteration 38800, train loss = 2.361044, train accuracy = 0.156250\n",
      "[2018-07-16 14:40:59.472008] Iteration 38900, train loss = 2.408619, train accuracy = 0.164062\n",
      "[2018-07-16 14:41:02.571587] Iteration 39000, train loss = 2.348594, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:41:06.923224] Iteration 39100, train loss = 2.332342, train accuracy = 0.179688\n",
      "[2018-07-16 14:41:09.987689] Iteration 39200, train loss = 2.455078, train accuracy = 0.125000\n",
      "[2018-07-16 14:41:13.047567] Iteration 39300, train loss = 2.553276, train accuracy = 0.117188\n",
      "[2018-07-16 14:41:16.108045] Iteration 39400, train loss = 2.555823, train accuracy = 0.109375\n",
      "[2018-07-16 14:41:19.162598] Iteration 39500, train loss = 2.512131, train accuracy = 0.109375\n",
      "[2018-07-16 14:41:22.235752] Iteration 39600, train loss = 2.434202, train accuracy = 0.117188\n",
      "[2018-07-16 14:41:25.311300] Iteration 39700, train loss = 2.569620, train accuracy = 0.148438\n",
      "[2018-07-16 14:41:28.404077] Iteration 39800, train loss = 2.380540, train accuracy = 0.187500\n",
      "[2018-07-16 14:41:31.505904] Iteration 39900, train loss = 2.458827, train accuracy = 0.132812\n",
      "[2018-07-16 14:41:34.590848] Iteration 40000, train loss = 2.452572, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:41:38.931428] Iteration 40100, train loss = 2.456029, train accuracy = 0.109375\n",
      "[2018-07-16 14:41:42.021444] Iteration 40200, train loss = 2.597201, train accuracy = 0.085938\n",
      "[2018-07-16 14:41:45.091580] Iteration 40300, train loss = 2.384945, train accuracy = 0.164062\n",
      "[2018-07-16 14:41:48.174413] Iteration 40400, train loss = 2.426747, train accuracy = 0.125000\n",
      "[2018-07-16 14:41:51.266472] Iteration 40500, train loss = 2.468889, train accuracy = 0.164062\n",
      "[2018-07-16 14:41:54.349707] Iteration 40600, train loss = 2.415581, train accuracy = 0.132812\n",
      "[2018-07-16 14:41:57.410567] Iteration 40700, train loss = 2.470605, train accuracy = 0.093750\n",
      "[2018-07-16 14:42:00.473323] Iteration 40800, train loss = 2.566268, train accuracy = 0.101562\n",
      "[2018-07-16 14:42:03.539787] Iteration 40900, train loss = 2.443080, train accuracy = 0.132812\n",
      "[2018-07-16 14:42:06.603129] Iteration 41000, train loss = 2.616010, train accuracy = 0.101562\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:42:10.820802] Iteration 41100, train loss = 2.437902, train accuracy = 0.156250\n",
      "[2018-07-16 14:42:13.990303] Iteration 41200, train loss = 2.447561, train accuracy = 0.117188\n",
      "[2018-07-16 14:42:17.081423] Iteration 41300, train loss = 2.524836, train accuracy = 0.101562\n",
      "[2018-07-16 14:42:20.169353] Iteration 41400, train loss = 2.473338, train accuracy = 0.125000\n",
      "[2018-07-16 14:42:23.240699] Iteration 41500, train loss = 2.494626, train accuracy = 0.140625\n",
      "[2018-07-16 14:42:26.315883] Iteration 41600, train loss = 2.450897, train accuracy = 0.148438\n",
      "[2018-07-16 14:42:29.378704] Iteration 41700, train loss = 2.485369, train accuracy = 0.140625\n",
      "[2018-07-16 14:42:32.445995] Iteration 41800, train loss = 2.414159, train accuracy = 0.140625\n",
      "[2018-07-16 14:42:35.531973] Iteration 41900, train loss = 2.381981, train accuracy = 0.148438\n",
      "[2018-07-16 14:42:38.597513] Iteration 42000, train loss = 2.429283, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:42:42.814530] Iteration 42100, train loss = 2.491196, train accuracy = 0.085938\n",
      "[2018-07-16 14:42:45.879340] Iteration 42200, train loss = 2.612323, train accuracy = 0.109375\n",
      "[2018-07-16 14:42:49.037373] Iteration 42300, train loss = 2.378652, train accuracy = 0.125000\n",
      "[2018-07-16 14:42:52.089923] Iteration 42400, train loss = 2.558031, train accuracy = 0.085938\n",
      "[2018-07-16 14:42:55.159820] Iteration 42500, train loss = 2.451186, train accuracy = 0.078125\n",
      "[2018-07-16 14:42:58.222522] Iteration 42600, train loss = 2.528897, train accuracy = 0.125000\n",
      "[2018-07-16 14:43:01.300641] Iteration 42700, train loss = 2.548874, train accuracy = 0.117188\n",
      "[2018-07-16 14:43:04.392867] Iteration 42800, train loss = 2.499346, train accuracy = 0.148438\n",
      "[2018-07-16 14:43:07.472678] Iteration 42900, train loss = 2.422550, train accuracy = 0.132812\n",
      "[2018-07-16 14:43:10.548198] Iteration 43000, train loss = 2.489030, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:43:14.814365] Iteration 43100, train loss = 2.455920, train accuracy = 0.093750\n",
      "[2018-07-16 14:43:17.896020] Iteration 43200, train loss = 2.441546, train accuracy = 0.171875\n",
      "[2018-07-16 14:43:20.986365] Iteration 43300, train loss = 2.364520, train accuracy = 0.140625\n",
      "[2018-07-16 14:43:24.148952] Iteration 43400, train loss = 2.483888, train accuracy = 0.101562\n",
      "[2018-07-16 14:43:27.204517] Iteration 43500, train loss = 2.464991, train accuracy = 0.093750\n",
      "[2018-07-16 14:43:30.269223] Iteration 43600, train loss = 2.533679, train accuracy = 0.101562\n",
      "[2018-07-16 14:43:33.343794] Iteration 43700, train loss = 2.422593, train accuracy = 0.132812\n",
      "[2018-07-16 14:43:36.408241] Iteration 43800, train loss = 2.561670, train accuracy = 0.109375\n",
      "[2018-07-16 14:43:39.487349] Iteration 43900, train loss = 2.377202, train accuracy = 0.140625\n",
      "[2018-07-16 14:43:42.546136] Iteration 44000, train loss = 2.377954, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.141400\n",
      "[2018-07-16 14:43:46.757388] Iteration 44100, train loss = 2.485580, train accuracy = 0.078125\n",
      "[2018-07-16 14:43:49.854750] Iteration 44200, train loss = 2.504707, train accuracy = 0.117188\n",
      "[2018-07-16 14:43:52.926780] Iteration 44300, train loss = 2.556183, train accuracy = 0.132812\n",
      "[2018-07-16 14:43:56.009144] Iteration 44400, train loss = 2.458510, train accuracy = 0.117188\n",
      "[2018-07-16 14:43:59.161943] Iteration 44500, train loss = 2.514413, train accuracy = 0.101562\n",
      "[2018-07-16 14:44:02.235782] Iteration 44600, train loss = 2.411444, train accuracy = 0.140625\n",
      "[2018-07-16 14:44:05.308601] Iteration 44700, train loss = 2.475806, train accuracy = 0.140625\n",
      "[2018-07-16 14:44:08.405943] Iteration 44800, train loss = 2.488643, train accuracy = 0.109375\n",
      "[2018-07-16 14:44:11.475139] Iteration 44900, train loss = 2.464545, train accuracy = 0.148438\n",
      "[2018-07-16 14:44:14.539833] Iteration 45000, train loss = 2.536879, train accuracy = 0.101562\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:44:18.749895] Iteration 45100, train loss = 2.440200, train accuracy = 0.195312\n",
      "[2018-07-16 14:44:21.795657] Iteration 45200, train loss = 2.418760, train accuracy = 0.156250\n",
      "[2018-07-16 14:44:24.859337] Iteration 45300, train loss = 2.508649, train accuracy = 0.093750\n",
      "[2018-07-16 14:44:27.930862] Iteration 45400, train loss = 2.433712, train accuracy = 0.148438\n",
      "[2018-07-16 14:44:31.016824] Iteration 45500, train loss = 2.475878, train accuracy = 0.132812\n",
      "[2018-07-16 14:44:34.167841] Iteration 45600, train loss = 2.535780, train accuracy = 0.109375\n",
      "[2018-07-16 14:44:37.240753] Iteration 45700, train loss = 2.514747, train accuracy = 0.132812\n",
      "[2018-07-16 14:44:40.325857] Iteration 45800, train loss = 2.421048, train accuracy = 0.109375\n",
      "[2018-07-16 14:44:43.407991] Iteration 45900, train loss = 2.495698, train accuracy = 0.117188\n",
      "[2018-07-16 14:44:46.507533] Iteration 46000, train loss = 2.444008, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:44:50.762374] Iteration 46100, train loss = 2.442119, train accuracy = 0.218750\n",
      "[2018-07-16 14:44:53.837516] Iteration 46200, train loss = 2.372329, train accuracy = 0.171875\n",
      "[2018-07-16 14:44:56.912266] Iteration 46300, train loss = 2.446994, train accuracy = 0.195312\n",
      "[2018-07-16 14:44:59.997452] Iteration 46400, train loss = 2.489448, train accuracy = 0.132812\n",
      "[2018-07-16 14:45:03.061361] Iteration 46500, train loss = 2.381099, train accuracy = 0.148438\n",
      "[2018-07-16 14:45:06.114323] Iteration 46600, train loss = 2.656567, train accuracy = 0.062500\n",
      "[2018-07-16 14:45:09.266610] Iteration 46700, train loss = 2.575166, train accuracy = 0.171875\n",
      "[2018-07-16 14:45:12.356371] Iteration 46800, train loss = 2.557949, train accuracy = 0.101562\n",
      "[2018-07-16 14:45:15.415596] Iteration 46900, train loss = 2.513055, train accuracy = 0.109375\n",
      "[2018-07-16 14:45:18.473432] Iteration 47000, train loss = 2.524578, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:45:22.679321] Iteration 47100, train loss = 2.511532, train accuracy = 0.117188\n",
      "[2018-07-16 14:45:25.755029] Iteration 47200, train loss = 2.523957, train accuracy = 0.109375\n",
      "[2018-07-16 14:45:28.835021] Iteration 47300, train loss = 2.535271, train accuracy = 0.125000\n",
      "[2018-07-16 14:45:31.928516] Iteration 47400, train loss = 2.633026, train accuracy = 0.117188\n",
      "[2018-07-16 14:45:35.009442] Iteration 47500, train loss = 2.500844, train accuracy = 0.140625\n",
      "[2018-07-16 14:45:38.088202] Iteration 47600, train loss = 2.583376, train accuracy = 0.117188\n",
      "[2018-07-16 14:45:41.165740] Iteration 47700, train loss = 2.423755, train accuracy = 0.179688\n",
      "[2018-07-16 14:45:44.323275] Iteration 47800, train loss = 2.398671, train accuracy = 0.125000\n",
      "[2018-07-16 14:45:47.400464] Iteration 47900, train loss = 2.563589, train accuracy = 0.140625\n",
      "[2018-07-16 14:45:50.490057] Iteration 48000, train loss = 2.537181, train accuracy = 0.078125\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 14:45:54.689152] Iteration 48100, train loss = 2.485207, train accuracy = 0.171875\n",
      "[2018-07-16 14:45:57.761948] Iteration 48200, train loss = 2.482053, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:00.820647] Iteration 48300, train loss = 2.601320, train accuracy = 0.125000\n",
      "[2018-07-16 14:46:03.898855] Iteration 48400, train loss = 2.459970, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:06.959105] Iteration 48500, train loss = 2.474927, train accuracy = 0.109375\n",
      "[2018-07-16 14:46:10.020240] Iteration 48600, train loss = 2.523104, train accuracy = 0.156250\n",
      "[2018-07-16 14:46:13.076728] Iteration 48700, train loss = 2.434809, train accuracy = 0.125000\n",
      "[2018-07-16 14:46:16.137464] Iteration 48800, train loss = 2.525489, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:19.296895] Iteration 48900, train loss = 2.606426, train accuracy = 0.085938\n",
      "[2018-07-16 14:46:22.392463] Iteration 49000, train loss = 2.448384, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:46:26.630250] Iteration 49100, train loss = 2.421924, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:29.713957] Iteration 49200, train loss = 2.580566, train accuracy = 0.140625\n",
      "[2018-07-16 14:46:32.779217] Iteration 49300, train loss = 2.566812, train accuracy = 0.125000\n",
      "[2018-07-16 14:46:35.830125] Iteration 49400, train loss = 2.519447, train accuracy = 0.085938\n",
      "[2018-07-16 14:46:38.887059] Iteration 49500, train loss = 2.456126, train accuracy = 0.156250\n",
      "[2018-07-16 14:46:41.956965] Iteration 49600, train loss = 2.444477, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:45.012327] Iteration 49700, train loss = 2.528385, train accuracy = 0.148438\n",
      "[2018-07-16 14:46:48.072107] Iteration 49800, train loss = 2.474655, train accuracy = 0.117188\n",
      "[2018-07-16 14:46:51.139073] Iteration 49900, train loss = 2.514383, train accuracy = 0.156250\n",
      "[2018-07-16 14:46:54.291055] Iteration 50000, train loss = 2.573784, train accuracy = 0.109375\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:46:58.526955] Iteration 50100, train loss = 2.454348, train accuracy = 0.148438\n",
      "[2018-07-16 14:47:01.586206] Iteration 50200, train loss = 2.471677, train accuracy = 0.062500\n",
      "[2018-07-16 14:47:04.677783] Iteration 50300, train loss = 2.552818, train accuracy = 0.109375\n",
      "[2018-07-16 14:47:07.757601] Iteration 50400, train loss = 2.432896, train accuracy = 0.132812\n",
      "[2018-07-16 14:47:10.819512] Iteration 50500, train loss = 2.436529, train accuracy = 0.109375\n",
      "[2018-07-16 14:47:13.886703] Iteration 50600, train loss = 2.451372, train accuracy = 0.132812\n",
      "[2018-07-16 14:47:16.974968] Iteration 50700, train loss = 2.409897, train accuracy = 0.117188\n",
      "[2018-07-16 14:47:20.052880] Iteration 50800, train loss = 2.486102, train accuracy = 0.140625\n",
      "[2018-07-16 14:47:23.136160] Iteration 50900, train loss = 2.454339, train accuracy = 0.117188\n",
      "[2018-07-16 14:47:26.200331] Iteration 51000, train loss = 2.408492, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:47:30.530016] Iteration 51100, train loss = 2.456582, train accuracy = 0.148438\n",
      "[2018-07-16 14:47:33.599229] Iteration 51200, train loss = 2.594457, train accuracy = 0.117188\n",
      "[2018-07-16 14:47:36.660521] Iteration 51300, train loss = 2.360496, train accuracy = 0.187500\n",
      "[2018-07-16 14:47:39.723809] Iteration 51400, train loss = 2.705326, train accuracy = 0.132812\n",
      "[2018-07-16 14:47:42.801851] Iteration 51500, train loss = 2.504796, train accuracy = 0.164062\n",
      "[2018-07-16 14:47:45.857873] Iteration 51600, train loss = 2.573684, train accuracy = 0.140625\n",
      "[2018-07-16 14:47:48.928212] Iteration 51700, train loss = 2.325596, train accuracy = 0.148438\n",
      "[2018-07-16 14:47:51.987463] Iteration 51800, train loss = 2.299334, train accuracy = 0.187500\n",
      "[2018-07-16 14:47:55.058790] Iteration 51900, train loss = 2.516424, train accuracy = 0.140625\n",
      "[2018-07-16 14:47:58.128755] Iteration 52000, train loss = 2.397571, train accuracy = 0.187500\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 14:48:02.379369] Iteration 52100, train loss = 2.539285, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:05.515842] Iteration 52200, train loss = 2.549155, train accuracy = 0.054688\n",
      "[2018-07-16 14:48:08.577439] Iteration 52300, train loss = 2.503207, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:11.641260] Iteration 52400, train loss = 2.620146, train accuracy = 0.062500\n",
      "[2018-07-16 14:48:14.709869] Iteration 52500, train loss = 2.487961, train accuracy = 0.109375\n",
      "[2018-07-16 14:48:17.804014] Iteration 52600, train loss = 2.437878, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:20.872901] Iteration 52700, train loss = 2.426479, train accuracy = 0.171875\n",
      "[2018-07-16 14:48:23.923137] Iteration 52800, train loss = 2.506865, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:26.982468] Iteration 52900, train loss = 2.319056, train accuracy = 0.156250\n",
      "[2018-07-16 14:48:30.061511] Iteration 53000, train loss = 2.464674, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.142400\n",
      "[2018-07-16 14:48:34.247659] Iteration 53100, train loss = 2.388197, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:37.325635] Iteration 53200, train loss = 2.612743, train accuracy = 0.125000\n",
      "[2018-07-16 14:48:40.477803] Iteration 53300, train loss = 2.547937, train accuracy = 0.085938\n",
      "[2018-07-16 14:48:43.556910] Iteration 53400, train loss = 2.513677, train accuracy = 0.210938\n",
      "[2018-07-16 14:48:46.641395] Iteration 53500, train loss = 2.449554, train accuracy = 0.117188\n",
      "[2018-07-16 14:48:49.714273] Iteration 53600, train loss = 2.587357, train accuracy = 0.132812\n",
      "[2018-07-16 14:48:52.799669] Iteration 53700, train loss = 2.649301, train accuracy = 0.085938\n",
      "[2018-07-16 14:48:55.865878] Iteration 53800, train loss = 2.513401, train accuracy = 0.117188\n",
      "[2018-07-16 14:48:58.962230] Iteration 53900, train loss = 2.595849, train accuracy = 0.109375\n",
      "[2018-07-16 14:49:02.036318] Iteration 54000, train loss = 2.470613, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:49:06.282439] Iteration 54100, train loss = 2.564115, train accuracy = 0.125000\n",
      "[2018-07-16 14:49:09.340474] Iteration 54200, train loss = 2.394460, train accuracy = 0.148438\n",
      "[2018-07-16 14:49:12.407975] Iteration 54300, train loss = 2.556343, train accuracy = 0.132812\n",
      "[2018-07-16 14:49:15.540301] Iteration 54400, train loss = 2.467758, train accuracy = 0.054688\n",
      "[2018-07-16 14:49:18.594939] Iteration 54500, train loss = 2.356688, train accuracy = 0.179688\n",
      "[2018-07-16 14:49:21.681575] Iteration 54600, train loss = 2.444300, train accuracy = 0.125000\n",
      "[2018-07-16 14:49:24.754644] Iteration 54700, train loss = 2.438579, train accuracy = 0.132812\n",
      "[2018-07-16 14:49:27.839366] Iteration 54800, train loss = 2.457990, train accuracy = 0.140625\n",
      "[2018-07-16 14:49:30.901355] Iteration 54900, train loss = 2.513301, train accuracy = 0.148438\n",
      "[2018-07-16 14:49:33.957631] Iteration 55000, train loss = 2.435791, train accuracy = 0.078125\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 14:49:38.195508] Iteration 55100, train loss = 2.428528, train accuracy = 0.109375\n",
      "[2018-07-16 14:49:41.285250] Iteration 55200, train loss = 2.601511, train accuracy = 0.156250\n",
      "[2018-07-16 14:49:44.361776] Iteration 55300, train loss = 2.381142, train accuracy = 0.179688\n",
      "[2018-07-16 14:49:47.457494] Iteration 55400, train loss = 2.468214, train accuracy = 0.093750\n",
      "[2018-07-16 14:49:50.609391] Iteration 55500, train loss = 2.428598, train accuracy = 0.148438\n",
      "[2018-07-16 14:49:53.687015] Iteration 55600, train loss = 2.433547, train accuracy = 0.140625\n",
      "[2018-07-16 14:49:56.736815] Iteration 55700, train loss = 2.514867, train accuracy = 0.164062\n",
      "[2018-07-16 14:49:59.773841] Iteration 55800, train loss = 2.515527, train accuracy = 0.132812\n",
      "[2018-07-16 14:50:02.845642] Iteration 55900, train loss = 2.340912, train accuracy = 0.187500\n",
      "[2018-07-16 14:50:05.914465] Iteration 56000, train loss = 2.442136, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:50:10.121799] Iteration 56100, train loss = 2.457368, train accuracy = 0.148438\n",
      "[2018-07-16 14:50:13.192842] Iteration 56200, train loss = 2.547257, train accuracy = 0.140625\n",
      "[2018-07-16 14:50:16.262305] Iteration 56300, train loss = 2.509290, train accuracy = 0.156250\n",
      "[2018-07-16 14:50:19.332912] Iteration 56400, train loss = 2.559803, train accuracy = 0.148438\n",
      "[2018-07-16 14:50:22.398102] Iteration 56500, train loss = 2.505450, train accuracy = 0.164062\n",
      "[2018-07-16 14:50:25.559666] Iteration 56600, train loss = 2.454675, train accuracy = 0.132812\n",
      "[2018-07-16 14:50:28.634623] Iteration 56700, train loss = 2.453755, train accuracy = 0.140625\n",
      "[2018-07-16 14:50:31.706162] Iteration 56800, train loss = 2.573988, train accuracy = 0.109375\n",
      "[2018-07-16 14:50:34.764779] Iteration 56900, train loss = 2.423980, train accuracy = 0.132812\n",
      "[2018-07-16 14:50:37.854403] Iteration 57000, train loss = 2.539243, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:50:42.089288] Iteration 57100, train loss = 2.408680, train accuracy = 0.140625\n",
      "[2018-07-16 14:50:45.161970] Iteration 57200, train loss = 2.528002, train accuracy = 0.117188\n",
      "[2018-07-16 14:50:48.226454] Iteration 57300, train loss = 2.512054, train accuracy = 0.148438\n",
      "[2018-07-16 14:50:51.285487] Iteration 57400, train loss = 2.372443, train accuracy = 0.148438\n",
      "[2018-07-16 14:50:54.368887] Iteration 57500, train loss = 2.501891, train accuracy = 0.125000\n",
      "[2018-07-16 14:50:57.422267] Iteration 57600, train loss = 2.549634, train accuracy = 0.187500\n",
      "[2018-07-16 14:51:00.547229] Iteration 57700, train loss = 2.478574, train accuracy = 0.148438\n",
      "[2018-07-16 14:51:03.611823] Iteration 57800, train loss = 2.562395, train accuracy = 0.085938\n",
      "[2018-07-16 14:51:06.691926] Iteration 57900, train loss = 2.510573, train accuracy = 0.117188\n",
      "[2018-07-16 14:51:09.761601] Iteration 58000, train loss = 2.450790, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:51:13.991818] Iteration 58100, train loss = 2.374715, train accuracy = 0.148438\n",
      "[2018-07-16 14:51:17.068433] Iteration 58200, train loss = 2.416864, train accuracy = 0.148438\n",
      "[2018-07-16 14:51:20.140342] Iteration 58300, train loss = 2.551131, train accuracy = 0.148438\n",
      "[2018-07-16 14:51:23.214829] Iteration 58400, train loss = 2.558658, train accuracy = 0.101562\n",
      "[2018-07-16 14:51:26.285727] Iteration 58500, train loss = 2.434114, train accuracy = 0.203125\n",
      "[2018-07-16 14:51:29.348642] Iteration 58600, train loss = 2.439753, train accuracy = 0.125000\n",
      "[2018-07-16 14:51:32.409205] Iteration 58700, train loss = 2.476614, train accuracy = 0.171875\n",
      "[2018-07-16 14:51:35.569610] Iteration 58800, train loss = 2.548207, train accuracy = 0.070312\n",
      "[2018-07-16 14:51:38.623665] Iteration 58900, train loss = 2.492059, train accuracy = 0.117188\n",
      "[2018-07-16 14:51:41.694186] Iteration 59000, train loss = 2.469282, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.142300\n",
      "[2018-07-16 14:51:45.896793] Iteration 59100, train loss = 2.445490, train accuracy = 0.078125\n",
      "[2018-07-16 14:51:48.978253] Iteration 59200, train loss = 2.571565, train accuracy = 0.101562\n",
      "[2018-07-16 14:51:52.058357] Iteration 59300, train loss = 2.510146, train accuracy = 0.117188\n",
      "[2018-07-16 14:51:55.143336] Iteration 59400, train loss = 2.434589, train accuracy = 0.148438\n",
      "[2018-07-16 14:51:58.223068] Iteration 59500, train loss = 2.506532, train accuracy = 0.132812\n",
      "[2018-07-16 14:52:01.295226] Iteration 59600, train loss = 2.378496, train accuracy = 0.125000\n",
      "[2018-07-16 14:52:04.381376] Iteration 59700, train loss = 2.518202, train accuracy = 0.125000\n",
      "[2018-07-16 14:52:07.451567] Iteration 59800, train loss = 2.485128, train accuracy = 0.125000\n",
      "[2018-07-16 14:52:10.595838] Iteration 59900, train loss = 2.419331, train accuracy = 0.148438\n",
      "[2018-07-16 14:52:13.690685] Iteration 60000, train loss = 2.480299, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:52:17.922265] Iteration 60100, train loss = 2.477759, train accuracy = 0.093750\n",
      "[2018-07-16 14:52:20.970562] Iteration 60200, train loss = 2.597937, train accuracy = 0.109375\n",
      "[2018-07-16 14:52:24.024451] Iteration 60300, train loss = 2.432331, train accuracy = 0.117188\n",
      "[2018-07-16 14:52:27.087635] Iteration 60400, train loss = 2.413449, train accuracy = 0.195312\n",
      "[2018-07-16 14:52:30.140466] Iteration 60500, train loss = 2.490700, train accuracy = 0.140625\n",
      "[2018-07-16 14:52:33.192796] Iteration 60600, train loss = 2.414806, train accuracy = 0.179688\n",
      "[2018-07-16 14:52:36.266763] Iteration 60700, train loss = 2.465394, train accuracy = 0.148438\n",
      "[2018-07-16 14:52:39.321390] Iteration 60800, train loss = 2.425192, train accuracy = 0.117188\n",
      "[2018-07-16 14:52:42.411109] Iteration 60900, train loss = 2.476245, train accuracy = 0.132812\n",
      "[2018-07-16 14:52:45.619839] Iteration 61000, train loss = 2.443592, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:52:49.846178] Iteration 61100, train loss = 2.523784, train accuracy = 0.156250\n",
      "[2018-07-16 14:52:52.919702] Iteration 61200, train loss = 2.589166, train accuracy = 0.117188\n",
      "[2018-07-16 14:52:56.001918] Iteration 61300, train loss = 2.489157, train accuracy = 0.179688\n",
      "[2018-07-16 14:52:59.063190] Iteration 61400, train loss = 2.453500, train accuracy = 0.148438\n",
      "[2018-07-16 14:53:02.135948] Iteration 61500, train loss = 2.426568, train accuracy = 0.148438\n",
      "[2018-07-16 14:53:05.204338] Iteration 61600, train loss = 2.342024, train accuracy = 0.179688\n",
      "[2018-07-16 14:53:08.263313] Iteration 61700, train loss = 2.432234, train accuracy = 0.117188\n",
      "[2018-07-16 14:53:11.333583] Iteration 61800, train loss = 2.486456, train accuracy = 0.093750\n",
      "[2018-07-16 14:53:14.388940] Iteration 61900, train loss = 2.458611, train accuracy = 0.132812\n",
      "[2018-07-16 14:53:17.447575] Iteration 62000, train loss = 2.524534, train accuracy = 0.101562\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:53:21.751360] Iteration 62100, train loss = 2.474808, train accuracy = 0.148438\n",
      "[2018-07-16 14:53:24.841977] Iteration 62200, train loss = 2.466749, train accuracy = 0.148438\n",
      "[2018-07-16 14:53:27.906816] Iteration 62300, train loss = 2.470996, train accuracy = 0.140625\n",
      "[2018-07-16 14:53:30.982197] Iteration 62400, train loss = 2.454282, train accuracy = 0.148438\n",
      "[2018-07-16 14:53:34.049372] Iteration 62500, train loss = 2.388942, train accuracy = 0.210938\n",
      "[2018-07-16 14:53:37.122458] Iteration 62600, train loss = 2.480654, train accuracy = 0.117188\n",
      "[2018-07-16 14:53:40.199225] Iteration 62700, train loss = 2.567953, train accuracy = 0.125000\n",
      "[2018-07-16 14:53:43.257588] Iteration 62800, train loss = 2.403486, train accuracy = 0.179688\n",
      "[2018-07-16 14:53:46.327100] Iteration 62900, train loss = 2.421660, train accuracy = 0.132812\n",
      "[2018-07-16 14:53:49.396580] Iteration 63000, train loss = 2.395395, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:53:53.629501] Iteration 63100, train loss = 2.566571, train accuracy = 0.109375\n",
      "[2018-07-16 14:53:56.768655] Iteration 63200, train loss = 2.511857, train accuracy = 0.140625\n",
      "[2018-07-16 14:53:59.821865] Iteration 63300, train loss = 2.459794, train accuracy = 0.156250\n",
      "[2018-07-16 14:54:02.884873] Iteration 63400, train loss = 2.526426, train accuracy = 0.101562\n",
      "[2018-07-16 14:54:05.930080] Iteration 63500, train loss = 2.524926, train accuracy = 0.132812\n",
      "[2018-07-16 14:54:08.982106] Iteration 63600, train loss = 2.469759, train accuracy = 0.125000\n",
      "[2018-07-16 14:54:12.045581] Iteration 63700, train loss = 2.432935, train accuracy = 0.132812\n",
      "[2018-07-16 14:54:15.118488] Iteration 63800, train loss = 2.546614, train accuracy = 0.117188\n",
      "[2018-07-16 14:54:18.214408] Iteration 63900, train loss = 2.498338, train accuracy = 0.117188\n",
      "[2018-07-16 14:54:21.281598] Iteration 64000, train loss = 2.631057, train accuracy = 0.101562\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:54:25.484236] Iteration 64100, train loss = 2.564034, train accuracy = 0.093750\n",
      "[2018-07-16 14:54:28.558546] Iteration 64200, train loss = 2.489708, train accuracy = 0.156250\n",
      "[2018-07-16 14:54:31.744492] Iteration 64300, train loss = 2.426799, train accuracy = 0.125000\n",
      "[2018-07-16 14:54:34.814690] Iteration 64400, train loss = 2.512610, train accuracy = 0.140625\n",
      "[2018-07-16 14:54:37.882554] Iteration 64500, train loss = 2.461120, train accuracy = 0.140625\n",
      "[2018-07-16 14:54:40.946075] Iteration 64600, train loss = 2.380551, train accuracy = 0.140625\n",
      "[2018-07-16 14:54:44.006742] Iteration 64700, train loss = 2.396369, train accuracy = 0.195312\n",
      "[2018-07-16 14:54:47.067007] Iteration 64800, train loss = 2.403424, train accuracy = 0.140625\n",
      "[2018-07-16 14:54:50.126221] Iteration 64900, train loss = 2.541066, train accuracy = 0.125000\n",
      "[2018-07-16 14:54:53.191600] Iteration 65000, train loss = 2.413454, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:54:57.389233] Iteration 65100, train loss = 2.650372, train accuracy = 0.085938\n",
      "[2018-07-16 14:55:00.453667] Iteration 65200, train loss = 2.443787, train accuracy = 0.125000\n",
      "[2018-07-16 14:55:03.522258] Iteration 65300, train loss = 2.461210, train accuracy = 0.109375\n",
      "[2018-07-16 14:55:06.669025] Iteration 65400, train loss = 2.465730, train accuracy = 0.148438\n",
      "[2018-07-16 14:55:09.732072] Iteration 65500, train loss = 2.581543, train accuracy = 0.085938\n",
      "[2018-07-16 14:55:12.810895] Iteration 65600, train loss = 2.570797, train accuracy = 0.085938\n",
      "[2018-07-16 14:55:15.876560] Iteration 65700, train loss = 2.502457, train accuracy = 0.117188\n",
      "[2018-07-16 14:55:18.970060] Iteration 65800, train loss = 2.364931, train accuracy = 0.179688\n",
      "[2018-07-16 14:55:22.038894] Iteration 65900, train loss = 2.470684, train accuracy = 0.093750\n",
      "[2018-07-16 14:55:25.112706] Iteration 66000, train loss = 2.524863, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.141500\n",
      "[2018-07-16 14:55:29.323981] Iteration 66100, train loss = 2.444027, train accuracy = 0.117188\n",
      "[2018-07-16 14:55:32.371100] Iteration 66200, train loss = 2.502350, train accuracy = 0.148438\n",
      "[2018-07-16 14:55:35.437564] Iteration 66300, train loss = 2.482042, train accuracy = 0.101562\n",
      "[2018-07-16 14:55:38.490650] Iteration 66400, train loss = 2.559520, train accuracy = 0.078125\n",
      "[2018-07-16 14:55:41.634720] Iteration 66500, train loss = 2.562812, train accuracy = 0.093750\n",
      "[2018-07-16 14:55:44.696042] Iteration 66600, train loss = 2.528049, train accuracy = 0.117188\n",
      "[2018-07-16 14:55:47.760196] Iteration 66700, train loss = 2.497514, train accuracy = 0.085938\n",
      "[2018-07-16 14:55:50.843166] Iteration 66800, train loss = 2.455624, train accuracy = 0.187500\n",
      "[2018-07-16 14:55:53.913037] Iteration 66900, train loss = 2.532341, train accuracy = 0.093750\n",
      "[2018-07-16 14:55:56.994112] Iteration 67000, train loss = 2.516939, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141600\n",
      "[2018-07-16 14:56:01.201211] Iteration 67100, train loss = 2.500571, train accuracy = 0.101562\n",
      "[2018-07-16 14:56:04.268202] Iteration 67200, train loss = 2.509249, train accuracy = 0.148438\n",
      "[2018-07-16 14:56:07.341381] Iteration 67300, train loss = 2.489431, train accuracy = 0.164062\n",
      "[2018-07-16 14:56:10.402561] Iteration 67400, train loss = 2.472546, train accuracy = 0.171875\n",
      "[2018-07-16 14:56:13.487320] Iteration 67500, train loss = 2.472236, train accuracy = 0.140625\n",
      "[2018-07-16 14:56:16.657447] Iteration 67600, train loss = 2.519121, train accuracy = 0.062500\n",
      "[2018-07-16 14:56:19.720967] Iteration 67700, train loss = 2.484572, train accuracy = 0.156250\n",
      "[2018-07-16 14:56:22.779210] Iteration 67800, train loss = 2.467192, train accuracy = 0.132812\n",
      "[2018-07-16 14:56:25.844116] Iteration 67900, train loss = 2.503073, train accuracy = 0.125000\n",
      "[2018-07-16 14:56:28.902639] Iteration 68000, train loss = 2.495415, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 14:56:33.099756] Iteration 68100, train loss = 2.440009, train accuracy = 0.101562\n",
      "[2018-07-16 14:56:36.173467] Iteration 68200, train loss = 2.490864, train accuracy = 0.132812\n",
      "[2018-07-16 14:56:39.265312] Iteration 68300, train loss = 2.670327, train accuracy = 0.085938\n",
      "[2018-07-16 14:56:42.332143] Iteration 68400, train loss = 2.527631, train accuracy = 0.125000\n",
      "[2018-07-16 14:56:45.394876] Iteration 68500, train loss = 2.337429, train accuracy = 0.132812\n",
      "[2018-07-16 14:56:48.466529] Iteration 68600, train loss = 2.528979, train accuracy = 0.171875\n",
      "[2018-07-16 14:56:51.673282] Iteration 68700, train loss = 2.515526, train accuracy = 0.093750\n",
      "[2018-07-16 14:56:54.753510] Iteration 68800, train loss = 2.359969, train accuracy = 0.179688\n",
      "[2018-07-16 14:56:57.833693] Iteration 68900, train loss = 2.467673, train accuracy = 0.195312\n",
      "[2018-07-16 14:57:00.895982] Iteration 69000, train loss = 2.479400, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.143000\n",
      "[2018-07-16 14:57:05.121167] Iteration 69100, train loss = 2.431250, train accuracy = 0.156250\n",
      "[2018-07-16 14:57:08.190523] Iteration 69200, train loss = 2.547152, train accuracy = 0.117188\n",
      "[2018-07-16 14:57:11.244990] Iteration 69300, train loss = 2.435002, train accuracy = 0.156250\n",
      "[2018-07-16 14:57:14.291471] Iteration 69400, train loss = 2.392408, train accuracy = 0.164062\n",
      "[2018-07-16 14:57:17.349394] Iteration 69500, train loss = 2.603732, train accuracy = 0.070312\n",
      "[2018-07-16 14:57:20.408413] Iteration 69600, train loss = 2.594538, train accuracy = 0.132812\n",
      "[2018-07-16 14:57:23.464469] Iteration 69700, train loss = 2.415645, train accuracy = 0.179688\n",
      "[2018-07-16 14:57:26.657288] Iteration 69800, train loss = 2.454013, train accuracy = 0.171875\n",
      "[2018-07-16 14:57:29.731481] Iteration 69900, train loss = 2.493525, train accuracy = 0.125000\n",
      "[2018-07-16 14:57:32.792418] Iteration 70000, train loss = 2.420901, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 14:57:37.002286] Iteration 70100, train loss = 2.450865, train accuracy = 0.148438\n",
      "[2018-07-16 14:57:40.074538] Iteration 70200, train loss = 2.439538, train accuracy = 0.156250\n",
      "[2018-07-16 14:57:43.150500] Iteration 70300, train loss = 2.416310, train accuracy = 0.203125\n",
      "[2018-07-16 14:57:46.217679] Iteration 70400, train loss = 2.490212, train accuracy = 0.148438\n",
      "[2018-07-16 14:57:49.293729] Iteration 70500, train loss = 2.578576, train accuracy = 0.101562\n",
      "[2018-07-16 14:57:52.374930] Iteration 70600, train loss = 2.535921, train accuracy = 0.085938\n",
      "[2018-07-16 14:57:55.455581] Iteration 70700, train loss = 2.482322, train accuracy = 0.148438\n",
      "[2018-07-16 14:57:58.516664] Iteration 70800, train loss = 2.539166, train accuracy = 0.218750\n",
      "[2018-07-16 14:58:01.649026] Iteration 70900, train loss = 2.569251, train accuracy = 0.125000\n",
      "[2018-07-16 14:58:04.714096] Iteration 71000, train loss = 2.521574, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 14:58:08.917105] Iteration 71100, train loss = 2.556093, train accuracy = 0.085938\n",
      "[2018-07-16 14:58:11.991502] Iteration 71200, train loss = 2.473881, train accuracy = 0.148438\n",
      "[2018-07-16 14:58:15.056897] Iteration 71300, train loss = 2.456926, train accuracy = 0.140625\n",
      "[2018-07-16 14:58:18.134872] Iteration 71400, train loss = 2.484660, train accuracy = 0.164062\n",
      "[2018-07-16 14:58:21.195363] Iteration 71500, train loss = 2.466005, train accuracy = 0.132812\n",
      "[2018-07-16 14:58:24.254604] Iteration 71600, train loss = 2.519735, train accuracy = 0.164062\n",
      "[2018-07-16 14:58:27.347015] Iteration 71700, train loss = 2.443803, train accuracy = 0.164062\n",
      "[2018-07-16 14:58:30.408828] Iteration 71800, train loss = 2.489442, train accuracy = 0.132812\n",
      "[2018-07-16 14:58:33.482745] Iteration 71900, train loss = 2.432492, train accuracy = 0.164062\n",
      "[2018-07-16 14:58:36.628492] Iteration 72000, train loss = 2.506233, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.142100\n",
      "[2018-07-16 14:58:40.870929] Iteration 72100, train loss = 2.552690, train accuracy = 0.132812\n",
      "[2018-07-16 14:58:43.937785] Iteration 72200, train loss = 2.471228, train accuracy = 0.156250\n",
      "[2018-07-16 14:58:46.993316] Iteration 72300, train loss = 2.515244, train accuracy = 0.093750\n",
      "[2018-07-16 14:58:50.057907] Iteration 72400, train loss = 2.541324, train accuracy = 0.117188\n",
      "[2018-07-16 14:58:53.134750] Iteration 72500, train loss = 2.493809, train accuracy = 0.140625\n",
      "[2018-07-16 14:58:56.197238] Iteration 72600, train loss = 2.561257, train accuracy = 0.148438\n",
      "[2018-07-16 14:58:59.266237] Iteration 72700, train loss = 2.426673, train accuracy = 0.132812\n",
      "[2018-07-16 14:59:02.344100] Iteration 72800, train loss = 2.498541, train accuracy = 0.125000\n",
      "[2018-07-16 14:59:05.415590] Iteration 72900, train loss = 2.369962, train accuracy = 0.164062\n",
      "[2018-07-16 14:59:08.482570] Iteration 73000, train loss = 2.528693, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.142300\n",
      "[2018-07-16 14:59:12.772651] Iteration 73100, train loss = 2.417714, train accuracy = 0.148438\n",
      "[2018-07-16 14:59:15.861991] Iteration 73200, train loss = 2.444010, train accuracy = 0.132812\n",
      "[2018-07-16 14:59:18.928674] Iteration 73300, train loss = 2.476731, train accuracy = 0.179688\n",
      "[2018-07-16 14:59:22.004642] Iteration 73400, train loss = 2.438724, train accuracy = 0.125000\n",
      "[2018-07-16 14:59:25.071428] Iteration 73500, train loss = 2.464015, train accuracy = 0.140625\n",
      "[2018-07-16 14:59:28.130625] Iteration 73600, train loss = 2.483962, train accuracy = 0.140625\n",
      "[2018-07-16 14:59:31.190395] Iteration 73700, train loss = 2.439413, train accuracy = 0.164062\n",
      "[2018-07-16 14:59:34.250385] Iteration 73800, train loss = 2.589641, train accuracy = 0.117188\n",
      "[2018-07-16 14:59:37.309924] Iteration 73900, train loss = 2.404903, train accuracy = 0.140625\n",
      "[2018-07-16 14:59:40.381007] Iteration 74000, train loss = 2.468833, train accuracy = 0.109375\n",
      "Evaluating...\n",
      "Test accuracy = 0.141800\n",
      "[2018-07-16 14:59:44.571922] Iteration 74100, train loss = 2.445186, train accuracy = 0.109375\n",
      "[2018-07-16 14:59:47.725657] Iteration 74200, train loss = 2.497124, train accuracy = 0.132812\n",
      "[2018-07-16 14:59:50.784756] Iteration 74300, train loss = 2.495245, train accuracy = 0.132812\n",
      "[2018-07-16 14:59:53.862812] Iteration 74400, train loss = 2.593865, train accuracy = 0.148438\n",
      "[2018-07-16 14:59:56.953565] Iteration 74500, train loss = 2.400612, train accuracy = 0.179688\n",
      "[2018-07-16 15:00:00.028438] Iteration 74600, train loss = 2.426788, train accuracy = 0.148438\n",
      "[2018-07-16 15:00:03.097593] Iteration 74700, train loss = 2.519700, train accuracy = 0.101562\n",
      "[2018-07-16 15:00:06.165255] Iteration 74800, train loss = 2.481337, train accuracy = 0.187500\n",
      "[2018-07-16 15:00:09.236199] Iteration 74900, train loss = 2.365589, train accuracy = 0.093750\n",
      "[2018-07-16 15:00:12.313171] Iteration 75000, train loss = 2.454291, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 15:00:16.506759] Iteration 75100, train loss = 2.615678, train accuracy = 0.078125\n",
      "[2018-07-16 15:00:19.573602] Iteration 75200, train loss = 2.515311, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:22.726991] Iteration 75300, train loss = 2.564831, train accuracy = 0.132812\n",
      "[2018-07-16 15:00:25.809443] Iteration 75400, train loss = 2.549432, train accuracy = 0.109375\n",
      "[2018-07-16 15:00:28.887626] Iteration 75500, train loss = 2.454740, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:31.949156] Iteration 75600, train loss = 2.579596, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:35.004606] Iteration 75700, train loss = 2.443928, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:38.081330] Iteration 75800, train loss = 2.503693, train accuracy = 0.148438\n",
      "[2018-07-16 15:00:41.144248] Iteration 75900, train loss = 2.506194, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:44.230997] Iteration 76000, train loss = 2.321794, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 15:00:48.429991] Iteration 76100, train loss = 2.394626, train accuracy = 0.187500\n",
      "[2018-07-16 15:00:51.498181] Iteration 76200, train loss = 2.518043, train accuracy = 0.109375\n",
      "[2018-07-16 15:00:54.561011] Iteration 76300, train loss = 2.484327, train accuracy = 0.125000\n",
      "[2018-07-16 15:00:57.700125] Iteration 76400, train loss = 2.599118, train accuracy = 0.109375\n",
      "[2018-07-16 15:01:00.796564] Iteration 76500, train loss = 2.414055, train accuracy = 0.148438\n",
      "[2018-07-16 15:01:03.882165] Iteration 76600, train loss = 2.534302, train accuracy = 0.117188\n",
      "[2018-07-16 15:01:06.937878] Iteration 76700, train loss = 2.463159, train accuracy = 0.171875\n",
      "[2018-07-16 15:01:10.008311] Iteration 76800, train loss = 2.724420, train accuracy = 0.062500\n",
      "[2018-07-16 15:01:13.072661] Iteration 76900, train loss = 2.481449, train accuracy = 0.164062\n",
      "[2018-07-16 15:01:16.126540] Iteration 77000, train loss = 2.564481, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.141700\n",
      "[2018-07-16 15:01:20.313654] Iteration 77100, train loss = 2.416672, train accuracy = 0.109375\n",
      "[2018-07-16 15:01:23.384781] Iteration 77200, train loss = 2.468949, train accuracy = 0.164062\n",
      "[2018-07-16 15:01:26.449375] Iteration 77300, train loss = 2.427860, train accuracy = 0.148438\n",
      "[2018-07-16 15:01:29.501566] Iteration 77400, train loss = 2.393631, train accuracy = 0.117188\n",
      "[2018-07-16 15:01:32.638439] Iteration 77500, train loss = 2.381363, train accuracy = 0.125000\n",
      "[2018-07-16 15:01:35.748224] Iteration 77600, train loss = 2.498303, train accuracy = 0.093750\n",
      "[2018-07-16 15:01:38.814159] Iteration 77700, train loss = 2.558866, train accuracy = 0.125000\n",
      "[2018-07-16 15:01:41.908735] Iteration 77800, train loss = 2.551316, train accuracy = 0.140625\n",
      "[2018-07-16 15:01:44.979664] Iteration 77900, train loss = 2.363554, train accuracy = 0.117188\n",
      "[2018-07-16 15:01:48.062885] Iteration 78000, train loss = 2.512536, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.142000\n",
      "[2018-07-16 15:01:52.292962] Iteration 78100, train loss = 2.485327, train accuracy = 0.101562\n",
      "[2018-07-16 15:01:55.354908] Iteration 78200, train loss = 2.504604, train accuracy = 0.085938\n",
      "[2018-07-16 15:01:58.413725] Iteration 78300, train loss = 2.564833, train accuracy = 0.140625\n",
      "[2018-07-16 15:02:01.478187] Iteration 78400, train loss = 2.574455, train accuracy = 0.093750\n",
      "[2018-07-16 15:02:04.550337] Iteration 78500, train loss = 2.606848, train accuracy = 0.101562\n",
      "[2018-07-16 15:02:07.640152] Iteration 78600, train loss = 2.534846, train accuracy = 0.140625\n",
      "[2018-07-16 15:02:10.737731] Iteration 78700, train loss = 2.470845, train accuracy = 0.148438\n",
      "[2018-07-16 15:02:13.819131] Iteration 78800, train loss = 2.541094, train accuracy = 0.117188\n",
      "[2018-07-16 15:02:16.874781] Iteration 78900, train loss = 2.613602, train accuracy = 0.117188\n",
      "[2018-07-16 15:02:19.960248] Iteration 79000, train loss = 2.505545, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.141900\n",
      "[2018-07-16 15:02:24.174371] Iteration 79100, train loss = 2.546037, train accuracy = 0.140625\n",
      "[2018-07-16 15:02:27.246608] Iteration 79200, train loss = 2.495996, train accuracy = 0.140625\n",
      "[2018-07-16 15:02:30.308103] Iteration 79300, train loss = 2.499091, train accuracy = 0.156250\n",
      "[2018-07-16 15:02:33.363234] Iteration 79400, train loss = 2.517615, train accuracy = 0.117188\n",
      "[2018-07-16 15:02:36.424845] Iteration 79500, train loss = 2.421589, train accuracy = 0.117188\n",
      "[2018-07-16 15:02:39.497326] Iteration 79600, train loss = 2.431585, train accuracy = 0.156250\n",
      "[2018-07-16 15:02:42.575264] Iteration 79700, train loss = 2.496188, train accuracy = 0.093750\n",
      "[2018-07-16 15:02:45.708833] Iteration 79800, train loss = 2.604303, train accuracy = 0.156250\n",
      "[2018-07-16 15:02:48.787600] Iteration 79900, train loss = 2.584481, train accuracy = 0.125000\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune后的准确率\n",
      "Test accuracy = 0.141700\n"
     ]
    }
   ],
   "source": [
    "print('prune后的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.     0.     0.125  0.125  0.     0.     0.125 -0.25   0.    -0.125]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
