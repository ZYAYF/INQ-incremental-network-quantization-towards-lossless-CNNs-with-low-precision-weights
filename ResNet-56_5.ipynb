{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "# import ipdb\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('residual_net_n', 9, '')\n",
    "tf.app.flags.DEFINE_string('train_tf_path', 'E:/dynamicquantization/data/train.tf', '')\n",
    "tf.app.flags.DEFINE_string('val_tf_path', 'E:/dynamicquantization/data/test.tf', '')\n",
    "tf.app.flags.DEFINE_integer('train_batch_size', 128, '')\n",
    "tf.app.flags.DEFINE_integer('val_batch_size', 100, '')\n",
    "tf.app.flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay')\n",
    "tf.app.flags.DEFINE_integer('summary_interval', 100, 'Interval for summary.')\n",
    "tf.app.flags.DEFINE_integer('val_interval', 1000, 'Interval for evaluation.')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000, 'Maximum number of iterations.')\n",
    "tf.app.flags.DEFINE_integer('save_interval', 5000, '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  with tf.device('/cpu:0'):\n",
    "    params = tf.constant(embedding_params)\n",
    "    embedding = tf.gather(params, label)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_in, n_out, k, s, p='SAME', bias=False, scope='conv'):\n",
    "  with tf.variable_scope(scope):\n",
    "    kernel = tf.Variable(\n",
    "      tf.truncated_normal([k, k, n_in, n_out],\n",
    "        stddev=math.sqrt(2/(k*k*n_in))),\n",
    "      name='weight')\n",
    "    tf.add_to_collection('weights', kernel)\n",
    "    conv = tf.nn.conv2d(x, kernel, [1,s,s,1], padding=p)\n",
    "    if bias:\n",
    "      bias = tf.get_variable('bias', [n_out], initializer=tf.constant_initializer(0.0))\n",
    "      tf.add_to_collection('biases', bias)\n",
    "      conv = tf.nn.bias_add(conv, bias)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn', affine=True):\n",
    "  \"\"\"\n",
    "  Batch normalization on convolutional maps.\n",
    "  Args:\n",
    "    x: Tensor, 4D BHWD input maps\n",
    "    n_out: integer, depth of input maps\n",
    "    phase_train: boolean tf.Variable, true indicates training phase\n",
    "    scope: string, variable scope\n",
    "    affine: whether to affine-transform outputs\n",
    "  Return:\n",
    "    normed: batch-normalized maps\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "      name='beta', trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "      name='gamma', trainable=affine)\n",
    "    tf.add_to_collection('biases', beta)\n",
    "    tf.add_to_collection('weights', gamma)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "      ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "      with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train,\n",
    "      mean_var_with_update,\n",
    "      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \n",
    "      beta, gamma, 1e-3, affine)\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x, n_in, n_out, subsample, phase_train, scope='res_block'):\n",
    "  with tf.variable_scope(scope):\n",
    "    if subsample:\n",
    "      y = conv2d(x, n_in, n_out, 3, 2, 'SAME', False, scope='conv_1')\n",
    "      shortcut = conv2d(x, n_in, n_out, 3, 2, 'SAME',\n",
    "                False, scope='shortcut')\n",
    "    else:\n",
    "      y = conv2d(x, n_in, n_out, 3, 1, 'SAME', False, scope='conv_1')\n",
    "      shortcut = tf.identity(x, name='shortcut')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_1')\n",
    "    y = tf.nn.relu(y, name='relu_1')\n",
    "    y = conv2d(y, n_out, n_out, 3, 1, 'SAME', True, scope='conv_2')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_2')\n",
    "    y = y + shortcut\n",
    "    y = tf.nn.relu(y, name='relu_2')\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_group(x, n_in, n_out, n, first_subsample, phase_train, scope='res_group'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = residual_block(x, n_in, n_out, first_subsample, phase_train, scope='block_1')\n",
    "    for i in range(n - 1):\n",
    "      y = residual_block(y, n_out, n_out, False, phase_train, scope='block_%d' % (i + 2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_net(x, n, n_classes, phase_train, scope='res_net'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = conv2d(x, 3, 16, 3, 1, 'SAME', False, scope='conv_init')\n",
    "    y = batch_norm(y, 16, phase_train, scope='bn_init')\n",
    "    y = tf.nn.relu(y, name='relu_init')\n",
    "    y = residual_group(y, 16, 16, n, False, phase_train, scope='group_1')\n",
    "    y = residual_group(y, 16, 32, n, True, phase_train, scope='group_2')\n",
    "    y = residual_group(y, 32, 64, n, True, phase_train, scope='group_3')\n",
    "#     y = conv2d(y, 64, n_classes, 1, 1, 'SAME', True, scope='conv_last')\n",
    "    y = tf.nn.avg_pool(y, [1, 8, 8, 1], [1, 1, 1, 1], 'VALID', name='avg_pool')\n",
    "    y = tf.reshape(y, [-1, 64])\n",
    "    w = tf.get_variable(name='weight_fc', shape=[64, n_classes], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('weights', w)\n",
    "    b = tf.get_variable(name='weight_biase', shape=[n_classes], initializer=tf.constant_initializer(0))\n",
    "    tf.add_to_collection('last_biases', b)\n",
    "    y = tf.matmul(y, w) + b\n",
    "#     y = tf.squeeze(y, squeeze_dims=[1, 2])\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, labels, scope='loss'):\n",
    "  with tf.variable_scope(scope):\n",
    "    # entropy loss\n",
    "    targets = one_hot_embedding(labels, 10)\n",
    "    entropy_loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets),\n",
    "      name='entropy_loss')\n",
    "    tf.add_to_collection('losses', entropy_loss)\n",
    "    # weight l2 decay loss\n",
    "    weight_l2_losses = [tf.nn.l2_loss(o) for o in tf.get_collection('weights')]\n",
    "    weight_decay_loss = FLAGS.weight_decay*tf.add_n(weight_l2_losses)\n",
    "    tf.add_to_collection('losses', weight_decay_loss)\n",
    "  # for var in tf.get_collection('losses'):\n",
    "    # tf.scalar_summary('losses/' + var.op.name, var)\n",
    "  # total loss\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _accuracy(logits, gt_label, scope='accuracy'):\n",
    "  with tf.variable_scope(scope):\n",
    "    pred_label = tf.argmax(logits, 1)\n",
    "    acc = 1.0 - tf.nn.zero_fraction(\n",
    "      tf.cast(tf.equal(pred_label, gt_label), tf.int32))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_op(loss, global_step, learning_rate):\n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params, name='gradients')\n",
    "  optim = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "  update = optim.apply_gradients(zip(gradients, params))\n",
    "  with tf.control_dependencies([update]):\n",
    "    train_op = tf.no_op(name='train_op')\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "  # meanstd = joblib.load(FLAGS.mean_std_path)\n",
    "  # mean, std = meanstd['mean'], meanstd['std']\n",
    "  mean = [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(\n",
    "    image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_batch(train_records_path, batch_size):\n",
    "  with tf.variable_scope('train_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "      train_image = normalize_image(train_image)\n",
    "      train_image = random_distort_image(train_image)\n",
    "      train_image_batch, train_label_batch = tf.train.shuffle_batch(\n",
    "        [train_image, train_label], batch_size=batch_size, num_threads=4,\n",
    "        capacity=50000,\n",
    "        min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  with tf.variable_scope('evaluate_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "      test_image = normalize_image(test_image)\n",
    "      test_image_batch, test_label_batch = tf.train.batch(\n",
    "        [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "        capacity=10000)\n",
    "  return test_image_batch, test_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch = make_train_batch(FLAGS.train_tf_path, FLAGS.train_batch_size)\n",
    "val_image_batch, val_label_batch = make_validation_batch(FLAGS.val_tf_path, FLAGS.val_batch_size)\n",
    "\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "\n",
    "\n",
    "logits = residual_net(image_batch, FLAGS.residual_net_n, 10, phase_train)\n",
    "\n",
    "\n",
    "loss = _loss(logits, label_batch)\n",
    "accuracy = _accuracy(logits, label_batch)\n",
    "\n",
    "# train one step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "INFO:tensorflow:Restoring parameters from E:/dynamicquantization/full_precision/res56/model/res.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(Thread-6, started daemon 11664)>,\n",
       " <Thread(Thread-7, started daemon 11608)>,\n",
       " <Thread(Thread-8, started daemon 9704)>,\n",
       " <Thread(Thread-9, started daemon 6732)>,\n",
       " <Thread(Thread-10, started daemon 3192)>,\n",
       " <Thread(Thread-11, started daemon 6424)>,\n",
       " <Thread(Thread-12, started daemon 4676)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print('Initializing...')\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'E:/dynamicquantization/full_precision/res56/model/res.ckpt')\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test accuracy = 0.932300\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# validation\n",
    "\n",
    "print('Evaluating...')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "  fetches = [logits, label_batch, loss]\n",
    "  session_outputs = sess.run(\n",
    "    fetches, {phase_train.name: False})\n",
    "  val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "  val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "  val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(\n",
    "  pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_prune_on_grads(grads_and_vars, dict_nzidx):\n",
    "    for key, nzidx in dict_nzidx.items():\n",
    "        count = 0\n",
    "        for grad, var in grads_and_vars:\n",
    "            if var.name == key:\n",
    "                nzidx_obj = tf.cast(tf.constant(sess.run(dict_nzidx[key])), tf.float32)\n",
    "                grads_and_vars[count] = (tf.multiply(nzidx_obj, grad), var)\n",
    "            count += 1\n",
    "    return grads_and_vars\n",
    "\n",
    "def apply_inq(weights, inq_dict, var_name, prune_rate):  \n",
    "    for target in var_name:\n",
    "        wl = target\n",
    "        bit = 32\n",
    "\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = sess.run(weight_obj)\n",
    "\n",
    "        weight_rest = np.reshape(weight_arr, [-1])\n",
    "        dic_tem = np.reshape(sess.run(inq_dict[wl]), [-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest, [-1]))), 0)\n",
    "        num_prune = int(len(weight_rest) * prune_rate)\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]]\n",
    "\n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        n2 = n1 + 1 - bit / 4\n",
    "        upper_bound = 2 ** (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        lower_bound = 2 ** (n1 + 1 - bit / 4)\n",
    "\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2 ** (np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0] * 4 / 3)))) * np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr = np.reshape(weight_rest, np.shape(weight_arr))\n",
    "        dic_tem[idx_rest[:num_prune]] = np.zeros_like(dic_tem[idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = tf.cast(np.reshape(dic_tem, np.shape(sess.run(inq_dict[wl]))), tf.float32)\n",
    "        sess.run(weights[wl].assign(weight_arr))\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮  量化\n",
    "prune_rate =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.932300\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0994248  -0.09481467  0.05994598  0.03949992 -0.00532192 -0.01733711\n",
      "  0.00191606  0.01031727 -0.02863898 -0.06500313]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.5)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 17:59:06.133179] Iteration 100, train loss = 0.204455, train accuracy = 0.976562\n",
      "[2018-06-02 17:59:30.256179] Iteration 200, train loss = 0.205678, train accuracy = 0.976562\n",
      "[2018-06-02 17:59:54.374179] Iteration 300, train loss = 0.222801, train accuracy = 0.968750\n",
      "[2018-06-02 18:00:18.414179] Iteration 400, train loss = 0.250542, train accuracy = 0.968750\n",
      "[2018-06-02 18:00:42.508179] Iteration 500, train loss = 0.184068, train accuracy = 0.984375\n",
      "[2018-06-02 18:01:06.542179] Iteration 600, train loss = 0.230968, train accuracy = 0.968750\n",
      "[2018-06-02 18:01:30.717179] Iteration 700, train loss = 0.166728, train accuracy = 0.984375\n",
      "[2018-06-02 18:01:54.739179] Iteration 800, train loss = 0.162788, train accuracy = 1.000000\n",
      "[2018-06-02 18:02:18.784179] Iteration 900, train loss = 0.195352, train accuracy = 0.984375\n",
      "[2018-06-02 18:02:42.788179] Iteration 1000, train loss = 0.168970, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.910700\n",
      "[2018-06-02 18:03:12.097179] Iteration 1100, train loss = 0.219035, train accuracy = 0.976562\n",
      "[2018-06-02 18:03:36.282179] Iteration 1200, train loss = 0.166355, train accuracy = 0.992188\n",
      "[2018-06-02 18:04:00.400179] Iteration 1300, train loss = 0.174343, train accuracy = 0.984375\n",
      "[2018-06-02 18:04:24.487179] Iteration 1400, train loss = 0.189561, train accuracy = 0.984375\n",
      "[2018-06-02 18:04:48.551179] Iteration 1500, train loss = 0.231660, train accuracy = 0.984375\n",
      "[2018-06-02 18:05:12.624179] Iteration 1600, train loss = 0.168024, train accuracy = 0.992188\n",
      "[2018-06-02 18:05:36.718179] Iteration 1700, train loss = 0.198382, train accuracy = 0.968750\n",
      "[2018-06-02 18:06:00.774179] Iteration 1800, train loss = 0.182877, train accuracy = 0.984375\n",
      "[2018-06-02 18:06:24.913179] Iteration 1900, train loss = 0.174426, train accuracy = 0.984375\n",
      "[2018-06-02 18:06:48.985179] Iteration 2000, train loss = 0.171008, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.917200\n",
      "[2018-06-02 18:07:18.323179] Iteration 2100, train loss = 0.231685, train accuracy = 0.968750\n",
      "[2018-06-02 18:07:42.385179] Iteration 2200, train loss = 0.216117, train accuracy = 0.968750\n",
      "[2018-06-02 18:08:06.483179] Iteration 2300, train loss = 0.182497, train accuracy = 0.984375\n",
      "[2018-06-02 18:08:30.476179] Iteration 2400, train loss = 0.183080, train accuracy = 0.984375\n",
      "[2018-06-02 18:08:54.499179] Iteration 2500, train loss = 0.156386, train accuracy = 1.000000\n",
      "[2018-06-02 18:09:18.583179] Iteration 2600, train loss = 0.189321, train accuracy = 0.976562\n",
      "[2018-06-02 18:09:42.721179] Iteration 2700, train loss = 0.212278, train accuracy = 0.968750\n",
      "[2018-06-02 18:10:06.852179] Iteration 2800, train loss = 0.180674, train accuracy = 0.992188\n",
      "[2018-06-02 18:10:30.866179] Iteration 2900, train loss = 0.163678, train accuracy = 0.992188\n",
      "[2018-06-02 18:10:54.910179] Iteration 3000, train loss = 0.200969, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.919900\n",
      "[2018-06-02 18:11:24.184179] Iteration 3100, train loss = 0.151873, train accuracy = 1.000000\n",
      "[2018-06-02 18:11:48.299179] Iteration 3200, train loss = 0.152901, train accuracy = 1.000000\n",
      "[2018-06-02 18:12:12.427179] Iteration 3300, train loss = 0.200699, train accuracy = 0.976562\n",
      "[2018-06-02 18:12:36.555179] Iteration 3400, train loss = 0.148825, train accuracy = 1.000000\n",
      "[2018-06-02 18:13:00.548179] Iteration 3500, train loss = 0.196266, train accuracy = 0.968750\n",
      "[2018-06-02 18:13:24.632179] Iteration 3600, train loss = 0.146557, train accuracy = 1.000000\n",
      "[2018-06-02 18:13:48.719179] Iteration 3700, train loss = 0.156657, train accuracy = 0.992188\n",
      "[2018-06-02 18:14:12.835179] Iteration 3800, train loss = 0.141353, train accuracy = 1.000000\n",
      "[2018-06-02 18:14:36.876179] Iteration 3900, train loss = 0.180220, train accuracy = 0.992188\n",
      "[2018-06-02 18:15:00.902179] Iteration 4000, train loss = 0.155502, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921000\n",
      "[2018-06-02 18:15:30.160179] Iteration 4100, train loss = 0.210641, train accuracy = 0.960938\n",
      "[2018-06-02 18:15:54.379179] Iteration 4200, train loss = 0.161669, train accuracy = 0.984375\n",
      "[2018-06-02 18:16:18.389179] Iteration 4300, train loss = 0.153602, train accuracy = 0.992188\n",
      "[2018-06-02 18:16:42.507179] Iteration 4400, train loss = 0.142633, train accuracy = 1.000000\n",
      "[2018-06-02 18:17:06.596179] Iteration 4500, train loss = 0.163730, train accuracy = 0.992188\n",
      "[2018-06-02 18:17:30.658179] Iteration 4600, train loss = 0.166680, train accuracy = 1.000000\n",
      "[2018-06-02 18:17:54.695179] Iteration 4700, train loss = 0.153956, train accuracy = 0.992188\n",
      "[2018-06-02 18:18:18.853179] Iteration 4800, train loss = 0.147460, train accuracy = 1.000000\n",
      "[2018-06-02 18:18:42.922179] Iteration 4900, train loss = 0.168127, train accuracy = 0.976562\n",
      "[2018-06-02 18:19:07.005179] Iteration 5000, train loss = 0.170017, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.921500\n",
      "[2018-06-02 18:19:36.396179] Iteration 5100, train loss = 0.176123, train accuracy = 0.984375\n",
      "[2018-06-02 18:20:00.509179] Iteration 5200, train loss = 0.164253, train accuracy = 0.992188\n",
      "[2018-06-02 18:20:24.657179] Iteration 5300, train loss = 0.159104, train accuracy = 1.000000\n",
      "[2018-06-02 18:20:48.655179] Iteration 5400, train loss = 0.164350, train accuracy = 1.000000\n",
      "[2018-06-02 18:21:12.740179] Iteration 5500, train loss = 0.161238, train accuracy = 0.984375\n",
      "[2018-06-02 18:21:36.806179] Iteration 5600, train loss = 0.146962, train accuracy = 1.000000\n",
      "[2018-06-02 18:22:00.957179] Iteration 5700, train loss = 0.160916, train accuracy = 0.992188\n",
      "[2018-06-02 18:22:25.066179] Iteration 5800, train loss = 0.147268, train accuracy = 1.000000\n",
      "[2018-06-02 18:22:49.270179] Iteration 5900, train loss = 0.155493, train accuracy = 0.992188\n",
      "[2018-06-02 18:23:13.284179] Iteration 6000, train loss = 0.160284, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922500\n",
      "[2018-06-02 18:23:42.706179] Iteration 6100, train loss = 0.147113, train accuracy = 1.000000\n",
      "[2018-06-02 18:24:06.877179] Iteration 6200, train loss = 0.144017, train accuracy = 1.000000\n",
      "[2018-06-02 18:24:30.974179] Iteration 6300, train loss = 0.189554, train accuracy = 0.984375\n",
      "[2018-06-02 18:24:55.012179] Iteration 6400, train loss = 0.165597, train accuracy = 1.000000\n",
      "[2018-06-02 18:25:19.121179] Iteration 6500, train loss = 0.157163, train accuracy = 0.992188\n",
      "[2018-06-02 18:25:43.151179] Iteration 6600, train loss = 0.156544, train accuracy = 0.992188\n",
      "[2018-06-02 18:26:07.316179] Iteration 6700, train loss = 0.179879, train accuracy = 0.992188\n",
      "[2018-06-02 18:26:31.489179] Iteration 6800, train loss = 0.150524, train accuracy = 0.992188\n",
      "[2018-06-02 18:26:55.593179] Iteration 6900, train loss = 0.174010, train accuracy = 0.984375\n",
      "[2018-06-02 18:27:19.839179] Iteration 7000, train loss = 0.152319, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922700\n",
      "[2018-06-02 18:27:49.218179] Iteration 7100, train loss = 0.156989, train accuracy = 1.000000\n",
      "[2018-06-02 18:28:13.275179] Iteration 7200, train loss = 0.176952, train accuracy = 0.992188\n",
      "[2018-06-02 18:28:37.309179] Iteration 7300, train loss = 0.143910, train accuracy = 1.000000\n",
      "[2018-06-02 18:29:01.420179] Iteration 7400, train loss = 0.173391, train accuracy = 0.984375\n",
      "[2018-06-02 18:29:25.478179] Iteration 7500, train loss = 0.143729, train accuracy = 1.000000\n",
      "[2018-06-02 18:29:49.648179] Iteration 7600, train loss = 0.151597, train accuracy = 0.992188\n",
      "[2018-06-02 18:30:13.822179] Iteration 7700, train loss = 0.182846, train accuracy = 0.976562\n",
      "[2018-06-02 18:30:37.958179] Iteration 7800, train loss = 0.146108, train accuracy = 1.000000\n",
      "[2018-06-02 18:31:02.044179] Iteration 7900, train loss = 0.152269, train accuracy = 0.992188\n",
      "[2018-06-02 18:31:26.194179] Iteration 8000, train loss = 0.177270, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.923400\n",
      "[2018-06-02 18:31:55.500179] Iteration 8100, train loss = 0.161695, train accuracy = 1.000000\n",
      "[2018-06-02 18:32:19.698179] Iteration 8200, train loss = 0.209441, train accuracy = 0.976562\n",
      "[2018-06-02 18:32:43.792179] Iteration 8300, train loss = 0.150043, train accuracy = 1.000000\n",
      "[2018-06-02 18:33:07.917179] Iteration 8400, train loss = 0.160872, train accuracy = 0.992188\n",
      "[2018-06-02 18:33:31.869179] Iteration 8500, train loss = 0.160281, train accuracy = 0.992188\n",
      "[2018-06-02 18:33:55.999179] Iteration 8600, train loss = 0.144377, train accuracy = 1.000000\n",
      "[2018-06-02 18:34:20.145179] Iteration 8700, train loss = 0.157716, train accuracy = 0.992188\n",
      "[2018-06-02 18:34:44.226179] Iteration 8800, train loss = 0.150417, train accuracy = 1.000000\n",
      "[2018-06-02 18:35:08.354179] Iteration 8900, train loss = 0.145715, train accuracy = 1.000000\n",
      "[2018-06-02 18:35:32.401179] Iteration 9000, train loss = 0.145584, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "[2018-06-02 18:36:01.650179] Iteration 9100, train loss = 0.198899, train accuracy = 0.976562\n",
      "[2018-06-02 18:36:25.747179] Iteration 9200, train loss = 0.148194, train accuracy = 1.000000\n",
      "[2018-06-02 18:36:49.922179] Iteration 9300, train loss = 0.144056, train accuracy = 1.000000\n",
      "[2018-06-02 18:37:13.948179] Iteration 9400, train loss = 0.160894, train accuracy = 0.984375\n",
      "[2018-06-02 18:37:38.069179] Iteration 9500, train loss = 0.152148, train accuracy = 0.992188\n",
      "[2018-06-02 18:38:02.175179] Iteration 9600, train loss = 0.181886, train accuracy = 0.984375\n",
      "[2018-06-02 18:38:26.267179] Iteration 9700, train loss = 0.153249, train accuracy = 1.000000\n",
      "[2018-06-02 18:38:50.369179] Iteration 9800, train loss = 0.166395, train accuracy = 0.992188\n",
      "[2018-06-02 18:39:14.500179] Iteration 9900, train loss = 0.149252, train accuracy = 0.992188\n",
      "[2018-06-02 18:39:38.555179] Iteration 10000, train loss = 0.144736, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-02 18:40:07.900179] Iteration 10100, train loss = 0.173260, train accuracy = 0.984375\n",
      "[2018-06-02 18:40:31.973179] Iteration 10200, train loss = 0.155347, train accuracy = 1.000000\n",
      "[2018-06-02 18:40:56.198179] Iteration 10300, train loss = 0.158628, train accuracy = 0.992188\n",
      "[2018-06-02 18:41:20.264179] Iteration 10400, train loss = 0.148471, train accuracy = 1.000000\n",
      "[2018-06-02 18:41:44.351179] Iteration 10500, train loss = 0.141785, train accuracy = 1.000000\n",
      "[2018-06-02 18:42:08.293179] Iteration 10600, train loss = 0.162166, train accuracy = 0.992188\n",
      "[2018-06-02 18:42:32.301179] Iteration 10700, train loss = 0.155560, train accuracy = 1.000000\n",
      "[2018-06-02 18:42:56.509179] Iteration 10800, train loss = 0.150312, train accuracy = 1.000000\n",
      "[2018-06-02 18:43:20.724179] Iteration 10900, train loss = 0.142778, train accuracy = 1.000000\n",
      "[2018-06-02 18:43:44.859179] Iteration 11000, train loss = 0.170733, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-02 18:44:14.155179] Iteration 11100, train loss = 0.142421, train accuracy = 1.000000\n",
      "[2018-06-02 18:44:38.318179] Iteration 11200, train loss = 0.151927, train accuracy = 1.000000\n",
      "[2018-06-02 18:45:02.435179] Iteration 11300, train loss = 0.151317, train accuracy = 1.000000\n",
      "[2018-06-02 18:45:26.499179] Iteration 11400, train loss = 0.166557, train accuracy = 0.984375\n",
      "[2018-06-02 18:45:50.530179] Iteration 11500, train loss = 0.167228, train accuracy = 1.000000\n",
      "[2018-06-02 18:46:14.604179] Iteration 11600, train loss = 0.144927, train accuracy = 1.000000\n",
      "[2018-06-02 18:46:38.732179] Iteration 11700, train loss = 0.142116, train accuracy = 1.000000\n",
      "[2018-06-02 18:47:02.795179] Iteration 11800, train loss = 0.151737, train accuracy = 1.000000\n",
      "[2018-06-02 18:47:26.950179] Iteration 11900, train loss = 0.157034, train accuracy = 1.000000\n",
      "[2018-06-02 18:47:51.001179] Iteration 12000, train loss = 0.167936, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 18:48:20.315179] Iteration 12100, train loss = 0.149719, train accuracy = 1.000000\n",
      "[2018-06-02 18:48:44.417179] Iteration 12200, train loss = 0.152881, train accuracy = 1.000000\n",
      "[2018-06-02 18:49:08.580179] Iteration 12300, train loss = 0.148320, train accuracy = 1.000000\n",
      "[2018-06-02 18:49:32.710179] Iteration 12400, train loss = 0.154653, train accuracy = 1.000000\n",
      "[2018-06-02 18:49:56.826179] Iteration 12500, train loss = 0.146665, train accuracy = 1.000000\n",
      "[2018-06-02 18:50:20.936179] Iteration 12600, train loss = 0.150116, train accuracy = 1.000000\n",
      "[2018-06-02 18:50:45.096179] Iteration 12700, train loss = 0.165195, train accuracy = 0.984375\n",
      "[2018-06-02 18:51:09.151179] Iteration 12800, train loss = 0.150348, train accuracy = 1.000000\n",
      "[2018-06-02 18:51:33.206179] Iteration 12900, train loss = 0.168049, train accuracy = 0.992188\n",
      "[2018-06-02 18:51:57.319179] Iteration 13000, train loss = 0.145285, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "[2018-06-02 18:52:26.614179] Iteration 13100, train loss = 0.154808, train accuracy = 0.992188\n",
      "[2018-06-02 18:52:50.701179] Iteration 13200, train loss = 0.149033, train accuracy = 1.000000\n",
      "[2018-06-02 18:53:14.865179] Iteration 13300, train loss = 0.145032, train accuracy = 1.000000\n",
      "[2018-06-02 18:53:38.934179] Iteration 13400, train loss = 0.154912, train accuracy = 0.992188\n",
      "[2018-06-02 18:54:03.056179] Iteration 13500, train loss = 0.162867, train accuracy = 0.992188\n",
      "[2018-06-02 18:54:27.160179] Iteration 13600, train loss = 0.147690, train accuracy = 1.000000\n",
      "[2018-06-02 18:54:51.189179] Iteration 13700, train loss = 0.151150, train accuracy = 0.992188\n",
      "[2018-06-02 18:55:15.274179] Iteration 13800, train loss = 0.144704, train accuracy = 1.000000\n",
      "[2018-06-02 18:55:39.372179] Iteration 13900, train loss = 0.150747, train accuracy = 1.000000\n",
      "[2018-06-02 18:56:03.467179] Iteration 14000, train loss = 0.161584, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.925200\n",
      "[2018-06-02 18:56:32.788179] Iteration 14100, train loss = 0.148830, train accuracy = 1.000000\n",
      "[2018-06-02 18:56:57.037179] Iteration 14200, train loss = 0.160416, train accuracy = 0.984375\n",
      "[2018-06-02 18:57:21.232179] Iteration 14300, train loss = 0.142570, train accuracy = 1.000000\n",
      "[2018-06-02 18:57:45.249179] Iteration 14400, train loss = 0.176041, train accuracy = 0.984375\n",
      "[2018-06-02 18:58:09.313179] Iteration 14500, train loss = 0.165630, train accuracy = 0.984375\n",
      "[2018-06-02 18:58:33.319179] Iteration 14600, train loss = 0.144472, train accuracy = 1.000000\n",
      "[2018-06-02 18:58:57.372179] Iteration 14700, train loss = 0.159114, train accuracy = 0.992188\n",
      "[2018-06-02 18:59:21.494179] Iteration 14800, train loss = 0.152814, train accuracy = 0.992188\n",
      "[2018-06-02 18:59:45.661179] Iteration 14900, train loss = 0.147973, train accuracy = 1.000000\n",
      "[2018-06-02 19:00:09.715179] Iteration 15000, train loss = 0.156394, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925400\n",
      "[2018-06-02 19:00:39.029179] Iteration 15100, train loss = 0.150644, train accuracy = 0.992188\n",
      "[2018-06-02 19:01:03.074179] Iteration 15200, train loss = 0.166878, train accuracy = 0.992188\n",
      "[2018-06-02 19:01:27.273179] Iteration 15300, train loss = 0.140332, train accuracy = 1.000000\n",
      "[2018-06-02 19:01:51.338179] Iteration 15400, train loss = 0.148992, train accuracy = 1.000000\n",
      "[2018-06-02 19:02:15.368179] Iteration 15500, train loss = 0.141736, train accuracy = 1.000000\n",
      "[2018-06-02 19:02:39.383179] Iteration 15600, train loss = 0.168340, train accuracy = 0.992188\n",
      "[2018-06-02 19:03:03.514179] Iteration 15700, train loss = 0.153791, train accuracy = 1.000000\n",
      "[2018-06-02 19:03:27.610179] Iteration 15800, train loss = 0.171047, train accuracy = 0.992188\n",
      "[2018-06-02 19:03:51.688179] Iteration 15900, train loss = 0.149592, train accuracy = 0.992188\n",
      "[2018-06-02 19:04:15.863179] Iteration 16000, train loss = 0.159489, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925400\n",
      "[2018-06-02 19:04:45.195179] Iteration 16100, train loss = 0.166719, train accuracy = 0.992188\n",
      "[2018-06-02 19:05:09.332179] Iteration 16200, train loss = 0.146629, train accuracy = 1.000000\n",
      "[2018-06-02 19:05:33.389179] Iteration 16300, train loss = 0.145064, train accuracy = 1.000000\n",
      "[2018-06-02 19:05:57.449179] Iteration 16400, train loss = 0.154302, train accuracy = 0.992188\n",
      "[2018-06-02 19:06:21.556179] Iteration 16500, train loss = 0.152553, train accuracy = 0.992188\n",
      "[2018-06-02 19:06:45.630179] Iteration 16600, train loss = 0.147906, train accuracy = 1.000000\n",
      "[2018-06-02 19:07:09.647179] Iteration 16700, train loss = 0.150328, train accuracy = 1.000000\n",
      "[2018-06-02 19:07:33.766179] Iteration 16800, train loss = 0.175473, train accuracy = 0.984375\n",
      "[2018-06-02 19:07:57.828179] Iteration 16900, train loss = 0.147743, train accuracy = 1.000000\n",
      "[2018-06-02 19:08:21.920179] Iteration 17000, train loss = 0.148373, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925900\n",
      "[2018-06-02 19:08:51.361179] Iteration 17100, train loss = 0.139804, train accuracy = 1.000000\n",
      "[2018-06-02 19:09:15.430179] Iteration 17200, train loss = 0.168133, train accuracy = 0.992188\n",
      "[2018-06-02 19:09:39.509179] Iteration 17300, train loss = 0.139567, train accuracy = 1.000000\n",
      "[2018-06-02 19:10:03.624179] Iteration 17400, train loss = 0.157688, train accuracy = 1.000000\n",
      "[2018-06-02 19:10:27.665179] Iteration 17500, train loss = 0.158067, train accuracy = 0.992188\n",
      "[2018-06-02 19:10:51.678179] Iteration 17600, train loss = 0.159940, train accuracy = 0.992188\n",
      "[2018-06-02 19:11:15.815179] Iteration 17700, train loss = 0.140709, train accuracy = 1.000000\n",
      "[2018-06-02 19:11:39.872179] Iteration 17800, train loss = 0.152911, train accuracy = 1.000000\n",
      "[2018-06-02 19:12:03.975179] Iteration 17900, train loss = 0.145980, train accuracy = 1.000000\n",
      "[2018-06-02 19:12:28.079179] Iteration 18000, train loss = 0.166516, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925700\n",
      "[2018-06-02 19:12:57.434179] Iteration 18100, train loss = 0.150717, train accuracy = 1.000000\n",
      "[2018-06-02 19:13:21.527179] Iteration 18200, train loss = 0.159628, train accuracy = 0.992188\n",
      "[2018-06-02 19:13:45.594179] Iteration 18300, train loss = 0.144160, train accuracy = 1.000000\n",
      "[2018-06-02 19:14:09.685179] Iteration 18400, train loss = 0.171972, train accuracy = 0.984375\n",
      "[2018-06-02 19:14:33.659179] Iteration 18500, train loss = 0.145827, train accuracy = 1.000000\n",
      "[2018-06-02 19:14:57.631179] Iteration 18600, train loss = 0.146774, train accuracy = 1.000000\n",
      "[2018-06-02 19:15:21.788179] Iteration 18700, train loss = 0.150503, train accuracy = 0.992188\n",
      "[2018-06-02 19:15:45.874179] Iteration 18800, train loss = 0.150329, train accuracy = 0.992188\n",
      "[2018-06-02 19:16:09.914179] Iteration 18900, train loss = 0.149376, train accuracy = 1.000000\n",
      "[2018-06-02 19:16:34.016179] Iteration 19000, train loss = 0.151617, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925400\n",
      "[2018-06-02 19:17:03.241179] Iteration 19100, train loss = 0.165535, train accuracy = 0.992188\n",
      "[2018-06-02 19:17:27.307179] Iteration 19200, train loss = 0.158980, train accuracy = 0.984375\n",
      "[2018-06-02 19:17:51.370179] Iteration 19300, train loss = 0.148253, train accuracy = 1.000000\n",
      "[2018-06-02 19:18:15.442179] Iteration 19400, train loss = 0.147830, train accuracy = 1.000000\n",
      "[2018-06-02 19:18:39.520179] Iteration 19500, train loss = 0.143154, train accuracy = 1.000000\n",
      "[2018-06-02 19:19:03.612179] Iteration 19600, train loss = 0.149902, train accuracy = 1.000000\n",
      "[2018-06-02 19:19:27.636179] Iteration 19700, train loss = 0.152284, train accuracy = 0.992188\n",
      "[2018-06-02 19:19:51.711179] Iteration 19800, train loss = 0.151106, train accuracy = 1.000000\n",
      "[2018-06-02 19:20:15.751179] Iteration 19900, train loss = 0.149365, train accuracy = 1.000000\n",
      "[2018-06-02 19:20:39.813179] Iteration 20000, train loss = 0.148514, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925700\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 19:21:09.107179] Iteration 20100, train loss = 0.161381, train accuracy = 0.992188\n",
      "[2018-06-02 19:21:33.203179] Iteration 20200, train loss = 0.150448, train accuracy = 1.000000\n",
      "[2018-06-02 19:21:57.260179] Iteration 20300, train loss = 0.148432, train accuracy = 1.000000\n",
      "[2018-06-02 19:22:21.443179] Iteration 20400, train loss = 0.140621, train accuracy = 1.000000\n",
      "[2018-06-02 19:22:45.480179] Iteration 20500, train loss = 0.145427, train accuracy = 1.000000\n",
      "[2018-06-02 19:23:09.661179] Iteration 20600, train loss = 0.161779, train accuracy = 0.992188\n",
      "[2018-06-02 19:23:33.692179] Iteration 20700, train loss = 0.138893, train accuracy = 1.000000\n",
      "[2018-06-02 19:23:57.771179] Iteration 20800, train loss = 0.141870, train accuracy = 1.000000\n",
      "[2018-06-02 19:24:21.929179] Iteration 20900, train loss = 0.149715, train accuracy = 1.000000\n",
      "[2018-06-02 19:24:45.915179] Iteration 21000, train loss = 0.156943, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925300\n",
      "[2018-06-02 19:25:15.261179] Iteration 21100, train loss = 0.157756, train accuracy = 1.000000\n",
      "[2018-06-02 19:25:39.284179] Iteration 21200, train loss = 0.147731, train accuracy = 1.000000\n",
      "[2018-06-02 19:26:03.319179] Iteration 21300, train loss = 0.163762, train accuracy = 0.992188\n",
      "[2018-06-02 19:26:27.410179] Iteration 21400, train loss = 0.150072, train accuracy = 1.000000\n",
      "[2018-06-02 19:26:51.583179] Iteration 21500, train loss = 0.154099, train accuracy = 1.000000\n",
      "[2018-06-02 19:27:15.662179] Iteration 21600, train loss = 0.150510, train accuracy = 0.992188\n",
      "[2018-06-02 19:27:39.795179] Iteration 21700, train loss = 0.145526, train accuracy = 1.000000\n",
      "[2018-06-02 19:28:03.965179] Iteration 21800, train loss = 0.176573, train accuracy = 0.984375\n",
      "[2018-06-02 19:28:28.043179] Iteration 21900, train loss = 0.169068, train accuracy = 0.984375\n",
      "[2018-06-02 19:28:52.114179] Iteration 22000, train loss = 0.157765, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925300\n",
      "[2018-06-02 19:29:21.384179] Iteration 22100, train loss = 0.146892, train accuracy = 1.000000\n",
      "[2018-06-02 19:29:45.458179] Iteration 22200, train loss = 0.151980, train accuracy = 0.992188\n",
      "[2018-06-02 19:30:09.577179] Iteration 22300, train loss = 0.157606, train accuracy = 0.992188\n",
      "[2018-06-02 19:30:33.718179] Iteration 22400, train loss = 0.140423, train accuracy = 1.000000\n",
      "[2018-06-02 19:30:57.704179] Iteration 22500, train loss = 0.153280, train accuracy = 0.992188\n",
      "[2018-06-02 19:31:21.844179] Iteration 22600, train loss = 0.149015, train accuracy = 1.000000\n",
      "[2018-06-02 19:31:45.938179] Iteration 22700, train loss = 0.150220, train accuracy = 1.000000\n",
      "[2018-06-02 19:32:10.033179] Iteration 22800, train loss = 0.175971, train accuracy = 0.984375\n",
      "[2018-06-02 19:32:34.133179] Iteration 22900, train loss = 0.146482, train accuracy = 1.000000\n",
      "[2018-06-02 19:32:58.165179] Iteration 23000, train loss = 0.149037, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926000\n",
      "[2018-06-02 19:33:27.432179] Iteration 23100, train loss = 0.142679, train accuracy = 1.000000\n",
      "[2018-06-02 19:33:51.528179] Iteration 23200, train loss = 0.161061, train accuracy = 0.992188\n",
      "[2018-06-02 19:34:15.609179] Iteration 23300, train loss = 0.146235, train accuracy = 1.000000\n",
      "[2018-06-02 19:34:39.687179] Iteration 23400, train loss = 0.141730, train accuracy = 1.000000\n",
      "[2018-06-02 19:35:03.700179] Iteration 23500, train loss = 0.154998, train accuracy = 1.000000\n",
      "[2018-06-02 19:35:27.736179] Iteration 23600, train loss = 0.161367, train accuracy = 1.000000\n",
      "[2018-06-02 19:35:51.860179] Iteration 23700, train loss = 0.150218, train accuracy = 1.000000\n",
      "[2018-06-02 19:36:15.953179] Iteration 23800, train loss = 0.149551, train accuracy = 1.000000\n",
      "[2018-06-02 19:36:40.057179] Iteration 23900, train loss = 0.151347, train accuracy = 0.992188\n",
      "[2018-06-02 19:37:04.112179] Iteration 24000, train loss = 0.153021, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925700\n",
      "[2018-06-02 19:37:33.370179] Iteration 24100, train loss = 0.155376, train accuracy = 0.992188\n",
      "[2018-06-02 19:37:57.492179] Iteration 24200, train loss = 0.150812, train accuracy = 1.000000\n",
      "[2018-06-02 19:38:21.579179] Iteration 24300, train loss = 0.144073, train accuracy = 1.000000\n",
      "[2018-06-02 19:38:45.682179] Iteration 24400, train loss = 0.149768, train accuracy = 1.000000\n",
      "[2018-06-02 19:39:09.740179] Iteration 24500, train loss = 0.144086, train accuracy = 1.000000\n",
      "[2018-06-02 19:39:33.865179] Iteration 24600, train loss = 0.142050, train accuracy = 1.000000\n",
      "[2018-06-02 19:39:57.960179] Iteration 24700, train loss = 0.147136, train accuracy = 1.000000\n",
      "[2018-06-02 19:40:22.050179] Iteration 24800, train loss = 0.155252, train accuracy = 0.992188\n",
      "[2018-06-02 19:40:46.084179] Iteration 24900, train loss = 0.139729, train accuracy = 1.000000\n",
      "[2018-06-02 19:41:10.186179] Iteration 25000, train loss = 0.140595, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926000\n",
      "[2018-06-02 19:41:39.465179] Iteration 25100, train loss = 0.143208, train accuracy = 1.000000\n",
      "[2018-06-02 19:42:03.561179] Iteration 25200, train loss = 0.139163, train accuracy = 1.000000\n",
      "[2018-06-02 19:42:27.602179] Iteration 25300, train loss = 0.153990, train accuracy = 1.000000\n",
      "[2018-06-02 19:42:51.765179] Iteration 25400, train loss = 0.149137, train accuracy = 1.000000\n",
      "[2018-06-02 19:43:15.844179] Iteration 25500, train loss = 0.152665, train accuracy = 1.000000\n",
      "[2018-06-02 19:43:39.929179] Iteration 25600, train loss = 0.151946, train accuracy = 1.000000\n",
      "[2018-06-02 19:44:03.953179] Iteration 25700, train loss = 0.153617, train accuracy = 1.000000\n",
      "[2018-06-02 19:44:28.074179] Iteration 25800, train loss = 0.140778, train accuracy = 1.000000\n",
      "[2018-06-02 19:44:52.086179] Iteration 25900, train loss = 0.145891, train accuracy = 1.000000\n",
      "[2018-06-02 19:45:16.266179] Iteration 26000, train loss = 0.157556, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925900\n",
      "[2018-06-02 19:45:45.668179] Iteration 26100, train loss = 0.143755, train accuracy = 1.000000\n",
      "[2018-06-02 19:46:09.971179] Iteration 26200, train loss = 0.164609, train accuracy = 0.984375\n",
      "[2018-06-02 19:46:34.017179] Iteration 26300, train loss = 0.144032, train accuracy = 1.000000\n",
      "[2018-06-02 19:46:58.141179] Iteration 26400, train loss = 0.140488, train accuracy = 1.000000\n",
      "[2018-06-02 19:47:22.216179] Iteration 26500, train loss = 0.145475, train accuracy = 1.000000\n",
      "[2018-06-02 19:47:46.316179] Iteration 26600, train loss = 0.153551, train accuracy = 1.000000\n",
      "[2018-06-02 19:48:10.347179] Iteration 26700, train loss = 0.151544, train accuracy = 0.992188\n",
      "[2018-06-02 19:48:34.465179] Iteration 26800, train loss = 0.151947, train accuracy = 1.000000\n",
      "[2018-06-02 19:48:58.525179] Iteration 26900, train loss = 0.152158, train accuracy = 1.000000\n",
      "[2018-06-02 19:49:22.561179] Iteration 27000, train loss = 0.144865, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926000\n",
      "[2018-06-02 19:49:51.877179] Iteration 27100, train loss = 0.145732, train accuracy = 1.000000\n",
      "[2018-06-02 19:50:15.972179] Iteration 27200, train loss = 0.147083, train accuracy = 1.000000\n",
      "[2018-06-02 19:50:40.045179] Iteration 27300, train loss = 0.155594, train accuracy = 0.984375\n",
      "[2018-06-02 19:51:04.131179] Iteration 27400, train loss = 0.167091, train accuracy = 0.992188\n",
      "[2018-06-02 19:51:28.233179] Iteration 27500, train loss = 0.145583, train accuracy = 1.000000\n",
      "[2018-06-02 19:51:52.238179] Iteration 27600, train loss = 0.154224, train accuracy = 1.000000\n",
      "[2018-06-02 19:52:16.337179] Iteration 27700, train loss = 0.147453, train accuracy = 1.000000\n",
      "[2018-06-02 19:52:40.440179] Iteration 27800, train loss = 0.146458, train accuracy = 1.000000\n",
      "[2018-06-02 19:53:04.529179] Iteration 27900, train loss = 0.144811, train accuracy = 1.000000\n",
      "[2018-06-02 19:53:28.644179] Iteration 28000, train loss = 0.143013, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926400\n",
      "[2018-06-02 19:53:58.026179] Iteration 28100, train loss = 0.149832, train accuracy = 1.000000\n",
      "[2018-06-02 19:54:22.222179] Iteration 28200, train loss = 0.142761, train accuracy = 1.000000\n",
      "[2018-06-02 19:54:46.276179] Iteration 28300, train loss = 0.171936, train accuracy = 0.992188\n",
      "[2018-06-02 19:55:10.346179] Iteration 28400, train loss = 0.153383, train accuracy = 1.000000\n",
      "[2018-06-02 19:55:34.399179] Iteration 28500, train loss = 0.161640, train accuracy = 0.984375\n",
      "[2018-06-02 19:55:58.482179] Iteration 28600, train loss = 0.155125, train accuracy = 0.992188\n",
      "[2018-06-02 19:56:22.612179] Iteration 28700, train loss = 0.144943, train accuracy = 1.000000\n",
      "[2018-06-02 19:56:46.698179] Iteration 28800, train loss = 0.153144, train accuracy = 1.000000\n",
      "[2018-06-02 19:57:11.079179] Iteration 28900, train loss = 0.158134, train accuracy = 0.992188\n",
      "[2018-06-02 19:57:35.401179] Iteration 29000, train loss = 0.152145, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925700\n",
      "[2018-06-02 19:58:04.910179] Iteration 29100, train loss = 0.151578, train accuracy = 1.000000\n",
      "[2018-06-02 19:58:29.102179] Iteration 29200, train loss = 0.153957, train accuracy = 0.992188\n",
      "[2018-06-02 19:58:53.369179] Iteration 29300, train loss = 0.178485, train accuracy = 0.976562\n",
      "[2018-06-02 19:59:17.735179] Iteration 29400, train loss = 0.144473, train accuracy = 1.000000\n",
      "[2018-06-02 19:59:41.862179] Iteration 29500, train loss = 0.166578, train accuracy = 0.992188\n",
      "[2018-06-02 20:00:05.992179] Iteration 29600, train loss = 0.155030, train accuracy = 0.992188\n",
      "[2018-06-02 20:00:30.170179] Iteration 29700, train loss = 0.150970, train accuracy = 1.000000\n",
      "[2018-06-02 20:00:54.267179] Iteration 29800, train loss = 0.157464, train accuracy = 0.992188\n",
      "[2018-06-02 20:01:18.358179] Iteration 29900, train loss = 0.144653, train accuracy = 1.000000\n",
      "[2018-06-02 20:01:42.461179] Iteration 30000, train loss = 0.191445, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.926600\n",
      "[2018-06-02 20:02:11.690179] Iteration 30100, train loss = 0.146994, train accuracy = 1.000000\n",
      "[2018-06-02 20:02:35.751179] Iteration 30200, train loss = 0.148917, train accuracy = 0.992188\n",
      "[2018-06-02 20:02:59.828179] Iteration 30300, train loss = 0.146084, train accuracy = 1.000000\n",
      "[2018-06-02 20:03:23.856179] Iteration 30400, train loss = 0.159286, train accuracy = 0.984375\n",
      "[2018-06-02 20:03:47.900179] Iteration 30500, train loss = 0.145133, train accuracy = 1.000000\n",
      "[2018-06-02 20:04:11.958179] Iteration 30600, train loss = 0.161641, train accuracy = 0.992188\n",
      "[2018-06-02 20:04:36.013179] Iteration 30700, train loss = 0.155362, train accuracy = 0.992188\n",
      "[2018-06-02 20:05:00.176179] Iteration 30800, train loss = 0.142450, train accuracy = 1.000000\n",
      "[2018-06-02 20:05:24.310179] Iteration 30900, train loss = 0.143809, train accuracy = 1.000000\n",
      "[2018-06-02 20:05:48.401179] Iteration 31000, train loss = 0.150858, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925900\n",
      "[2018-06-02 20:06:17.661179] Iteration 31100, train loss = 0.153402, train accuracy = 0.992188\n",
      "[2018-06-02 20:06:41.774179] Iteration 31200, train loss = 0.143758, train accuracy = 1.000000\n",
      "[2018-06-02 20:07:05.800179] Iteration 31300, train loss = 0.157585, train accuracy = 0.992188\n",
      "[2018-06-02 20:07:29.895179] Iteration 31400, train loss = 0.146826, train accuracy = 1.000000\n",
      "[2018-06-02 20:07:54.003179] Iteration 31500, train loss = 0.141939, train accuracy = 1.000000\n",
      "[2018-06-02 20:08:18.083179] Iteration 31600, train loss = 0.147978, train accuracy = 1.000000\n",
      "[2018-06-02 20:08:42.259179] Iteration 31700, train loss = 0.145382, train accuracy = 1.000000\n",
      "[2018-06-02 20:09:06.362179] Iteration 31800, train loss = 0.147789, train accuracy = 1.000000\n",
      "[2018-06-02 20:09:30.420179] Iteration 31900, train loss = 0.153836, train accuracy = 1.000000\n",
      "[2018-06-02 20:09:54.538179] Iteration 32000, train loss = 0.150775, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926400\n",
      "[2018-06-02 20:10:23.786179] Iteration 32100, train loss = 0.176566, train accuracy = 0.992188\n",
      "[2018-06-02 20:10:47.778179] Iteration 32200, train loss = 0.144862, train accuracy = 1.000000\n",
      "[2018-06-02 20:11:11.852179] Iteration 32300, train loss = 0.143731, train accuracy = 1.000000\n",
      "[2018-06-02 20:11:35.862179] Iteration 32400, train loss = 0.162154, train accuracy = 0.992188\n",
      "[2018-06-02 20:11:59.883179] Iteration 32500, train loss = 0.150840, train accuracy = 1.000000\n",
      "[2018-06-02 20:12:23.950179] Iteration 32600, train loss = 0.154917, train accuracy = 1.000000\n",
      "[2018-06-02 20:12:48.006179] Iteration 32700, train loss = 0.148549, train accuracy = 0.992188\n",
      "[2018-06-02 20:13:12.168179] Iteration 32800, train loss = 0.140877, train accuracy = 1.000000\n",
      "[2018-06-02 20:13:36.153179] Iteration 32900, train loss = 0.142354, train accuracy = 1.000000\n",
      "[2018-06-02 20:14:00.196179] Iteration 33000, train loss = 0.154700, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926500\n",
      "[2018-06-02 20:14:29.538179] Iteration 33100, train loss = 0.143652, train accuracy = 1.000000\n",
      "[2018-06-02 20:14:53.692179] Iteration 33200, train loss = 0.147894, train accuracy = 1.000000\n",
      "[2018-06-02 20:15:17.765179] Iteration 33300, train loss = 0.150837, train accuracy = 1.000000\n",
      "[2018-06-02 20:15:41.857179] Iteration 33400, train loss = 0.155312, train accuracy = 0.992188\n",
      "[2018-06-02 20:16:05.892179] Iteration 33500, train loss = 0.180785, train accuracy = 0.992188\n",
      "[2018-06-02 20:16:30.016179] Iteration 33600, train loss = 0.151095, train accuracy = 1.000000\n",
      "[2018-06-02 20:16:54.097179] Iteration 33700, train loss = 0.150641, train accuracy = 1.000000\n",
      "[2018-06-02 20:17:18.181179] Iteration 33800, train loss = 0.146199, train accuracy = 1.000000\n",
      "[2018-06-02 20:17:42.202179] Iteration 33900, train loss = 0.146165, train accuracy = 1.000000\n",
      "[2018-06-02 20:18:06.495179] Iteration 34000, train loss = 0.141644, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926400\n",
      "[2018-06-02 20:18:35.693179] Iteration 34100, train loss = 0.148146, train accuracy = 1.000000\n",
      "[2018-06-02 20:18:59.907179] Iteration 34200, train loss = 0.146447, train accuracy = 1.000000\n",
      "[2018-06-02 20:19:23.938179] Iteration 34300, train loss = 0.148652, train accuracy = 1.000000\n",
      "[2018-06-02 20:19:48.126179] Iteration 34400, train loss = 0.143709, train accuracy = 1.000000\n",
      "[2018-06-02 20:20:12.191179] Iteration 34500, train loss = 0.150043, train accuracy = 1.000000\n",
      "[2018-06-02 20:20:36.287179] Iteration 34600, train loss = 0.146484, train accuracy = 1.000000\n",
      "[2018-06-02 20:21:00.756179] Iteration 34700, train loss = 0.154168, train accuracy = 1.000000\n",
      "[2018-06-02 20:21:25.379179] Iteration 34800, train loss = 0.180024, train accuracy = 0.984375\n",
      "[2018-06-02 20:21:49.530179] Iteration 34900, train loss = 0.142749, train accuracy = 1.000000\n",
      "[2018-06-02 20:22:13.623179] Iteration 35000, train loss = 0.152823, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926000\n",
      "[2018-06-02 20:22:42.985179] Iteration 35100, train loss = 0.157934, train accuracy = 0.992188\n",
      "[2018-06-02 20:23:07.013179] Iteration 35200, train loss = 0.151676, train accuracy = 1.000000\n",
      "[2018-06-02 20:23:31.051179] Iteration 35300, train loss = 0.154884, train accuracy = 0.992188\n",
      "[2018-06-02 20:23:55.047179] Iteration 35400, train loss = 0.153102, train accuracy = 0.984375\n",
      "[2018-06-02 20:24:19.147179] Iteration 35500, train loss = 0.159586, train accuracy = 0.984375\n",
      "[2018-06-02 20:24:43.185179] Iteration 35600, train loss = 0.147594, train accuracy = 1.000000\n",
      "[2018-06-02 20:25:07.367179] Iteration 35700, train loss = 0.196796, train accuracy = 0.984375\n",
      "[2018-06-02 20:25:31.494179] Iteration 35800, train loss = 0.155994, train accuracy = 1.000000\n",
      "[2018-06-02 20:25:55.639179] Iteration 35900, train loss = 0.143655, train accuracy = 1.000000\n",
      "[2018-06-02 20:26:19.758179] Iteration 36000, train loss = 0.145114, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.926100\n",
      "[2018-06-02 20:26:49.013179] Iteration 36100, train loss = 0.156546, train accuracy = 0.992188\n",
      "[2018-06-02 20:27:13.112179] Iteration 36200, train loss = 0.148361, train accuracy = 1.000000\n",
      "[2018-06-02 20:27:37.174179] Iteration 36300, train loss = 0.155487, train accuracy = 1.000000\n",
      "[2018-06-02 20:28:01.198179] Iteration 36400, train loss = 0.144898, train accuracy = 1.000000\n",
      "[2018-06-02 20:28:25.216179] Iteration 36500, train loss = 0.144889, train accuracy = 1.000000\n",
      "[2018-06-02 20:28:49.192179] Iteration 36600, train loss = 0.147033, train accuracy = 1.000000\n",
      "[2018-06-02 20:29:13.323179] Iteration 36700, train loss = 0.172079, train accuracy = 0.992188\n",
      "[2018-06-02 20:29:37.533179] Iteration 36800, train loss = 0.142931, train accuracy = 1.000000\n",
      "[2018-06-02 20:30:01.624179] Iteration 36900, train loss = 0.153283, train accuracy = 0.992188\n",
      "[2018-06-02 20:30:25.736179] Iteration 37000, train loss = 0.173047, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.926100\n",
      "[2018-06-02 20:30:55.048179] Iteration 37100, train loss = 0.142026, train accuracy = 1.000000\n",
      "[2018-06-02 20:31:19.156179] Iteration 37200, train loss = 0.146878, train accuracy = 1.000000\n",
      "[2018-06-02 20:31:43.258179] Iteration 37300, train loss = 0.150883, train accuracy = 1.000000\n",
      "[2018-06-02 20:32:07.309179] Iteration 37400, train loss = 0.143139, train accuracy = 1.000000\n",
      "[2018-06-02 20:32:31.447179] Iteration 37500, train loss = 0.143753, train accuracy = 1.000000\n",
      "[2018-06-02 20:32:55.604179] Iteration 37600, train loss = 0.158512, train accuracy = 1.000000\n",
      "[2018-06-02 20:33:19.685179] Iteration 37700, train loss = 0.171716, train accuracy = 0.992188\n",
      "[2018-06-02 20:33:43.845179] Iteration 37800, train loss = 0.143203, train accuracy = 1.000000\n",
      "[2018-06-02 20:34:07.973179] Iteration 37900, train loss = 0.157771, train accuracy = 0.992188\n",
      "[2018-06-02 20:34:32.035179] Iteration 38000, train loss = 0.172805, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.925900\n",
      "[2018-06-02 20:35:01.312179] Iteration 38100, train loss = 0.146470, train accuracy = 1.000000\n",
      "[2018-06-02 20:35:25.689179] Iteration 38200, train loss = 0.153123, train accuracy = 1.000000\n",
      "[2018-06-02 20:35:50.199179] Iteration 38300, train loss = 0.141355, train accuracy = 1.000000\n",
      "[2018-06-02 20:36:14.536179] Iteration 38400, train loss = 0.144196, train accuracy = 1.000000\n",
      "[2018-06-02 20:36:38.732179] Iteration 38500, train loss = 0.147499, train accuracy = 1.000000\n",
      "[2018-06-02 20:37:03.221179] Iteration 38600, train loss = 0.150875, train accuracy = 1.000000\n",
      "[2018-06-02 20:37:27.607179] Iteration 38700, train loss = 0.153278, train accuracy = 1.000000\n",
      "[2018-06-02 20:37:51.959179] Iteration 38800, train loss = 0.170622, train accuracy = 0.992188\n",
      "[2018-06-02 20:38:16.066179] Iteration 38900, train loss = 0.151349, train accuracy = 0.992188\n",
      "[2018-06-02 20:38:40.218179] Iteration 39000, train loss = 0.169186, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.926300\n",
      "[2018-06-02 20:39:09.501179] Iteration 39100, train loss = 0.144295, train accuracy = 1.000000\n",
      "[2018-06-02 20:39:33.594179] Iteration 39200, train loss = 0.144873, train accuracy = 1.000000\n",
      "[2018-06-02 20:39:57.786179] Iteration 39300, train loss = 0.147908, train accuracy = 1.000000\n",
      "[2018-06-02 20:40:21.973179] Iteration 39400, train loss = 0.148477, train accuracy = 0.992188\n",
      "[2018-06-02 20:40:46.187179] Iteration 39500, train loss = 0.157564, train accuracy = 0.992188\n",
      "[2018-06-02 20:41:10.650179] Iteration 39600, train loss = 0.147218, train accuracy = 1.000000\n",
      "[2018-06-02 20:41:34.831179] Iteration 39700, train loss = 0.142328, train accuracy = 1.000000\n",
      "[2018-06-02 20:41:58.954179] Iteration 39800, train loss = 0.143849, train accuracy = 1.000000\n",
      "[2018-06-02 20:42:23.091179] Iteration 39900, train loss = 0.140865, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "curr_lr = 0\n",
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二轮 量化\n",
    "prune_rate = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.925700\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.125      -0.125       0.0625      0.03125     0.00133434 -0.01295556\n",
      "  0.00145553  0.00607884 -0.02535522 -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.75)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 20:48:16.336179] Iteration 100, train loss = 0.210692, train accuracy = 0.984375\n",
      "[2018-06-02 20:48:40.386179] Iteration 200, train loss = 0.210892, train accuracy = 0.976562\n",
      "[2018-06-02 20:49:04.420179] Iteration 300, train loss = 0.193509, train accuracy = 0.984375\n",
      "[2018-06-02 20:49:28.505179] Iteration 400, train loss = 0.159760, train accuracy = 1.000000\n",
      "[2018-06-02 20:49:52.646179] Iteration 500, train loss = 0.169668, train accuracy = 0.984375\n",
      "[2018-06-02 20:50:16.724179] Iteration 600, train loss = 0.182863, train accuracy = 0.984375\n",
      "[2018-06-02 20:50:40.973179] Iteration 700, train loss = 0.168829, train accuracy = 0.992188\n",
      "[2018-06-02 20:51:05.053179] Iteration 800, train loss = 0.154280, train accuracy = 1.000000\n",
      "[2018-06-02 20:51:29.153179] Iteration 900, train loss = 0.151885, train accuracy = 1.000000\n",
      "[2018-06-02 20:51:53.190179] Iteration 1000, train loss = 0.176439, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.919700\n",
      "[2018-06-02 20:52:22.475179] Iteration 1100, train loss = 0.150860, train accuracy = 1.000000\n",
      "[2018-06-02 20:52:46.594179] Iteration 1200, train loss = 0.145297, train accuracy = 1.000000\n",
      "[2018-06-02 20:53:10.672179] Iteration 1300, train loss = 0.150713, train accuracy = 0.992188\n",
      "[2018-06-02 20:53:34.723179] Iteration 1400, train loss = 0.146527, train accuracy = 1.000000\n",
      "[2018-06-02 20:53:58.810179] Iteration 1500, train loss = 0.159839, train accuracy = 0.992188\n",
      "[2018-06-02 20:54:22.782179] Iteration 1600, train loss = 0.143215, train accuracy = 1.000000\n",
      "[2018-06-02 20:54:46.903179] Iteration 1700, train loss = 0.181183, train accuracy = 0.984375\n",
      "[2018-06-02 20:55:10.915179] Iteration 1800, train loss = 0.176976, train accuracy = 0.984375\n",
      "[2018-06-02 20:55:34.913179] Iteration 1900, train loss = 0.160519, train accuracy = 0.984375\n",
      "[2018-06-02 20:55:59.042179] Iteration 2000, train loss = 0.160614, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.921100\n",
      "[2018-06-02 20:56:28.335179] Iteration 2100, train loss = 0.162496, train accuracy = 1.000000\n",
      "[2018-06-02 20:56:52.506179] Iteration 2200, train loss = 0.143097, train accuracy = 1.000000\n",
      "[2018-06-02 20:57:16.617179] Iteration 2300, train loss = 0.182245, train accuracy = 0.984375\n",
      "[2018-06-02 20:57:40.755179] Iteration 2400, train loss = 0.185970, train accuracy = 0.976562\n",
      "[2018-06-02 20:58:04.829179] Iteration 2500, train loss = 0.163762, train accuracy = 0.992188\n",
      "[2018-06-02 20:58:28.902179] Iteration 2600, train loss = 0.150488, train accuracy = 1.000000\n",
      "[2018-06-02 20:58:53.023179] Iteration 2700, train loss = 0.157675, train accuracy = 0.992188\n",
      "[2018-06-02 20:59:17.135179] Iteration 2800, train loss = 0.179517, train accuracy = 0.976562\n",
      "[2018-06-02 20:59:41.238179] Iteration 2900, train loss = 0.151477, train accuracy = 1.000000\n",
      "[2018-06-02 21:00:05.405179] Iteration 3000, train loss = 0.153517, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.921400\n",
      "[2018-06-02 21:00:34.605179] Iteration 3100, train loss = 0.162670, train accuracy = 0.992188\n",
      "[2018-06-02 21:00:58.715179] Iteration 3200, train loss = 0.141413, train accuracy = 1.000000\n",
      "[2018-06-02 21:01:22.839179] Iteration 3300, train loss = 0.155489, train accuracy = 1.000000\n",
      "[2018-06-02 21:01:46.883179] Iteration 3400, train loss = 0.176481, train accuracy = 0.992188\n",
      "[2018-06-02 21:02:10.944179] Iteration 3500, train loss = 0.175937, train accuracy = 0.992188\n",
      "[2018-06-02 21:02:35.062179] Iteration 3600, train loss = 0.161807, train accuracy = 1.000000\n",
      "[2018-06-02 21:02:59.177179] Iteration 3700, train loss = 0.145022, train accuracy = 1.000000\n",
      "[2018-06-02 21:03:23.271179] Iteration 3800, train loss = 0.148878, train accuracy = 0.992188\n",
      "[2018-06-02 21:03:47.333179] Iteration 3900, train loss = 0.168989, train accuracy = 0.992188\n",
      "[2018-06-02 21:04:11.448179] Iteration 4000, train loss = 0.156644, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922300\n",
      "[2018-06-02 21:04:40.781179] Iteration 4100, train loss = 0.170386, train accuracy = 0.992188\n",
      "[2018-06-02 21:05:04.851179] Iteration 4200, train loss = 0.168928, train accuracy = 0.992188\n",
      "[2018-06-02 21:05:28.905179] Iteration 4300, train loss = 0.148033, train accuracy = 1.000000\n",
      "[2018-06-02 21:05:53.037179] Iteration 4400, train loss = 0.176847, train accuracy = 0.984375\n",
      "[2018-06-02 21:06:17.156179] Iteration 4500, train loss = 0.152455, train accuracy = 0.992188\n",
      "[2018-06-02 21:06:41.254179] Iteration 4600, train loss = 0.154135, train accuracy = 0.992188\n",
      "[2018-06-02 21:07:05.298179] Iteration 4700, train loss = 0.145174, train accuracy = 1.000000\n",
      "[2018-06-02 21:07:29.395179] Iteration 4800, train loss = 0.158232, train accuracy = 0.992188\n",
      "[2018-06-02 21:07:53.458179] Iteration 4900, train loss = 0.145542, train accuracy = 1.000000\n",
      "[2018-06-02 21:08:17.581179] Iteration 5000, train loss = 0.163809, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.922900\n",
      "[2018-06-02 21:08:47.065179] Iteration 5100, train loss = 0.149695, train accuracy = 1.000000\n",
      "[2018-06-02 21:09:11.276179] Iteration 5200, train loss = 0.170381, train accuracy = 0.992188\n",
      "[2018-06-02 21:09:35.463179] Iteration 5300, train loss = 0.141878, train accuracy = 1.000000\n",
      "[2018-06-02 21:10:00.296179] Iteration 5400, train loss = 0.146477, train accuracy = 1.000000\n",
      "[2018-06-02 21:10:24.850179] Iteration 5500, train loss = 0.158996, train accuracy = 0.992188\n",
      "[2018-06-02 21:10:49.708179] Iteration 5600, train loss = 0.171442, train accuracy = 0.984375\n",
      "[2018-06-02 21:11:14.308179] Iteration 5700, train loss = 0.149955, train accuracy = 1.000000\n",
      "[2018-06-02 21:11:38.959179] Iteration 5800, train loss = 0.161561, train accuracy = 0.992188\n",
      "[2018-06-02 21:12:03.455179] Iteration 5900, train loss = 0.152534, train accuracy = 0.992188\n",
      "[2018-06-02 21:12:27.807179] Iteration 6000, train loss = 0.155997, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923000\n",
      "[2018-06-02 21:12:57.438179] Iteration 6100, train loss = 0.155072, train accuracy = 0.992188\n",
      "[2018-06-02 21:13:21.676179] Iteration 6200, train loss = 0.167032, train accuracy = 0.992188\n",
      "[2018-06-02 21:13:45.987179] Iteration 6300, train loss = 0.182162, train accuracy = 0.984375\n",
      "[2018-06-02 21:14:10.227179] Iteration 6400, train loss = 0.184333, train accuracy = 0.992188\n",
      "[2018-06-02 21:14:34.479179] Iteration 6500, train loss = 0.171145, train accuracy = 0.992188\n",
      "[2018-06-02 21:14:58.698179] Iteration 6600, train loss = 0.162070, train accuracy = 0.984375\n",
      "[2018-06-02 21:15:23.256179] Iteration 6700, train loss = 0.152951, train accuracy = 1.000000\n",
      "[2018-06-02 21:15:48.106179] Iteration 6800, train loss = 0.151830, train accuracy = 1.000000\n",
      "[2018-06-02 21:16:12.823179] Iteration 6900, train loss = 0.177660, train accuracy = 0.992188\n",
      "[2018-06-02 21:16:37.234179] Iteration 7000, train loss = 0.146626, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "[2018-06-02 21:17:07.015179] Iteration 7100, train loss = 0.145228, train accuracy = 1.000000\n",
      "[2018-06-02 21:17:31.511179] Iteration 7200, train loss = 0.155847, train accuracy = 0.992188\n",
      "[2018-06-02 21:17:55.886179] Iteration 7300, train loss = 0.144921, train accuracy = 1.000000\n",
      "[2018-06-02 21:18:20.339179] Iteration 7400, train loss = 0.160754, train accuracy = 0.992188\n",
      "[2018-06-02 21:18:44.522179] Iteration 7500, train loss = 0.157681, train accuracy = 1.000000\n",
      "[2018-06-02 21:19:08.624179] Iteration 7600, train loss = 0.175787, train accuracy = 0.976562\n",
      "[2018-06-02 21:19:32.764179] Iteration 7700, train loss = 0.169408, train accuracy = 0.992188\n",
      "[2018-06-02 21:19:56.806179] Iteration 7800, train loss = 0.167144, train accuracy = 1.000000\n",
      "[2018-06-02 21:20:20.869179] Iteration 7900, train loss = 0.146645, train accuracy = 1.000000\n",
      "[2018-06-02 21:20:45.014179] Iteration 8000, train loss = 0.159825, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.924100\n",
      "[2018-06-02 21:21:14.286179] Iteration 8100, train loss = 0.157354, train accuracy = 1.000000\n",
      "[2018-06-02 21:21:38.408179] Iteration 8200, train loss = 0.150383, train accuracy = 1.000000\n",
      "[2018-06-02 21:22:02.510179] Iteration 8300, train loss = 0.167615, train accuracy = 0.992188\n",
      "[2018-06-02 21:22:26.605179] Iteration 8400, train loss = 0.154063, train accuracy = 0.992188\n",
      "[2018-06-02 21:22:50.638179] Iteration 8500, train loss = 0.157836, train accuracy = 1.000000\n",
      "[2018-06-02 21:23:14.790179] Iteration 8600, train loss = 0.155150, train accuracy = 1.000000\n",
      "[2018-06-02 21:23:38.884179] Iteration 8700, train loss = 0.160156, train accuracy = 0.992188\n",
      "[2018-06-02 21:24:02.952179] Iteration 8800, train loss = 0.156018, train accuracy = 0.992188\n",
      "[2018-06-02 21:24:27.034179] Iteration 8900, train loss = 0.169234, train accuracy = 0.992188\n",
      "[2018-06-02 21:24:51.076179] Iteration 9000, train loss = 0.147361, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 21:25:20.363179] Iteration 9100, train loss = 0.177063, train accuracy = 0.992188\n",
      "[2018-06-02 21:25:44.428179] Iteration 9200, train loss = 0.144016, train accuracy = 1.000000\n",
      "[2018-06-02 21:26:08.540179] Iteration 9300, train loss = 0.150338, train accuracy = 1.000000\n",
      "[2018-06-02 21:26:32.629179] Iteration 9400, train loss = 0.158570, train accuracy = 0.992188\n",
      "[2018-06-02 21:26:56.790179] Iteration 9500, train loss = 0.164894, train accuracy = 0.984375\n",
      "[2018-06-02 21:27:20.834179] Iteration 9600, train loss = 0.145218, train accuracy = 1.000000\n",
      "[2018-06-02 21:27:44.986179] Iteration 9700, train loss = 0.151405, train accuracy = 1.000000\n",
      "[2018-06-02 21:28:09.010179] Iteration 9800, train loss = 0.182382, train accuracy = 0.984375\n",
      "[2018-06-02 21:28:33.107179] Iteration 9900, train loss = 0.158353, train accuracy = 1.000000\n",
      "[2018-06-02 21:28:57.238179] Iteration 10000, train loss = 0.184420, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.924600\n",
      "[2018-06-02 21:29:26.561179] Iteration 10100, train loss = 0.155467, train accuracy = 0.992188\n",
      "[2018-06-02 21:29:50.631179] Iteration 10200, train loss = 0.165880, train accuracy = 0.992188\n",
      "[2018-06-02 21:30:14.791179] Iteration 10300, train loss = 0.160927, train accuracy = 0.992188\n",
      "[2018-06-02 21:30:38.979179] Iteration 10400, train loss = 0.144648, train accuracy = 1.000000\n",
      "[2018-06-02 21:31:03.096179] Iteration 10500, train loss = 0.144694, train accuracy = 1.000000\n",
      "[2018-06-02 21:31:27.185179] Iteration 10600, train loss = 0.170050, train accuracy = 1.000000\n",
      "[2018-06-02 21:31:51.362179] Iteration 10700, train loss = 0.163827, train accuracy = 0.992188\n",
      "[2018-06-02 21:32:15.428179] Iteration 10800, train loss = 0.193979, train accuracy = 0.976562\n",
      "[2018-06-02 21:32:39.603179] Iteration 10900, train loss = 0.146353, train accuracy = 1.000000\n",
      "[2018-06-02 21:33:03.730179] Iteration 11000, train loss = 0.150283, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924600\n",
      "[2018-06-02 21:33:33.014179] Iteration 11100, train loss = 0.160880, train accuracy = 1.000000\n",
      "[2018-06-02 21:33:57.056179] Iteration 11200, train loss = 0.160029, train accuracy = 0.992188\n",
      "[2018-06-02 21:34:21.190179] Iteration 11300, train loss = 0.142418, train accuracy = 1.000000\n",
      "[2018-06-02 21:34:45.334179] Iteration 11400, train loss = 0.157802, train accuracy = 1.000000\n",
      "[2018-06-02 21:35:09.417179] Iteration 11500, train loss = 0.150954, train accuracy = 0.992188\n",
      "[2018-06-02 21:35:33.600179] Iteration 11600, train loss = 0.143656, train accuracy = 1.000000\n",
      "[2018-06-02 21:35:57.652179] Iteration 11700, train loss = 0.143667, train accuracy = 1.000000\n",
      "[2018-06-02 21:36:21.703179] Iteration 11800, train loss = 0.196023, train accuracy = 0.984375\n",
      "[2018-06-02 21:36:45.739179] Iteration 11900, train loss = 0.153949, train accuracy = 1.000000\n",
      "[2018-06-02 21:37:09.808179] Iteration 12000, train loss = 0.143069, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924900\n",
      "[2018-06-02 21:37:39.025179] Iteration 12100, train loss = 0.154630, train accuracy = 1.000000\n",
      "[2018-06-02 21:38:03.087179] Iteration 12200, train loss = 0.181851, train accuracy = 0.984375\n",
      "[2018-06-02 21:38:27.273179] Iteration 12300, train loss = 0.143899, train accuracy = 1.000000\n",
      "[2018-06-02 21:38:51.477179] Iteration 12400, train loss = 0.166484, train accuracy = 0.984375\n",
      "[2018-06-02 21:39:15.498179] Iteration 12500, train loss = 0.148636, train accuracy = 0.992188\n",
      "[2018-06-02 21:39:39.548179] Iteration 12600, train loss = 0.163099, train accuracy = 0.984375\n",
      "[2018-06-02 21:40:03.620179] Iteration 12700, train loss = 0.149214, train accuracy = 1.000000\n",
      "[2018-06-02 21:40:27.660179] Iteration 12800, train loss = 0.142640, train accuracy = 1.000000\n",
      "[2018-06-02 21:40:51.752179] Iteration 12900, train loss = 0.147050, train accuracy = 1.000000\n",
      "[2018-06-02 21:41:15.896179] Iteration 13000, train loss = 0.164070, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924700\n",
      "[2018-06-02 21:41:45.119179] Iteration 13100, train loss = 0.165351, train accuracy = 1.000000\n",
      "[2018-06-02 21:42:09.383179] Iteration 13200, train loss = 0.155444, train accuracy = 0.992188\n",
      "[2018-06-02 21:42:33.494179] Iteration 13300, train loss = 0.176704, train accuracy = 0.984375\n",
      "[2018-06-02 21:42:57.600179] Iteration 13400, train loss = 0.147439, train accuracy = 1.000000\n",
      "[2018-06-02 21:43:21.674179] Iteration 13500, train loss = 0.187232, train accuracy = 0.992188\n",
      "[2018-06-02 21:43:45.875179] Iteration 13600, train loss = 0.154114, train accuracy = 0.992188\n",
      "[2018-06-02 21:44:09.853179] Iteration 13700, train loss = 0.157060, train accuracy = 1.000000\n",
      "[2018-06-02 21:44:34.036179] Iteration 13800, train loss = 0.205787, train accuracy = 0.968750\n",
      "[2018-06-02 21:44:58.150179] Iteration 13900, train loss = 0.163353, train accuracy = 0.984375\n",
      "[2018-06-02 21:45:22.264179] Iteration 14000, train loss = 0.156026, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925300\n",
      "[2018-06-02 21:45:51.541179] Iteration 14100, train loss = 0.148235, train accuracy = 1.000000\n",
      "[2018-06-02 21:46:15.616179] Iteration 14200, train loss = 0.178643, train accuracy = 0.992188\n",
      "[2018-06-02 21:46:39.755179] Iteration 14300, train loss = 0.171860, train accuracy = 0.976562\n",
      "[2018-06-02 21:47:04.030179] Iteration 14400, train loss = 0.178158, train accuracy = 0.992188\n",
      "[2018-06-02 21:47:28.126179] Iteration 14500, train loss = 0.161436, train accuracy = 0.992188\n",
      "[2018-06-02 21:47:52.217179] Iteration 14600, train loss = 0.143770, train accuracy = 1.000000\n",
      "[2018-06-02 21:48:16.330179] Iteration 14700, train loss = 0.152386, train accuracy = 0.992188\n",
      "[2018-06-02 21:48:40.416179] Iteration 14800, train loss = 0.172789, train accuracy = 0.984375\n",
      "[2018-06-02 21:49:04.604179] Iteration 14900, train loss = 0.150288, train accuracy = 1.000000\n",
      "[2018-06-02 21:49:28.674179] Iteration 15000, train loss = 0.168192, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 21:49:58.017179] Iteration 15100, train loss = 0.142877, train accuracy = 1.000000\n",
      "[2018-06-02 21:50:22.062179] Iteration 15200, train loss = 0.150751, train accuracy = 0.992188\n",
      "[2018-06-02 21:50:46.154179] Iteration 15300, train loss = 0.179717, train accuracy = 0.984375\n",
      "[2018-06-02 21:51:10.313179] Iteration 15400, train loss = 0.177120, train accuracy = 0.984375\n",
      "[2018-06-02 21:51:34.455179] Iteration 15500, train loss = 0.161543, train accuracy = 0.992188\n",
      "[2018-06-02 21:51:58.518179] Iteration 15600, train loss = 0.144644, train accuracy = 1.000000\n",
      "[2018-06-02 21:52:22.688179] Iteration 15700, train loss = 0.161455, train accuracy = 0.992188\n",
      "[2018-06-02 21:52:46.839179] Iteration 15800, train loss = 0.157893, train accuracy = 1.000000\n",
      "[2018-06-02 21:53:10.939179] Iteration 15900, train loss = 0.164838, train accuracy = 0.984375\n",
      "[2018-06-02 21:53:35.024179] Iteration 16000, train loss = 0.165691, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924900\n",
      "[2018-06-02 21:54:04.253179] Iteration 16100, train loss = 0.155973, train accuracy = 0.992188\n",
      "[2018-06-02 21:54:28.480179] Iteration 16200, train loss = 0.158539, train accuracy = 0.992188\n",
      "[2018-06-02 21:54:52.642179] Iteration 16300, train loss = 0.147141, train accuracy = 1.000000\n",
      "[2018-06-02 21:55:16.772179] Iteration 16400, train loss = 0.148395, train accuracy = 1.000000\n",
      "[2018-06-02 21:55:40.800179] Iteration 16500, train loss = 0.177355, train accuracy = 0.984375\n",
      "[2018-06-02 21:56:04.970179] Iteration 16600, train loss = 0.146292, train accuracy = 1.000000\n",
      "[2018-06-02 21:56:29.060179] Iteration 16700, train loss = 0.153986, train accuracy = 1.000000\n",
      "[2018-06-02 21:56:53.115179] Iteration 16800, train loss = 0.155195, train accuracy = 1.000000\n",
      "[2018-06-02 21:57:17.245179] Iteration 16900, train loss = 0.170675, train accuracy = 0.992188\n",
      "[2018-06-02 21:57:41.433179] Iteration 17000, train loss = 0.146184, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-02 21:58:10.804179] Iteration 17100, train loss = 0.155475, train accuracy = 0.992188\n",
      "[2018-06-02 21:58:35.006179] Iteration 17200, train loss = 0.161970, train accuracy = 0.992188\n",
      "[2018-06-02 21:58:59.118179] Iteration 17300, train loss = 0.166238, train accuracy = 0.984375\n",
      "[2018-06-02 21:59:23.219179] Iteration 17400, train loss = 0.148624, train accuracy = 1.000000\n",
      "[2018-06-02 21:59:47.321179] Iteration 17500, train loss = 0.143868, train accuracy = 1.000000\n",
      "[2018-06-02 22:00:11.474179] Iteration 17600, train loss = 0.147270, train accuracy = 1.000000\n",
      "[2018-06-02 22:00:35.508179] Iteration 17700, train loss = 0.151411, train accuracy = 1.000000\n",
      "[2018-06-02 22:00:59.552179] Iteration 17800, train loss = 0.146918, train accuracy = 1.000000\n",
      "[2018-06-02 22:01:23.691179] Iteration 17900, train loss = 0.145578, train accuracy = 1.000000\n",
      "[2018-06-02 22:01:47.889179] Iteration 18000, train loss = 0.155131, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 22:02:17.153179] Iteration 18100, train loss = 0.178058, train accuracy = 0.976562\n",
      "[2018-06-02 22:02:41.212179] Iteration 18200, train loss = 0.166802, train accuracy = 0.984375\n",
      "[2018-06-02 22:03:05.297179] Iteration 18300, train loss = 0.152926, train accuracy = 1.000000\n",
      "[2018-06-02 22:03:29.424179] Iteration 18400, train loss = 0.149304, train accuracy = 1.000000\n",
      "[2018-06-02 22:03:53.534179] Iteration 18500, train loss = 0.149673, train accuracy = 1.000000\n",
      "[2018-06-02 22:04:17.648179] Iteration 18600, train loss = 0.145981, train accuracy = 1.000000\n",
      "[2018-06-02 22:04:41.658179] Iteration 18700, train loss = 0.144539, train accuracy = 1.000000\n",
      "[2018-06-02 22:05:05.811179] Iteration 18800, train loss = 0.141507, train accuracy = 1.000000\n",
      "[2018-06-02 22:05:29.915179] Iteration 18900, train loss = 0.151662, train accuracy = 1.000000\n",
      "[2018-06-02 22:05:53.973179] Iteration 19000, train loss = 0.210975, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "[2018-06-02 22:06:23.250179] Iteration 19100, train loss = 0.166787, train accuracy = 0.992188\n",
      "[2018-06-02 22:06:47.350179] Iteration 19200, train loss = 0.148361, train accuracy = 1.000000\n",
      "[2018-06-02 22:07:11.425179] Iteration 19300, train loss = 0.156569, train accuracy = 0.992188\n",
      "[2018-06-02 22:07:35.501179] Iteration 19400, train loss = 0.140222, train accuracy = 1.000000\n",
      "[2018-06-02 22:07:59.703179] Iteration 19500, train loss = 0.153195, train accuracy = 0.992188\n",
      "[2018-06-02 22:08:23.785179] Iteration 19600, train loss = 0.150426, train accuracy = 1.000000\n",
      "[2018-06-02 22:08:47.817179] Iteration 19700, train loss = 0.150885, train accuracy = 0.992188\n",
      "[2018-06-02 22:09:11.913179] Iteration 19800, train loss = 0.144879, train accuracy = 1.000000\n",
      "[2018-06-02 22:09:36.032179] Iteration 19900, train loss = 0.152228, train accuracy = 0.992188\n",
      "[2018-06-02 22:10:00.201179] Iteration 20000, train loss = 0.174963, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.925200\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-02 22:10:29.473179] Iteration 20100, train loss = 0.166461, train accuracy = 0.992188\n",
      "[2018-06-02 22:10:53.600179] Iteration 20200, train loss = 0.180569, train accuracy = 0.992188\n",
      "[2018-06-02 22:11:17.737179] Iteration 20300, train loss = 0.154082, train accuracy = 0.992188\n",
      "[2018-06-02 22:11:41.856179] Iteration 20400, train loss = 0.146014, train accuracy = 1.000000\n",
      "[2018-06-02 22:12:05.901179] Iteration 20500, train loss = 0.162994, train accuracy = 0.992188\n",
      "[2018-06-02 22:12:29.994179] Iteration 20600, train loss = 0.152598, train accuracy = 0.992188\n",
      "[2018-06-02 22:12:54.031179] Iteration 20700, train loss = 0.173811, train accuracy = 0.992188\n",
      "[2018-06-02 22:13:18.191179] Iteration 20800, train loss = 0.149597, train accuracy = 1.000000\n",
      "[2018-06-02 22:13:42.343179] Iteration 20900, train loss = 0.149946, train accuracy = 1.000000\n",
      "[2018-06-02 22:14:06.506179] Iteration 21000, train loss = 0.141834, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 22:14:35.760179] Iteration 21100, train loss = 0.156418, train accuracy = 0.992188\n",
      "[2018-06-02 22:14:59.829179] Iteration 21200, train loss = 0.158734, train accuracy = 1.000000\n",
      "[2018-06-02 22:15:23.906179] Iteration 21300, train loss = 0.151273, train accuracy = 1.000000\n",
      "[2018-06-02 22:15:47.922179] Iteration 21400, train loss = 0.147675, train accuracy = 1.000000\n",
      "[2018-06-02 22:16:11.934179] Iteration 21500, train loss = 0.175133, train accuracy = 0.984375\n",
      "[2018-06-02 22:16:36.063179] Iteration 21600, train loss = 0.168774, train accuracy = 0.984375\n",
      "[2018-06-02 22:17:00.142179] Iteration 21700, train loss = 0.149870, train accuracy = 1.000000\n",
      "[2018-06-02 22:17:24.282179] Iteration 21800, train loss = 0.165673, train accuracy = 0.992188\n",
      "[2018-06-02 22:17:48.375179] Iteration 21900, train loss = 0.159524, train accuracy = 0.984375\n",
      "[2018-06-02 22:18:12.443179] Iteration 22000, train loss = 0.152950, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 22:18:41.700179] Iteration 22100, train loss = 0.150721, train accuracy = 1.000000\n",
      "[2018-06-02 22:19:05.811179] Iteration 22200, train loss = 0.148611, train accuracy = 1.000000\n",
      "[2018-06-02 22:19:29.995179] Iteration 22300, train loss = 0.167617, train accuracy = 0.976562\n",
      "[2018-06-02 22:19:54.150179] Iteration 22400, train loss = 0.166353, train accuracy = 0.992188\n",
      "[2018-06-02 22:20:18.229179] Iteration 22500, train loss = 0.150585, train accuracy = 1.000000\n",
      "[2018-06-02 22:20:42.344179] Iteration 22600, train loss = 0.170001, train accuracy = 0.984375\n",
      "[2018-06-02 22:21:06.426179] Iteration 22700, train loss = 0.174409, train accuracy = 0.984375\n",
      "[2018-06-02 22:21:30.566179] Iteration 22800, train loss = 0.145831, train accuracy = 1.000000\n",
      "[2018-06-02 22:21:54.721179] Iteration 22900, train loss = 0.156279, train accuracy = 0.992188\n",
      "[2018-06-02 22:22:18.901179] Iteration 23000, train loss = 0.140826, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924800\n",
      "[2018-06-02 22:22:48.202179] Iteration 23100, train loss = 0.146234, train accuracy = 1.000000\n",
      "[2018-06-02 22:23:12.267179] Iteration 23200, train loss = 0.143497, train accuracy = 1.000000\n",
      "[2018-06-02 22:23:36.428179] Iteration 23300, train loss = 0.154812, train accuracy = 0.992188\n",
      "[2018-06-02 22:24:00.621179] Iteration 23400, train loss = 0.149388, train accuracy = 1.000000\n",
      "[2018-06-02 22:24:24.709179] Iteration 23500, train loss = 0.178090, train accuracy = 0.984375\n",
      "[2018-06-02 22:24:48.855179] Iteration 23600, train loss = 0.142896, train accuracy = 1.000000\n",
      "[2018-06-02 22:25:12.950179] Iteration 23700, train loss = 0.144509, train accuracy = 1.000000\n",
      "[2018-06-02 22:25:37.052179] Iteration 23800, train loss = 0.151492, train accuracy = 1.000000\n",
      "[2018-06-02 22:26:01.240179] Iteration 23900, train loss = 0.162524, train accuracy = 0.984375\n",
      "[2018-06-02 22:26:25.358179] Iteration 24000, train loss = 0.162112, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 22:26:54.675179] Iteration 24100, train loss = 0.154515, train accuracy = 1.000000\n",
      "[2018-06-02 22:27:18.733179] Iteration 24200, train loss = 0.151393, train accuracy = 0.992188\n",
      "[2018-06-02 22:27:42.856179] Iteration 24300, train loss = 0.148015, train accuracy = 0.992188\n",
      "[2018-06-02 22:28:06.985179] Iteration 24400, train loss = 0.146136, train accuracy = 1.000000\n",
      "[2018-06-02 22:28:31.135179] Iteration 24500, train loss = 0.147126, train accuracy = 1.000000\n",
      "[2018-06-02 22:28:55.269179] Iteration 24600, train loss = 0.140356, train accuracy = 1.000000\n",
      "[2018-06-02 22:29:19.412179] Iteration 24700, train loss = 0.153249, train accuracy = 1.000000\n",
      "[2018-06-02 22:29:43.513179] Iteration 24800, train loss = 0.167241, train accuracy = 0.992188\n",
      "[2018-06-02 22:30:07.623179] Iteration 24900, train loss = 0.149888, train accuracy = 1.000000\n",
      "[2018-06-02 22:30:31.818179] Iteration 25000, train loss = 0.161564, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 22:31:01.150179] Iteration 25100, train loss = 0.144789, train accuracy = 1.000000\n",
      "[2018-06-02 22:31:25.264179] Iteration 25200, train loss = 0.160033, train accuracy = 0.984375\n",
      "[2018-06-02 22:31:49.339179] Iteration 25300, train loss = 0.158404, train accuracy = 0.992188\n",
      "[2018-06-02 22:32:13.507179] Iteration 25400, train loss = 0.147031, train accuracy = 1.000000\n",
      "[2018-06-02 22:32:37.702179] Iteration 25500, train loss = 0.152851, train accuracy = 0.992188\n",
      "[2018-06-02 22:33:01.845179] Iteration 25600, train loss = 0.191019, train accuracy = 0.976562\n",
      "[2018-06-02 22:33:26.007179] Iteration 25700, train loss = 0.151670, train accuracy = 1.000000\n",
      "[2018-06-02 22:33:50.147179] Iteration 25800, train loss = 0.145416, train accuracy = 1.000000\n",
      "[2018-06-02 22:34:14.316179] Iteration 25900, train loss = 0.147903, train accuracy = 1.000000\n",
      "[2018-06-02 22:34:38.489179] Iteration 26000, train loss = 0.147508, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 22:35:07.891179] Iteration 26100, train loss = 0.160295, train accuracy = 0.992188\n",
      "[2018-06-02 22:35:32.007179] Iteration 26200, train loss = 0.145015, train accuracy = 1.000000\n",
      "[2018-06-02 22:35:56.134179] Iteration 26300, train loss = 0.176174, train accuracy = 0.992188\n",
      "[2018-06-02 22:36:20.322179] Iteration 26400, train loss = 0.143244, train accuracy = 1.000000\n",
      "[2018-06-02 22:36:44.436179] Iteration 26500, train loss = 0.149598, train accuracy = 1.000000\n",
      "[2018-06-02 22:37:08.512179] Iteration 26600, train loss = 0.157812, train accuracy = 0.992188\n",
      "[2018-06-02 22:37:32.615179] Iteration 26700, train loss = 0.152193, train accuracy = 1.000000\n",
      "[2018-06-02 22:37:56.703179] Iteration 26800, train loss = 0.148863, train accuracy = 1.000000\n",
      "[2018-06-02 22:38:20.835179] Iteration 26900, train loss = 0.161361, train accuracy = 0.992188\n",
      "[2018-06-02 22:38:44.810179] Iteration 27000, train loss = 0.156220, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 22:39:14.096179] Iteration 27100, train loss = 0.155271, train accuracy = 1.000000\n",
      "[2018-06-02 22:39:38.231179] Iteration 27200, train loss = 0.158689, train accuracy = 1.000000\n",
      "[2018-06-02 22:40:02.377179] Iteration 27300, train loss = 0.143273, train accuracy = 1.000000\n",
      "[2018-06-02 22:40:26.400179] Iteration 27400, train loss = 0.144153, train accuracy = 1.000000\n",
      "[2018-06-02 22:40:50.541179] Iteration 27500, train loss = 0.151129, train accuracy = 0.992188\n",
      "[2018-06-02 22:41:14.662179] Iteration 27600, train loss = 0.161534, train accuracy = 0.992188\n",
      "[2018-06-02 22:41:38.809179] Iteration 27700, train loss = 0.155660, train accuracy = 0.992188\n",
      "[2018-06-02 22:42:02.905179] Iteration 27800, train loss = 0.165369, train accuracy = 0.992188\n",
      "[2018-06-02 22:42:27.082179] Iteration 27900, train loss = 0.183768, train accuracy = 0.984375\n",
      "[2018-06-02 22:42:51.301179] Iteration 28000, train loss = 0.141544, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925100\n",
      "[2018-06-02 22:43:20.645179] Iteration 28100, train loss = 0.150655, train accuracy = 1.000000\n",
      "[2018-06-02 22:43:44.725179] Iteration 28200, train loss = 0.168464, train accuracy = 0.992188\n",
      "[2018-06-02 22:44:08.867179] Iteration 28300, train loss = 0.152460, train accuracy = 1.000000\n",
      "[2018-06-02 22:44:32.901179] Iteration 28400, train loss = 0.151482, train accuracy = 1.000000\n",
      "[2018-06-02 22:44:57.081179] Iteration 28500, train loss = 0.144253, train accuracy = 1.000000\n",
      "[2018-06-02 22:45:21.175179] Iteration 28600, train loss = 0.140242, train accuracy = 1.000000\n",
      "[2018-06-02 22:45:45.284179] Iteration 28700, train loss = 0.153741, train accuracy = 1.000000\n",
      "[2018-06-02 22:46:09.482179] Iteration 28800, train loss = 0.156355, train accuracy = 0.984375\n",
      "[2018-06-02 22:46:33.623179] Iteration 28900, train loss = 0.149453, train accuracy = 1.000000\n",
      "[2018-06-02 22:46:57.780179] Iteration 29000, train loss = 0.152414, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924700\n",
      "[2018-06-02 22:47:27.030179] Iteration 29100, train loss = 0.144192, train accuracy = 1.000000\n",
      "[2018-06-02 22:47:51.190179] Iteration 29200, train loss = 0.154188, train accuracy = 0.992188\n",
      "[2018-06-02 22:48:15.271179] Iteration 29300, train loss = 0.159030, train accuracy = 1.000000\n",
      "[2018-06-02 22:48:39.338179] Iteration 29400, train loss = 0.140840, train accuracy = 1.000000\n",
      "[2018-06-02 22:49:03.390179] Iteration 29500, train loss = 0.154039, train accuracy = 1.000000\n",
      "[2018-06-02 22:49:27.600179] Iteration 29600, train loss = 0.157715, train accuracy = 0.992188\n",
      "[2018-06-02 22:49:51.723179] Iteration 29700, train loss = 0.141355, train accuracy = 1.000000\n",
      "[2018-06-02 22:50:15.818179] Iteration 29800, train loss = 0.143361, train accuracy = 1.000000\n",
      "[2018-06-02 22:50:39.988179] Iteration 29900, train loss = 0.148152, train accuracy = 1.000000\n",
      "[2018-06-02 22:51:04.182179] Iteration 30000, train loss = 0.168299, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925200\n",
      "[2018-06-02 22:51:33.469179] Iteration 30100, train loss = 0.149237, train accuracy = 1.000000\n",
      "[2018-06-02 22:51:57.569179] Iteration 30200, train loss = 0.153216, train accuracy = 1.000000\n",
      "[2018-06-02 22:52:21.703179] Iteration 30300, train loss = 0.140345, train accuracy = 1.000000\n",
      "[2018-06-02 22:52:45.763179] Iteration 30400, train loss = 0.166980, train accuracy = 0.992188\n",
      "[2018-06-02 22:53:09.873179] Iteration 30500, train loss = 0.161791, train accuracy = 0.984375\n",
      "[2018-06-02 22:53:33.980179] Iteration 30600, train loss = 0.141338, train accuracy = 1.000000\n",
      "[2018-06-02 22:53:58.045179] Iteration 30700, train loss = 0.166885, train accuracy = 0.992188\n",
      "[2018-06-02 22:54:22.037179] Iteration 30800, train loss = 0.145292, train accuracy = 1.000000\n",
      "[2018-06-02 22:54:46.190179] Iteration 30900, train loss = 0.139023, train accuracy = 1.000000\n",
      "[2018-06-02 22:55:10.232179] Iteration 31000, train loss = 0.147087, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925300\n",
      "[2018-06-02 22:55:39.541179] Iteration 31100, train loss = 0.141102, train accuracy = 1.000000\n",
      "[2018-06-02 22:56:03.666179] Iteration 31200, train loss = 0.149087, train accuracy = 0.992188\n",
      "[2018-06-02 22:56:27.687179] Iteration 31300, train loss = 0.149182, train accuracy = 1.000000\n",
      "[2018-06-02 22:56:51.804179] Iteration 31400, train loss = 0.169704, train accuracy = 0.992188\n",
      "[2018-06-02 22:57:15.938179] Iteration 31500, train loss = 0.176349, train accuracy = 0.976562\n",
      "[2018-06-02 22:57:40.041179] Iteration 31600, train loss = 0.149526, train accuracy = 1.000000\n",
      "[2018-06-02 22:58:04.170179] Iteration 31700, train loss = 0.166526, train accuracy = 0.992188\n",
      "[2018-06-02 22:58:28.348179] Iteration 31800, train loss = 0.145182, train accuracy = 1.000000\n",
      "[2018-06-02 22:58:52.379179] Iteration 31900, train loss = 0.156884, train accuracy = 0.992188\n",
      "[2018-06-02 22:59:16.489179] Iteration 32000, train loss = 0.159237, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925600\n",
      "[2018-06-02 22:59:45.795179] Iteration 32100, train loss = 0.145360, train accuracy = 1.000000\n",
      "[2018-06-02 23:00:09.863179] Iteration 32200, train loss = 0.152751, train accuracy = 0.992188\n",
      "[2018-06-02 23:00:33.956179] Iteration 32300, train loss = 0.147370, train accuracy = 1.000000\n",
      "[2018-06-02 23:00:58.019179] Iteration 32400, train loss = 0.139835, train accuracy = 1.000000\n",
      "[2018-06-02 23:01:22.202179] Iteration 32500, train loss = 0.146942, train accuracy = 1.000000\n",
      "[2018-06-02 23:01:46.263179] Iteration 32600, train loss = 0.148287, train accuracy = 1.000000\n",
      "[2018-06-02 23:02:10.473179] Iteration 32700, train loss = 0.149690, train accuracy = 1.000000\n",
      "[2018-06-02 23:02:34.690179] Iteration 32800, train loss = 0.163913, train accuracy = 0.992188\n",
      "[2018-06-02 23:02:58.737179] Iteration 32900, train loss = 0.146387, train accuracy = 0.992188\n",
      "[2018-06-02 23:03:22.731179] Iteration 33000, train loss = 0.192414, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.924500\n",
      "[2018-06-02 23:03:52.033179] Iteration 33100, train loss = 0.166865, train accuracy = 1.000000\n",
      "[2018-06-02 23:04:16.107179] Iteration 33200, train loss = 0.148207, train accuracy = 1.000000\n",
      "[2018-06-02 23:04:40.200179] Iteration 33300, train loss = 0.148776, train accuracy = 1.000000\n",
      "[2018-06-02 23:05:04.252179] Iteration 33400, train loss = 0.146327, train accuracy = 1.000000\n",
      "[2018-06-02 23:05:28.440179] Iteration 33500, train loss = 0.167229, train accuracy = 0.992188\n",
      "[2018-06-02 23:05:52.456179] Iteration 33600, train loss = 0.145641, train accuracy = 1.000000\n",
      "[2018-06-02 23:06:16.573179] Iteration 33700, train loss = 0.160471, train accuracy = 0.992188\n",
      "[2018-06-02 23:06:40.642179] Iteration 33800, train loss = 0.166363, train accuracy = 0.992188\n",
      "[2018-06-02 23:07:04.702179] Iteration 33900, train loss = 0.151236, train accuracy = 0.992188\n",
      "[2018-06-02 23:07:28.813179] Iteration 34000, train loss = 0.184860, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.924700\n",
      "[2018-06-02 23:07:58.174179] Iteration 34100, train loss = 0.164173, train accuracy = 0.992188\n",
      "[2018-06-02 23:08:22.233179] Iteration 34200, train loss = 0.145318, train accuracy = 1.000000\n",
      "[2018-06-02 23:08:46.289179] Iteration 34300, train loss = 0.147388, train accuracy = 1.000000\n",
      "[2018-06-02 23:09:10.322179] Iteration 34400, train loss = 0.148916, train accuracy = 0.992188\n",
      "[2018-06-02 23:09:34.394179] Iteration 34500, train loss = 0.144935, train accuracy = 1.000000\n",
      "[2018-06-02 23:09:58.459179] Iteration 34600, train loss = 0.148268, train accuracy = 1.000000\n",
      "[2018-06-02 23:10:22.538179] Iteration 34700, train loss = 0.154744, train accuracy = 0.992188\n",
      "[2018-06-02 23:10:46.622179] Iteration 34800, train loss = 0.169679, train accuracy = 0.992188\n",
      "[2018-06-02 23:11:10.774179] Iteration 34900, train loss = 0.145566, train accuracy = 1.000000\n",
      "[2018-06-02 23:11:34.867179] Iteration 35000, train loss = 0.162800, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925000\n",
      "[2018-06-02 23:12:04.089179] Iteration 35100, train loss = 0.163468, train accuracy = 0.984375\n",
      "[2018-06-02 23:12:28.278179] Iteration 35200, train loss = 0.163465, train accuracy = 1.000000\n",
      "[2018-06-02 23:12:52.306179] Iteration 35300, train loss = 0.160097, train accuracy = 0.992188\n",
      "[2018-06-02 23:13:16.427179] Iteration 35400, train loss = 0.142302, train accuracy = 1.000000\n",
      "[2018-06-02 23:13:40.505179] Iteration 35500, train loss = 0.143739, train accuracy = 1.000000\n",
      "[2018-06-02 23:14:04.666179] Iteration 35600, train loss = 0.148458, train accuracy = 0.992188\n",
      "[2018-06-02 23:14:28.788179] Iteration 35700, train loss = 0.167574, train accuracy = 0.984375\n",
      "[2018-06-02 23:14:52.967179] Iteration 35800, train loss = 0.170960, train accuracy = 0.984375\n",
      "[2018-06-02 23:15:17.028179] Iteration 35900, train loss = 0.163261, train accuracy = 0.984375\n",
      "[2018-06-02 23:15:41.142179] Iteration 36000, train loss = 0.150266, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924900\n",
      "[2018-06-02 23:16:10.391179] Iteration 36100, train loss = 0.157689, train accuracy = 0.992188\n",
      "[2018-06-02 23:16:34.560179] Iteration 36200, train loss = 0.143779, train accuracy = 1.000000\n",
      "[2018-06-02 23:16:58.590179] Iteration 36300, train loss = 0.139155, train accuracy = 1.000000\n",
      "[2018-06-02 23:17:22.647179] Iteration 36400, train loss = 0.152616, train accuracy = 0.992188\n",
      "[2018-06-02 23:17:46.743179] Iteration 36500, train loss = 0.140036, train accuracy = 1.000000\n",
      "[2018-06-02 23:18:10.826179] Iteration 36600, train loss = 0.173065, train accuracy = 0.992188\n",
      "[2018-06-02 23:18:34.914179] Iteration 36700, train loss = 0.150615, train accuracy = 0.992188\n",
      "[2018-06-02 23:18:58.970179] Iteration 36800, train loss = 0.166666, train accuracy = 0.984375\n",
      "[2018-06-02 23:19:23.137179] Iteration 36900, train loss = 0.152304, train accuracy = 1.000000\n",
      "[2018-06-02 23:19:47.276179] Iteration 37000, train loss = 0.171536, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.925400\n",
      "[2018-06-02 23:20:16.555179] Iteration 37100, train loss = 0.148337, train accuracy = 1.000000\n",
      "[2018-06-02 23:20:40.633179] Iteration 37200, train loss = 0.140749, train accuracy = 1.000000\n",
      "[2018-06-02 23:21:04.793179] Iteration 37300, train loss = 0.156564, train accuracy = 0.992188\n",
      "[2018-06-02 23:21:28.900179] Iteration 37400, train loss = 0.149513, train accuracy = 1.000000\n",
      "[2018-06-02 23:21:52.966179] Iteration 37500, train loss = 0.156693, train accuracy = 0.992188\n",
      "[2018-06-02 23:22:17.010179] Iteration 37600, train loss = 0.170850, train accuracy = 0.992188\n",
      "[2018-06-02 23:22:41.090179] Iteration 37700, train loss = 0.150860, train accuracy = 1.000000\n",
      "[2018-06-02 23:23:05.242179] Iteration 37800, train loss = 0.149585, train accuracy = 1.000000\n",
      "[2018-06-02 23:23:29.324179] Iteration 37900, train loss = 0.172558, train accuracy = 0.976562\n",
      "[2018-06-02 23:23:53.350179] Iteration 38000, train loss = 0.153052, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.925200\n",
      "[2018-06-02 23:24:22.681179] Iteration 38100, train loss = 0.154641, train accuracy = 0.992188\n",
      "[2018-06-02 23:24:46.833179] Iteration 38200, train loss = 0.155790, train accuracy = 0.984375\n",
      "[2018-06-02 23:25:11.002179] Iteration 38300, train loss = 0.141776, train accuracy = 1.000000\n",
      "[2018-06-02 23:25:35.052179] Iteration 38400, train loss = 0.162397, train accuracy = 0.984375\n",
      "[2018-06-02 23:25:59.112179] Iteration 38500, train loss = 0.158087, train accuracy = 0.992188\n",
      "[2018-06-02 23:26:23.277179] Iteration 38600, train loss = 0.158414, train accuracy = 1.000000\n",
      "[2018-06-02 23:26:47.281179] Iteration 38700, train loss = 0.150844, train accuracy = 1.000000\n",
      "[2018-06-02 23:27:11.316179] Iteration 38800, train loss = 0.158208, train accuracy = 0.984375\n",
      "[2018-06-02 23:27:35.492179] Iteration 38900, train loss = 0.148724, train accuracy = 1.000000\n",
      "[2018-06-02 23:27:59.491179] Iteration 39000, train loss = 0.154955, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924900\n",
      "[2018-06-02 23:28:28.730179] Iteration 39100, train loss = 0.145989, train accuracy = 1.000000\n",
      "[2018-06-02 23:28:52.775179] Iteration 39200, train loss = 0.146323, train accuracy = 1.000000\n",
      "[2018-06-02 23:29:16.889179] Iteration 39300, train loss = 0.167095, train accuracy = 0.984375\n",
      "[2018-06-02 23:29:40.955179] Iteration 39400, train loss = 0.162630, train accuracy = 0.984375\n",
      "[2018-06-02 23:30:05.079179] Iteration 39500, train loss = 0.144331, train accuracy = 1.000000\n",
      "[2018-06-02 23:30:29.270179] Iteration 39600, train loss = 0.158897, train accuracy = 0.992188\n",
      "[2018-06-02 23:30:53.381179] Iteration 39700, train loss = 0.157884, train accuracy = 1.000000\n",
      "[2018-06-02 23:31:17.546179] Iteration 39800, train loss = 0.161577, train accuracy = 0.992188\n",
      "[2018-06-02 23:31:41.681179] Iteration 39900, train loss = 0.144064, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三轮  量化\n",
    "prune_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.925700\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.125      -0.125       0.0625      0.03125     0.00574518 -0.015625\n",
      "  0.00206376  0.00423952 -0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.85)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-02 23:40:06.483179] Iteration 100, train loss = 0.156368, train accuracy = 0.992188\n",
      "[2018-06-02 23:40:30.561179] Iteration 200, train loss = 0.161250, train accuracy = 0.992188\n",
      "[2018-06-02 23:40:54.616179] Iteration 300, train loss = 0.145311, train accuracy = 1.000000\n",
      "[2018-06-02 23:41:18.758179] Iteration 400, train loss = 0.168950, train accuracy = 0.992188\n",
      "[2018-06-02 23:41:42.879179] Iteration 500, train loss = 0.170627, train accuracy = 0.992188\n",
      "[2018-06-02 23:42:07.013179] Iteration 600, train loss = 0.146975, train accuracy = 1.000000\n",
      "[2018-06-02 23:42:31.060179] Iteration 700, train loss = 0.155021, train accuracy = 0.992188\n",
      "[2018-06-02 23:42:55.156179] Iteration 800, train loss = 0.151748, train accuracy = 1.000000\n",
      "[2018-06-02 23:43:19.261179] Iteration 900, train loss = 0.160321, train accuracy = 0.984375\n",
      "[2018-06-02 23:43:43.358179] Iteration 1000, train loss = 0.143503, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922400\n",
      "[2018-06-02 23:44:12.658179] Iteration 1100, train loss = 0.145651, train accuracy = 1.000000\n",
      "[2018-06-02 23:44:36.749179] Iteration 1200, train loss = 0.152199, train accuracy = 1.000000\n",
      "[2018-06-02 23:45:01.016179] Iteration 1300, train loss = 0.154477, train accuracy = 1.000000\n",
      "[2018-06-02 23:45:25.154179] Iteration 1400, train loss = 0.170949, train accuracy = 0.984375\n",
      "[2018-06-02 23:45:49.259179] Iteration 1500, train loss = 0.150959, train accuracy = 1.000000\n",
      "[2018-06-02 23:46:13.455179] Iteration 1600, train loss = 0.157289, train accuracy = 1.000000\n",
      "[2018-06-02 23:46:37.575179] Iteration 1700, train loss = 0.156235, train accuracy = 0.992188\n",
      "[2018-06-02 23:47:01.718179] Iteration 1800, train loss = 0.143493, train accuracy = 1.000000\n",
      "[2018-06-02 23:47:25.835179] Iteration 1900, train loss = 0.166293, train accuracy = 0.992188\n",
      "[2018-06-02 23:47:49.902179] Iteration 2000, train loss = 0.148479, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.922800\n",
      "[2018-06-02 23:48:19.257179] Iteration 2100, train loss = 0.170797, train accuracy = 0.976562\n",
      "[2018-06-02 23:48:43.295179] Iteration 2200, train loss = 0.142412, train accuracy = 1.000000\n",
      "[2018-06-02 23:49:07.405179] Iteration 2300, train loss = 0.177651, train accuracy = 0.992188\n",
      "[2018-06-02 23:49:31.505179] Iteration 2400, train loss = 0.160238, train accuracy = 0.992188\n",
      "[2018-06-02 23:49:55.583179] Iteration 2500, train loss = 0.165731, train accuracy = 0.992188\n",
      "[2018-06-02 23:50:19.704179] Iteration 2600, train loss = 0.146116, train accuracy = 1.000000\n",
      "[2018-06-02 23:50:43.830179] Iteration 2700, train loss = 0.151848, train accuracy = 0.992188\n",
      "[2018-06-02 23:51:07.909179] Iteration 2800, train loss = 0.181673, train accuracy = 0.984375\n",
      "[2018-06-02 23:51:31.995179] Iteration 2900, train loss = 0.153429, train accuracy = 1.000000\n",
      "[2018-06-02 23:51:56.103179] Iteration 3000, train loss = 0.143084, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923600\n",
      "[2018-06-02 23:52:25.431179] Iteration 3100, train loss = 0.164040, train accuracy = 0.984375\n",
      "[2018-06-02 23:52:49.465179] Iteration 3200, train loss = 0.155803, train accuracy = 0.992188\n",
      "[2018-06-02 23:53:13.652179] Iteration 3300, train loss = 0.156581, train accuracy = 1.000000\n",
      "[2018-06-02 23:53:37.713179] Iteration 3400, train loss = 0.143468, train accuracy = 1.000000\n",
      "[2018-06-02 23:54:01.956179] Iteration 3500, train loss = 0.158860, train accuracy = 0.992188\n",
      "[2018-06-02 23:54:26.088179] Iteration 3600, train loss = 0.163465, train accuracy = 0.992188\n",
      "[2018-06-02 23:54:50.189179] Iteration 3700, train loss = 0.150864, train accuracy = 1.000000\n",
      "[2018-06-02 23:55:14.262179] Iteration 3800, train loss = 0.168273, train accuracy = 0.992188\n",
      "[2018-06-02 23:55:38.485179] Iteration 3900, train loss = 0.150233, train accuracy = 0.992188\n",
      "[2018-06-02 23:56:02.553179] Iteration 4000, train loss = 0.147772, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-02 23:56:31.891179] Iteration 4100, train loss = 0.159002, train accuracy = 0.992188\n",
      "[2018-06-02 23:56:56.010179] Iteration 4200, train loss = 0.169931, train accuracy = 0.984375\n",
      "[2018-06-02 23:57:20.117179] Iteration 4300, train loss = 0.140727, train accuracy = 1.000000\n",
      "[2018-06-02 23:57:44.200179] Iteration 4400, train loss = 0.156816, train accuracy = 0.992188\n",
      "[2018-06-02 23:58:08.317179] Iteration 4500, train loss = 0.172516, train accuracy = 0.976562\n",
      "[2018-06-02 23:58:32.460179] Iteration 4600, train loss = 0.160330, train accuracy = 0.992188\n",
      "[2018-06-02 23:58:56.541179] Iteration 4700, train loss = 0.167100, train accuracy = 0.992188\n",
      "[2018-06-02 23:59:20.634179] Iteration 4800, train loss = 0.171065, train accuracy = 0.984375\n",
      "[2018-06-02 23:59:44.852179] Iteration 4900, train loss = 0.163665, train accuracy = 0.992188\n",
      "[2018-06-03 00:00:08.967179] Iteration 5000, train loss = 0.151464, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923500\n",
      "[2018-06-03 00:00:38.337179] Iteration 5100, train loss = 0.145222, train accuracy = 1.000000\n",
      "[2018-06-03 00:01:02.434179] Iteration 5200, train loss = 0.145534, train accuracy = 1.000000\n",
      "[2018-06-03 00:01:26.529179] Iteration 5300, train loss = 0.159310, train accuracy = 0.992188\n",
      "[2018-06-03 00:01:50.679179] Iteration 5400, train loss = 0.151378, train accuracy = 1.000000\n",
      "[2018-06-03 00:02:14.830179] Iteration 5500, train loss = 0.144000, train accuracy = 1.000000\n",
      "[2018-06-03 00:02:38.941179] Iteration 5600, train loss = 0.163209, train accuracy = 0.992188\n",
      "[2018-06-03 00:03:03.095179] Iteration 5700, train loss = 0.147251, train accuracy = 1.000000\n",
      "[2018-06-03 00:03:27.252179] Iteration 5800, train loss = 0.145822, train accuracy = 1.000000\n",
      "[2018-06-03 00:03:51.451179] Iteration 5900, train loss = 0.160802, train accuracy = 0.992188\n",
      "[2018-06-03 00:04:15.620179] Iteration 6000, train loss = 0.187535, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.923800\n",
      "[2018-06-03 00:04:45.041179] Iteration 6100, train loss = 0.145250, train accuracy = 1.000000\n",
      "[2018-06-03 00:05:09.202179] Iteration 6200, train loss = 0.146802, train accuracy = 1.000000\n",
      "[2018-06-03 00:05:33.270179] Iteration 6300, train loss = 0.156561, train accuracy = 1.000000\n",
      "[2018-06-03 00:05:57.483179] Iteration 6400, train loss = 0.147093, train accuracy = 1.000000\n",
      "[2018-06-03 00:06:21.598179] Iteration 6500, train loss = 0.147434, train accuracy = 1.000000\n",
      "[2018-06-03 00:06:45.747179] Iteration 6600, train loss = 0.151046, train accuracy = 1.000000\n",
      "[2018-06-03 00:07:09.943179] Iteration 6700, train loss = 0.148855, train accuracy = 1.000000\n",
      "[2018-06-03 00:07:34.112179] Iteration 6800, train loss = 0.151957, train accuracy = 1.000000\n",
      "[2018-06-03 00:07:58.202179] Iteration 6900, train loss = 0.155483, train accuracy = 1.000000\n",
      "[2018-06-03 00:08:22.255179] Iteration 7000, train loss = 0.141148, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923600\n",
      "[2018-06-03 00:08:51.658179] Iteration 7100, train loss = 0.146878, train accuracy = 1.000000\n",
      "[2018-06-03 00:09:15.801179] Iteration 7200, train loss = 0.152974, train accuracy = 1.000000\n",
      "[2018-06-03 00:09:39.911179] Iteration 7300, train loss = 0.162262, train accuracy = 0.984375\n",
      "[2018-06-03 00:10:04.021179] Iteration 7400, train loss = 0.154075, train accuracy = 1.000000\n",
      "[2018-06-03 00:10:28.156179] Iteration 7500, train loss = 0.165267, train accuracy = 0.992188\n",
      "[2018-06-03 00:10:52.254179] Iteration 7600, train loss = 0.167667, train accuracy = 0.984375\n",
      "[2018-06-03 00:11:16.388179] Iteration 7700, train loss = 0.152014, train accuracy = 0.992188\n",
      "[2018-06-03 00:11:40.579179] Iteration 7800, train loss = 0.157868, train accuracy = 0.984375\n",
      "[2018-06-03 00:12:04.779179] Iteration 7900, train loss = 0.152400, train accuracy = 1.000000\n",
      "[2018-06-03 00:12:28.909179] Iteration 8000, train loss = 0.147269, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923800\n",
      "[2018-06-03 00:12:58.207179] Iteration 8100, train loss = 0.150001, train accuracy = 1.000000\n",
      "[2018-06-03 00:13:22.309179] Iteration 8200, train loss = 0.150413, train accuracy = 1.000000\n",
      "[2018-06-03 00:13:46.491179] Iteration 8300, train loss = 0.155568, train accuracy = 1.000000\n",
      "[2018-06-03 00:14:10.643179] Iteration 8400, train loss = 0.146480, train accuracy = 1.000000\n",
      "[2018-06-03 00:14:34.917179] Iteration 8500, train loss = 0.146993, train accuracy = 1.000000\n",
      "[2018-06-03 00:14:59.114179] Iteration 8600, train loss = 0.142567, train accuracy = 1.000000\n",
      "[2018-06-03 00:15:23.165179] Iteration 8700, train loss = 0.156470, train accuracy = 0.992188\n",
      "[2018-06-03 00:15:47.264179] Iteration 8800, train loss = 0.171785, train accuracy = 0.992188\n",
      "[2018-06-03 00:16:11.376179] Iteration 8900, train loss = 0.157287, train accuracy = 0.992188\n",
      "[2018-06-03 00:16:35.545179] Iteration 9000, train loss = 0.148115, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923500\n",
      "[2018-06-03 00:17:04.886179] Iteration 9100, train loss = 0.152935, train accuracy = 1.000000\n",
      "[2018-06-03 00:17:29.092179] Iteration 9200, train loss = 0.143052, train accuracy = 1.000000\n",
      "[2018-06-03 00:17:53.237179] Iteration 9300, train loss = 0.147092, train accuracy = 1.000000\n",
      "[2018-06-03 00:18:17.300179] Iteration 9400, train loss = 0.171611, train accuracy = 0.992188\n",
      "[2018-06-03 00:18:41.393179] Iteration 9500, train loss = 0.155262, train accuracy = 1.000000\n",
      "[2018-06-03 00:19:05.432179] Iteration 9600, train loss = 0.143033, train accuracy = 1.000000\n",
      "[2018-06-03 00:19:29.534179] Iteration 9700, train loss = 0.152259, train accuracy = 1.000000\n",
      "[2018-06-03 00:19:53.650179] Iteration 9800, train loss = 0.145715, train accuracy = 1.000000\n",
      "[2018-06-03 00:20:17.744179] Iteration 9900, train loss = 0.158765, train accuracy = 0.992188\n",
      "[2018-06-03 00:20:41.930179] Iteration 10000, train loss = 0.154551, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923700\n",
      "[2018-06-03 00:21:11.201179] Iteration 10100, train loss = 0.146495, train accuracy = 1.000000\n",
      "[2018-06-03 00:21:35.274179] Iteration 10200, train loss = 0.191778, train accuracy = 0.984375\n",
      "[2018-06-03 00:21:59.455179] Iteration 10300, train loss = 0.139930, train accuracy = 1.000000\n",
      "[2018-06-03 00:22:23.562179] Iteration 10400, train loss = 0.149778, train accuracy = 0.992188\n",
      "[2018-06-03 00:22:47.716179] Iteration 10500, train loss = 0.152009, train accuracy = 0.992188\n",
      "[2018-06-03 00:23:11.777179] Iteration 10600, train loss = 0.161413, train accuracy = 0.992188\n",
      "[2018-06-03 00:23:35.897179] Iteration 10700, train loss = 0.148771, train accuracy = 1.000000\n",
      "[2018-06-03 00:24:00.021179] Iteration 10800, train loss = 0.138600, train accuracy = 1.000000\n",
      "[2018-06-03 00:24:24.118179] Iteration 10900, train loss = 0.146148, train accuracy = 1.000000\n",
      "[2018-06-03 00:24:48.288179] Iteration 11000, train loss = 0.144445, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923700\n",
      "[2018-06-03 00:25:17.572179] Iteration 11100, train loss = 0.146797, train accuracy = 1.000000\n",
      "[2018-06-03 00:25:41.719179] Iteration 11200, train loss = 0.148851, train accuracy = 1.000000\n",
      "[2018-06-03 00:26:05.873179] Iteration 11300, train loss = 0.146691, train accuracy = 1.000000\n",
      "[2018-06-03 00:26:30.099179] Iteration 11400, train loss = 0.143733, train accuracy = 1.000000\n",
      "[2018-06-03 00:26:54.179179] Iteration 11500, train loss = 0.144009, train accuracy = 1.000000\n",
      "[2018-06-03 00:27:18.268179] Iteration 11600, train loss = 0.187202, train accuracy = 0.984375\n",
      "[2018-06-03 00:27:42.469179] Iteration 11700, train loss = 0.148617, train accuracy = 1.000000\n",
      "[2018-06-03 00:28:06.693179] Iteration 11800, train loss = 0.145506, train accuracy = 1.000000\n",
      "[2018-06-03 00:28:30.779179] Iteration 11900, train loss = 0.152446, train accuracy = 1.000000\n",
      "[2018-06-03 00:28:54.946179] Iteration 12000, train loss = 0.142205, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 00:29:24.258179] Iteration 12100, train loss = 0.156366, train accuracy = 1.000000\n",
      "[2018-06-03 00:29:48.334179] Iteration 12200, train loss = 0.153873, train accuracy = 1.000000\n",
      "[2018-06-03 00:30:12.395179] Iteration 12300, train loss = 0.150156, train accuracy = 0.992188\n",
      "[2018-06-03 00:30:36.581179] Iteration 12400, train loss = 0.166516, train accuracy = 0.992188\n",
      "[2018-06-03 00:31:00.682179] Iteration 12500, train loss = 0.153767, train accuracy = 1.000000\n",
      "[2018-06-03 00:31:24.803179] Iteration 12600, train loss = 0.141918, train accuracy = 1.000000\n",
      "[2018-06-03 00:31:49.007179] Iteration 12700, train loss = 0.145622, train accuracy = 1.000000\n",
      "[2018-06-03 00:32:13.166179] Iteration 12800, train loss = 0.176077, train accuracy = 0.992188\n",
      "[2018-06-03 00:32:37.403179] Iteration 12900, train loss = 0.154250, train accuracy = 1.000000\n",
      "[2018-06-03 00:33:01.501179] Iteration 13000, train loss = 0.161122, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-03 00:33:30.841179] Iteration 13100, train loss = 0.150933, train accuracy = 1.000000\n",
      "[2018-06-03 00:33:55.034179] Iteration 13200, train loss = 0.158222, train accuracy = 0.992188\n",
      "[2018-06-03 00:34:19.151179] Iteration 13300, train loss = 0.148444, train accuracy = 1.000000\n",
      "[2018-06-03 00:34:43.255179] Iteration 13400, train loss = 0.149799, train accuracy = 1.000000\n",
      "[2018-06-03 00:35:07.388179] Iteration 13500, train loss = 0.160803, train accuracy = 0.992188\n",
      "[2018-06-03 00:35:31.545179] Iteration 13600, train loss = 0.151161, train accuracy = 0.992188\n",
      "[2018-06-03 00:35:55.598179] Iteration 13700, train loss = 0.155529, train accuracy = 1.000000\n",
      "[2018-06-03 00:36:19.750179] Iteration 13800, train loss = 0.150036, train accuracy = 0.992188\n",
      "[2018-06-03 00:36:43.878179] Iteration 13900, train loss = 0.154572, train accuracy = 1.000000\n",
      "[2018-06-03 00:37:07.965179] Iteration 14000, train loss = 0.165877, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-03 00:37:37.312179] Iteration 14100, train loss = 0.150222, train accuracy = 1.000000\n",
      "[2018-06-03 00:38:01.498179] Iteration 14200, train loss = 0.146023, train accuracy = 1.000000\n",
      "[2018-06-03 00:38:25.605179] Iteration 14300, train loss = 0.149164, train accuracy = 0.992188\n",
      "[2018-06-03 00:38:49.775179] Iteration 14400, train loss = 0.148403, train accuracy = 1.000000\n",
      "[2018-06-03 00:39:13.897179] Iteration 14500, train loss = 0.156772, train accuracy = 0.992188\n",
      "[2018-06-03 00:39:38.070179] Iteration 14600, train loss = 0.152006, train accuracy = 0.992188\n",
      "[2018-06-03 00:40:02.183179] Iteration 14700, train loss = 0.160179, train accuracy = 1.000000\n",
      "[2018-06-03 00:40:26.426179] Iteration 14800, train loss = 0.151552, train accuracy = 0.992188\n",
      "[2018-06-03 00:40:50.552179] Iteration 14900, train loss = 0.141937, train accuracy = 1.000000\n",
      "[2018-06-03 00:41:14.686179] Iteration 15000, train loss = 0.175738, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923800\n",
      "[2018-06-03 00:41:44.048179] Iteration 15100, train loss = 0.151779, train accuracy = 1.000000\n",
      "[2018-06-03 00:42:08.212179] Iteration 15200, train loss = 0.143352, train accuracy = 1.000000\n",
      "[2018-06-03 00:42:32.372179] Iteration 15300, train loss = 0.157618, train accuracy = 1.000000\n",
      "[2018-06-03 00:42:56.485179] Iteration 15400, train loss = 0.146248, train accuracy = 1.000000\n",
      "[2018-06-03 00:43:20.629179] Iteration 15500, train loss = 0.154015, train accuracy = 1.000000\n",
      "[2018-06-03 00:43:44.793179] Iteration 15600, train loss = 0.147575, train accuracy = 1.000000\n",
      "[2018-06-03 00:44:08.917179] Iteration 15700, train loss = 0.154849, train accuracy = 0.992188\n",
      "[2018-06-03 00:44:33.046179] Iteration 15800, train loss = 0.168571, train accuracy = 0.992188\n",
      "[2018-06-03 00:44:57.171179] Iteration 15900, train loss = 0.145126, train accuracy = 1.000000\n",
      "[2018-06-03 00:45:21.221179] Iteration 16000, train loss = 0.149012, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924200\n",
      "[2018-06-03 00:45:50.473179] Iteration 16100, train loss = 0.148450, train accuracy = 1.000000\n",
      "[2018-06-03 00:46:14.527179] Iteration 16200, train loss = 0.143465, train accuracy = 1.000000\n",
      "[2018-06-03 00:46:38.641179] Iteration 16300, train loss = 0.145881, train accuracy = 1.000000\n",
      "[2018-06-03 00:47:02.810179] Iteration 16400, train loss = 0.161597, train accuracy = 0.992188\n",
      "[2018-06-03 00:47:26.937179] Iteration 16500, train loss = 0.145418, train accuracy = 1.000000\n",
      "[2018-06-03 00:47:51.146179] Iteration 16600, train loss = 0.151500, train accuracy = 1.000000\n",
      "[2018-06-03 00:48:15.296179] Iteration 16700, train loss = 0.179106, train accuracy = 0.984375\n",
      "[2018-06-03 00:48:39.427179] Iteration 16800, train loss = 0.164225, train accuracy = 0.992188\n",
      "[2018-06-03 00:49:03.455179] Iteration 16900, train loss = 0.165716, train accuracy = 0.992188\n",
      "[2018-06-03 00:49:27.537179] Iteration 17000, train loss = 0.145254, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 00:49:56.864179] Iteration 17100, train loss = 0.145157, train accuracy = 1.000000\n",
      "[2018-06-03 00:50:20.973179] Iteration 17200, train loss = 0.149199, train accuracy = 0.992188\n",
      "[2018-06-03 00:50:45.114179] Iteration 17300, train loss = 0.166739, train accuracy = 0.992188\n",
      "[2018-06-03 00:51:09.297179] Iteration 17400, train loss = 0.156216, train accuracy = 0.992188\n",
      "[2018-06-03 00:51:33.445179] Iteration 17500, train loss = 0.169993, train accuracy = 0.992188\n",
      "[2018-06-03 00:51:57.598179] Iteration 17600, train loss = 0.164923, train accuracy = 0.992188\n",
      "[2018-06-03 00:52:21.655179] Iteration 17700, train loss = 0.166745, train accuracy = 0.992188\n",
      "[2018-06-03 00:52:45.813179] Iteration 17800, train loss = 0.166107, train accuracy = 0.992188\n",
      "[2018-06-03 00:53:09.959179] Iteration 17900, train loss = 0.179055, train accuracy = 0.992188\n",
      "[2018-06-03 00:53:34.039179] Iteration 18000, train loss = 0.165457, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924100\n",
      "[2018-06-03 00:54:03.421179] Iteration 18100, train loss = 0.147280, train accuracy = 1.000000\n",
      "[2018-06-03 00:54:27.564179] Iteration 18200, train loss = 0.144135, train accuracy = 1.000000\n",
      "[2018-06-03 00:54:51.643179] Iteration 18300, train loss = 0.145722, train accuracy = 1.000000\n",
      "[2018-06-03 00:55:15.764179] Iteration 18400, train loss = 0.157235, train accuracy = 0.992188\n",
      "[2018-06-03 00:55:39.951179] Iteration 18500, train loss = 0.147557, train accuracy = 0.992188\n",
      "[2018-06-03 00:56:04.123179] Iteration 18600, train loss = 0.166864, train accuracy = 0.984375\n",
      "[2018-06-03 00:56:28.269179] Iteration 18700, train loss = 0.152793, train accuracy = 1.000000\n",
      "[2018-06-03 00:56:52.323179] Iteration 18800, train loss = 0.162828, train accuracy = 0.992188\n",
      "[2018-06-03 00:57:16.528179] Iteration 18900, train loss = 0.152635, train accuracy = 0.992188\n",
      "[2018-06-03 00:57:40.696179] Iteration 19000, train loss = 0.144628, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 00:58:10.052179] Iteration 19100, train loss = 0.145091, train accuracy = 1.000000\n",
      "[2018-06-03 00:58:34.162179] Iteration 19200, train loss = 0.167388, train accuracy = 0.992188\n",
      "[2018-06-03 00:58:58.272179] Iteration 19300, train loss = 0.151352, train accuracy = 1.000000\n",
      "[2018-06-03 00:59:22.419179] Iteration 19400, train loss = 0.148637, train accuracy = 1.000000\n",
      "[2018-06-03 00:59:46.504179] Iteration 19500, train loss = 0.146247, train accuracy = 1.000000\n",
      "[2018-06-03 01:00:10.694179] Iteration 19600, train loss = 0.150738, train accuracy = 1.000000\n",
      "[2018-06-03 01:00:34.754179] Iteration 19700, train loss = 0.141900, train accuracy = 1.000000\n",
      "[2018-06-03 01:00:58.797179] Iteration 19800, train loss = 0.156707, train accuracy = 1.000000\n",
      "[2018-06-03 01:01:22.951179] Iteration 19900, train loss = 0.177564, train accuracy = 0.992188\n",
      "[2018-06-03 01:01:47.174179] Iteration 20000, train loss = 0.142689, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-03 01:02:16.482179] Iteration 20100, train loss = 0.140360, train accuracy = 1.000000\n",
      "[2018-06-03 01:02:40.568179] Iteration 20200, train loss = 0.159589, train accuracy = 0.992188\n",
      "[2018-06-03 01:03:04.698179] Iteration 20300, train loss = 0.142251, train accuracy = 1.000000\n",
      "[2018-06-03 01:03:28.801179] Iteration 20400, train loss = 0.146603, train accuracy = 1.000000\n",
      "[2018-06-03 01:03:52.879179] Iteration 20500, train loss = 0.154946, train accuracy = 0.992188\n",
      "[2018-06-03 01:04:17.099179] Iteration 20600, train loss = 0.149612, train accuracy = 1.000000\n",
      "[2018-06-03 01:04:41.166179] Iteration 20700, train loss = 0.145089, train accuracy = 1.000000\n",
      "[2018-06-03 01:05:05.316179] Iteration 20800, train loss = 0.141323, train accuracy = 1.000000\n",
      "[2018-06-03 01:05:29.426179] Iteration 20900, train loss = 0.148444, train accuracy = 1.000000\n",
      "[2018-06-03 01:05:53.533179] Iteration 21000, train loss = 0.157311, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924200\n",
      "[2018-06-03 01:06:22.969179] Iteration 21100, train loss = 0.161564, train accuracy = 0.992188\n",
      "[2018-06-03 01:06:47.134179] Iteration 21200, train loss = 0.146417, train accuracy = 1.000000\n",
      "[2018-06-03 01:07:11.264179] Iteration 21300, train loss = 0.158878, train accuracy = 1.000000\n",
      "[2018-06-03 01:07:35.342179] Iteration 21400, train loss = 0.153818, train accuracy = 0.992188\n",
      "[2018-06-03 01:07:59.499179] Iteration 21500, train loss = 0.149407, train accuracy = 1.000000\n",
      "[2018-06-03 01:08:23.654179] Iteration 21600, train loss = 0.146405, train accuracy = 1.000000\n",
      "[2018-06-03 01:08:47.795179] Iteration 21700, train loss = 0.155779, train accuracy = 0.992188\n",
      "[2018-06-03 01:09:11.943179] Iteration 21800, train loss = 0.153904, train accuracy = 1.000000\n",
      "[2018-06-03 01:09:36.140179] Iteration 21900, train loss = 0.145411, train accuracy = 1.000000\n",
      "[2018-06-03 01:10:00.317179] Iteration 22000, train loss = 0.160306, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-03 01:10:29.733179] Iteration 22100, train loss = 0.142448, train accuracy = 1.000000\n",
      "[2018-06-03 01:10:53.821179] Iteration 22200, train loss = 0.153350, train accuracy = 0.992188\n",
      "[2018-06-03 01:11:18.091179] Iteration 22300, train loss = 0.146517, train accuracy = 1.000000\n",
      "[2018-06-03 01:11:42.234179] Iteration 22400, train loss = 0.159976, train accuracy = 0.992188\n",
      "[2018-06-03 01:12:06.376179] Iteration 22500, train loss = 0.149799, train accuracy = 0.992188\n",
      "[2018-06-03 01:12:30.434179] Iteration 22600, train loss = 0.153689, train accuracy = 1.000000\n",
      "[2018-06-03 01:12:54.546179] Iteration 22700, train loss = 0.168288, train accuracy = 0.984375\n",
      "[2018-06-03 01:13:18.602179] Iteration 22800, train loss = 0.153700, train accuracy = 0.992188\n",
      "[2018-06-03 01:13:42.742179] Iteration 22900, train loss = 0.147947, train accuracy = 1.000000\n",
      "[2018-06-03 01:14:06.888179] Iteration 23000, train loss = 0.149679, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923800\n",
      "[2018-06-03 01:14:36.130179] Iteration 23100, train loss = 0.145436, train accuracy = 0.992188\n",
      "[2018-06-03 01:15:00.260179] Iteration 23200, train loss = 0.156922, train accuracy = 1.000000\n",
      "[2018-06-03 01:15:24.438179] Iteration 23300, train loss = 0.159088, train accuracy = 1.000000\n",
      "[2018-06-03 01:15:48.564179] Iteration 23400, train loss = 0.145495, train accuracy = 1.000000\n",
      "[2018-06-03 01:16:12.634179] Iteration 23500, train loss = 0.154447, train accuracy = 0.992188\n",
      "[2018-06-03 01:16:36.764179] Iteration 23600, train loss = 0.160669, train accuracy = 0.992188\n",
      "[2018-06-03 01:17:00.937179] Iteration 23700, train loss = 0.160805, train accuracy = 0.992188\n",
      "[2018-06-03 01:17:25.018179] Iteration 23800, train loss = 0.143123, train accuracy = 1.000000\n",
      "[2018-06-03 01:17:49.265179] Iteration 23900, train loss = 0.150849, train accuracy = 1.000000\n",
      "[2018-06-03 01:18:13.366179] Iteration 24000, train loss = 0.142275, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-03 01:18:42.688179] Iteration 24100, train loss = 0.177216, train accuracy = 0.984375\n",
      "[2018-06-03 01:19:06.814179] Iteration 24200, train loss = 0.152981, train accuracy = 0.992188\n",
      "[2018-06-03 01:19:30.922179] Iteration 24300, train loss = 0.146292, train accuracy = 0.992188\n",
      "[2018-06-03 01:19:55.041179] Iteration 24400, train loss = 0.156181, train accuracy = 1.000000\n",
      "[2018-06-03 01:20:19.196179] Iteration 24500, train loss = 0.160899, train accuracy = 0.984375\n",
      "[2018-06-03 01:20:43.343179] Iteration 24600, train loss = 0.148876, train accuracy = 0.992188\n",
      "[2018-06-03 01:21:07.430179] Iteration 24700, train loss = 0.167078, train accuracy = 0.992188\n",
      "[2018-06-03 01:21:31.646179] Iteration 24800, train loss = 0.144912, train accuracy = 1.000000\n",
      "[2018-06-03 01:21:55.690179] Iteration 24900, train loss = 0.153312, train accuracy = 0.992188\n",
      "[2018-06-03 01:22:19.770179] Iteration 25000, train loss = 0.146161, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 01:22:49.094179] Iteration 25100, train loss = 0.152142, train accuracy = 0.992188\n",
      "[2018-06-03 01:23:13.171179] Iteration 25200, train loss = 0.185099, train accuracy = 0.976562\n",
      "[2018-06-03 01:23:37.270179] Iteration 25300, train loss = 0.148835, train accuracy = 1.000000\n",
      "[2018-06-03 01:24:01.463179] Iteration 25400, train loss = 0.166905, train accuracy = 0.992188\n",
      "[2018-06-03 01:24:25.678179] Iteration 25500, train loss = 0.156193, train accuracy = 0.992188\n",
      "[2018-06-03 01:24:49.796179] Iteration 25600, train loss = 0.141751, train accuracy = 1.000000\n",
      "[2018-06-03 01:25:13.857179] Iteration 25700, train loss = 0.173931, train accuracy = 0.984375\n",
      "[2018-06-03 01:25:38.042179] Iteration 25800, train loss = 0.162962, train accuracy = 0.992188\n",
      "[2018-06-03 01:26:02.119179] Iteration 25900, train loss = 0.176844, train accuracy = 0.992188\n",
      "[2018-06-03 01:26:26.219179] Iteration 26000, train loss = 0.147292, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924300\n",
      "[2018-06-03 01:26:55.567179] Iteration 26100, train loss = 0.147854, train accuracy = 1.000000\n",
      "[2018-06-03 01:27:19.676179] Iteration 26200, train loss = 0.167660, train accuracy = 0.984375\n",
      "[2018-06-03 01:27:43.784179] Iteration 26300, train loss = 0.175460, train accuracy = 0.992188\n",
      "[2018-06-03 01:28:07.959179] Iteration 26400, train loss = 0.144444, train accuracy = 1.000000\n",
      "[2018-06-03 01:28:32.016179] Iteration 26500, train loss = 0.144269, train accuracy = 1.000000\n",
      "[2018-06-03 01:28:56.077179] Iteration 26600, train loss = 0.145753, train accuracy = 1.000000\n",
      "[2018-06-03 01:29:20.218179] Iteration 26700, train loss = 0.158213, train accuracy = 0.992188\n",
      "[2018-06-03 01:29:44.342179] Iteration 26800, train loss = 0.144837, train accuracy = 1.000000\n",
      "[2018-06-03 01:30:08.498179] Iteration 26900, train loss = 0.169724, train accuracy = 0.992188\n",
      "[2018-06-03 01:30:32.595179] Iteration 27000, train loss = 0.156464, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924100\n",
      "[2018-06-03 01:31:01.898179] Iteration 27100, train loss = 0.152012, train accuracy = 0.992188\n",
      "[2018-06-03 01:31:25.918179] Iteration 27200, train loss = 0.169605, train accuracy = 0.984375\n",
      "[2018-06-03 01:31:50.022179] Iteration 27300, train loss = 0.152139, train accuracy = 0.992188\n",
      "[2018-06-03 01:32:14.184179] Iteration 27400, train loss = 0.153386, train accuracy = 0.992188\n",
      "[2018-06-03 01:32:38.325179] Iteration 27500, train loss = 0.147445, train accuracy = 1.000000\n",
      "[2018-06-03 01:33:02.398179] Iteration 27600, train loss = 0.148215, train accuracy = 1.000000\n",
      "[2018-06-03 01:33:26.628179] Iteration 27700, train loss = 0.159854, train accuracy = 0.992188\n",
      "[2018-06-03 01:33:50.728179] Iteration 27800, train loss = 0.147025, train accuracy = 1.000000\n",
      "[2018-06-03 01:34:14.903179] Iteration 27900, train loss = 0.164334, train accuracy = 0.984375\n",
      "[2018-06-03 01:34:38.971179] Iteration 28000, train loss = 0.148630, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-03 01:35:08.340179] Iteration 28100, train loss = 0.148331, train accuracy = 0.992188\n",
      "[2018-06-03 01:35:32.385179] Iteration 28200, train loss = 0.142743, train accuracy = 1.000000\n",
      "[2018-06-03 01:35:56.479179] Iteration 28300, train loss = 0.140253, train accuracy = 1.000000\n",
      "[2018-06-03 01:36:20.598179] Iteration 28400, train loss = 0.142538, train accuracy = 1.000000\n",
      "[2018-06-03 01:36:44.728179] Iteration 28500, train loss = 0.158450, train accuracy = 0.992188\n",
      "[2018-06-03 01:37:08.872179] Iteration 28600, train loss = 0.191061, train accuracy = 0.968750\n",
      "[2018-06-03 01:37:33.001179] Iteration 28700, train loss = 0.147879, train accuracy = 1.000000\n",
      "[2018-06-03 01:37:57.110179] Iteration 28800, train loss = 0.149999, train accuracy = 1.000000\n",
      "[2018-06-03 01:38:21.185179] Iteration 28900, train loss = 0.143146, train accuracy = 1.000000\n",
      "[2018-06-03 01:38:45.368179] Iteration 29000, train loss = 0.152827, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-03 01:39:14.690179] Iteration 29100, train loss = 0.150264, train accuracy = 0.992188\n",
      "[2018-06-03 01:39:38.843179] Iteration 29200, train loss = 0.151008, train accuracy = 1.000000\n",
      "[2018-06-03 01:40:02.947179] Iteration 29300, train loss = 0.155274, train accuracy = 1.000000\n",
      "[2018-06-03 01:40:27.117179] Iteration 29400, train loss = 0.145452, train accuracy = 1.000000\n",
      "[2018-06-03 01:40:51.237179] Iteration 29500, train loss = 0.168419, train accuracy = 0.992188\n",
      "[2018-06-03 01:41:15.468179] Iteration 29600, train loss = 0.187525, train accuracy = 0.976562\n",
      "[2018-06-03 01:41:39.626179] Iteration 29700, train loss = 0.144391, train accuracy = 1.000000\n",
      "[2018-06-03 01:42:03.663179] Iteration 29800, train loss = 0.150544, train accuracy = 1.000000\n",
      "[2018-06-03 01:42:27.802179] Iteration 29900, train loss = 0.145219, train accuracy = 1.000000\n",
      "[2018-06-03 01:42:51.973179] Iteration 30000, train loss = 0.153005, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924400\n",
      "[2018-06-03 01:43:21.285179] Iteration 30100, train loss = 0.155369, train accuracy = 0.992188\n",
      "[2018-06-03 01:43:45.466179] Iteration 30200, train loss = 0.143189, train accuracy = 1.000000\n",
      "[2018-06-03 01:44:09.566179] Iteration 30300, train loss = 0.143443, train accuracy = 1.000000\n",
      "[2018-06-03 01:44:33.720179] Iteration 30400, train loss = 0.155116, train accuracy = 0.992188\n",
      "[2018-06-03 01:44:57.824179] Iteration 30500, train loss = 0.146242, train accuracy = 1.000000\n",
      "[2018-06-03 01:45:21.964179] Iteration 30600, train loss = 0.152324, train accuracy = 1.000000\n",
      "[2018-06-03 01:45:46.137179] Iteration 30700, train loss = 0.147884, train accuracy = 1.000000\n",
      "[2018-06-03 01:46:10.248179] Iteration 30800, train loss = 0.155136, train accuracy = 0.992188\n",
      "[2018-06-03 01:46:34.324179] Iteration 30900, train loss = 0.148488, train accuracy = 1.000000\n",
      "[2018-06-03 01:46:58.468179] Iteration 31000, train loss = 0.152159, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 01:47:27.780179] Iteration 31100, train loss = 0.144306, train accuracy = 1.000000\n",
      "[2018-06-03 01:47:51.836179] Iteration 31200, train loss = 0.175667, train accuracy = 0.992188\n",
      "[2018-06-03 01:48:15.967179] Iteration 31300, train loss = 0.155095, train accuracy = 1.000000\n",
      "[2018-06-03 01:48:40.023179] Iteration 31400, train loss = 0.146950, train accuracy = 1.000000\n",
      "[2018-06-03 01:49:04.233179] Iteration 31500, train loss = 0.143154, train accuracy = 1.000000\n",
      "[2018-06-03 01:49:28.352179] Iteration 31600, train loss = 0.146681, train accuracy = 1.000000\n",
      "[2018-06-03 01:49:52.489179] Iteration 31700, train loss = 0.214450, train accuracy = 0.984375\n",
      "[2018-06-03 01:50:16.643179] Iteration 31800, train loss = 0.169947, train accuracy = 0.992188\n",
      "[2018-06-03 01:50:40.822179] Iteration 31900, train loss = 0.146066, train accuracy = 1.000000\n",
      "[2018-06-03 01:51:04.970179] Iteration 32000, train loss = 0.176389, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.923700\n",
      "[2018-06-03 01:51:34.305179] Iteration 32100, train loss = 0.153573, train accuracy = 0.992188\n",
      "[2018-06-03 01:51:58.386179] Iteration 32200, train loss = 0.181075, train accuracy = 0.984375\n",
      "[2018-06-03 01:52:22.523179] Iteration 32300, train loss = 0.158436, train accuracy = 1.000000\n",
      "[2018-06-03 01:52:46.639179] Iteration 32400, train loss = 0.156634, train accuracy = 0.992188\n",
      "[2018-06-03 01:53:10.763179] Iteration 32500, train loss = 0.164349, train accuracy = 0.984375\n",
      "[2018-06-03 01:53:34.862179] Iteration 32600, train loss = 0.148957, train accuracy = 1.000000\n",
      "[2018-06-03 01:53:59.039179] Iteration 32700, train loss = 0.148024, train accuracy = 1.000000\n",
      "[2018-06-03 01:54:23.144179] Iteration 32800, train loss = 0.143412, train accuracy = 1.000000\n",
      "[2018-06-03 01:54:47.237179] Iteration 32900, train loss = 0.152774, train accuracy = 1.000000\n",
      "[2018-06-03 01:55:11.438179] Iteration 33000, train loss = 0.145883, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924100\n",
      "[2018-06-03 01:55:40.688179] Iteration 33100, train loss = 0.140089, train accuracy = 1.000000\n",
      "[2018-06-03 01:56:04.849179] Iteration 33200, train loss = 0.148606, train accuracy = 0.992188\n",
      "[2018-06-03 01:56:28.950179] Iteration 33300, train loss = 0.165660, train accuracy = 0.992188\n",
      "[2018-06-03 01:56:53.105179] Iteration 33400, train loss = 0.145563, train accuracy = 1.000000\n",
      "[2018-06-03 01:57:17.149179] Iteration 33500, train loss = 0.151469, train accuracy = 1.000000\n",
      "[2018-06-03 01:57:41.204179] Iteration 33600, train loss = 0.150113, train accuracy = 0.992188\n",
      "[2018-06-03 01:58:05.270179] Iteration 33700, train loss = 0.155273, train accuracy = 0.992188\n",
      "[2018-06-03 01:58:29.434179] Iteration 33800, train loss = 0.140433, train accuracy = 1.000000\n",
      "[2018-06-03 01:58:53.591179] Iteration 33900, train loss = 0.156942, train accuracy = 1.000000\n",
      "[2018-06-03 01:59:17.649179] Iteration 34000, train loss = 0.145870, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924200\n",
      "[2018-06-03 01:59:46.999179] Iteration 34100, train loss = 0.146528, train accuracy = 1.000000\n",
      "[2018-06-03 02:00:11.127179] Iteration 34200, train loss = 0.153494, train accuracy = 0.992188\n",
      "[2018-06-03 02:00:35.343179] Iteration 34300, train loss = 0.141733, train accuracy = 1.000000\n",
      "[2018-06-03 02:00:59.585179] Iteration 34400, train loss = 0.144512, train accuracy = 1.000000\n",
      "[2018-06-03 02:01:23.743179] Iteration 34500, train loss = 0.148800, train accuracy = 1.000000\n",
      "[2018-06-03 02:01:47.873179] Iteration 34600, train loss = 0.150275, train accuracy = 1.000000\n",
      "[2018-06-03 02:02:12.061179] Iteration 34700, train loss = 0.148112, train accuracy = 1.000000\n",
      "[2018-06-03 02:02:36.250179] Iteration 34800, train loss = 0.160102, train accuracy = 0.992188\n",
      "[2018-06-03 02:03:00.366179] Iteration 34900, train loss = 0.158220, train accuracy = 0.992188\n",
      "[2018-06-03 02:03:24.443179] Iteration 35000, train loss = 0.148101, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923800\n",
      "[2018-06-03 02:03:53.783179] Iteration 35100, train loss = 0.146735, train accuracy = 1.000000\n",
      "[2018-06-03 02:04:17.886179] Iteration 35200, train loss = 0.151919, train accuracy = 1.000000\n",
      "[2018-06-03 02:04:42.036179] Iteration 35300, train loss = 0.156483, train accuracy = 0.992188\n",
      "[2018-06-03 02:05:06.229179] Iteration 35400, train loss = 0.165977, train accuracy = 0.984375\n",
      "[2018-06-03 02:05:30.312179] Iteration 35500, train loss = 0.148111, train accuracy = 1.000000\n",
      "[2018-06-03 02:05:54.459179] Iteration 35600, train loss = 0.154308, train accuracy = 1.000000\n",
      "[2018-06-03 02:06:18.521179] Iteration 35700, train loss = 0.151862, train accuracy = 1.000000\n",
      "[2018-06-03 02:06:42.636179] Iteration 35800, train loss = 0.172762, train accuracy = 0.992188\n",
      "[2018-06-03 02:07:06.806179] Iteration 35900, train loss = 0.149985, train accuracy = 1.000000\n",
      "[2018-06-03 02:07:31.097179] Iteration 36000, train loss = 0.160724, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.924000\n",
      "[2018-06-03 02:08:00.423179] Iteration 36100, train loss = 0.149198, train accuracy = 1.000000\n",
      "[2018-06-03 02:08:24.552179] Iteration 36200, train loss = 0.143959, train accuracy = 1.000000\n",
      "[2018-06-03 02:08:48.755179] Iteration 36300, train loss = 0.151340, train accuracy = 1.000000\n",
      "[2018-06-03 02:09:12.941179] Iteration 36400, train loss = 0.155380, train accuracy = 0.992188\n",
      "[2018-06-03 02:09:37.112179] Iteration 36500, train loss = 0.164937, train accuracy = 0.984375\n",
      "[2018-06-03 02:10:01.245179] Iteration 36600, train loss = 0.154844, train accuracy = 1.000000\n",
      "[2018-06-03 02:10:25.426179] Iteration 36700, train loss = 0.145969, train accuracy = 1.000000\n",
      "[2018-06-03 02:10:49.587179] Iteration 36800, train loss = 0.156827, train accuracy = 0.992188\n",
      "[2018-06-03 02:11:13.700179] Iteration 36900, train loss = 0.155715, train accuracy = 0.992188\n",
      "[2018-06-03 02:11:37.825179] Iteration 37000, train loss = 0.148412, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.923900\n",
      "[2018-06-03 02:12:07.133179] Iteration 37100, train loss = 0.169050, train accuracy = 0.984375\n",
      "[2018-06-03 02:12:31.299179] Iteration 37200, train loss = 0.160234, train accuracy = 0.992188\n",
      "[2018-06-03 02:12:55.490179] Iteration 37300, train loss = 0.142485, train accuracy = 1.000000\n",
      "[2018-06-03 02:13:19.604179] Iteration 37400, train loss = 0.152829, train accuracy = 0.992188\n",
      "[2018-06-03 02:13:43.951179] Iteration 37500, train loss = 0.155960, train accuracy = 0.984375\n",
      "[2018-06-03 02:14:08.010179] Iteration 37600, train loss = 0.165153, train accuracy = 0.992188\n",
      "[2018-06-03 02:14:32.142179] Iteration 37700, train loss = 0.163164, train accuracy = 0.992188\n",
      "[2018-06-03 02:14:56.303179] Iteration 37800, train loss = 0.143542, train accuracy = 1.000000\n",
      "[2018-06-03 02:15:20.470179] Iteration 37900, train loss = 0.147929, train accuracy = 1.000000\n",
      "[2018-06-03 02:15:44.605179] Iteration 38000, train loss = 0.150837, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924200\n",
      "[2018-06-03 02:16:13.947179] Iteration 38100, train loss = 0.158016, train accuracy = 0.992188\n",
      "[2018-06-03 02:16:38.002179] Iteration 38200, train loss = 0.153404, train accuracy = 1.000000\n",
      "[2018-06-03 02:17:02.259179] Iteration 38300, train loss = 0.172543, train accuracy = 0.984375\n",
      "[2018-06-03 02:17:26.317179] Iteration 38400, train loss = 0.152849, train accuracy = 1.000000\n",
      "[2018-06-03 02:17:50.481179] Iteration 38500, train loss = 0.145769, train accuracy = 0.992188\n",
      "[2018-06-03 02:18:14.654179] Iteration 38600, train loss = 0.151518, train accuracy = 1.000000\n",
      "[2018-06-03 02:18:38.787179] Iteration 38700, train loss = 0.158288, train accuracy = 0.992188\n",
      "[2018-06-03 02:19:02.951179] Iteration 38800, train loss = 0.150283, train accuracy = 1.000000\n",
      "[2018-06-03 02:19:27.111179] Iteration 38900, train loss = 0.149067, train accuracy = 1.000000\n",
      "[2018-06-03 02:19:51.229179] Iteration 39000, train loss = 0.146327, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.924100\n",
      "[2018-06-03 02:20:20.478179] Iteration 39100, train loss = 0.200052, train accuracy = 0.984375\n",
      "[2018-06-03 02:20:44.585179] Iteration 39200, train loss = 0.148394, train accuracy = 1.000000\n",
      "[2018-06-03 02:21:08.782179] Iteration 39300, train loss = 0.156013, train accuracy = 0.992188\n",
      "[2018-06-03 02:21:32.872179] Iteration 39400, train loss = 0.145612, train accuracy = 1.000000\n",
      "[2018-06-03 02:21:57.011179] Iteration 39500, train loss = 0.151568, train accuracy = 1.000000\n",
      "[2018-06-03 02:22:21.140179] Iteration 39600, train loss = 0.188952, train accuracy = 0.992188\n",
      "[2018-06-03 02:22:45.339179] Iteration 39700, train loss = 0.180084, train accuracy = 0.984375\n",
      "[2018-06-03 02:23:09.364179] Iteration 39800, train loss = 0.176124, train accuracy = 0.984375\n",
      "[2018-06-03 02:23:33.513179] Iteration 39900, train loss = 0.146326, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(40000):\n",
    "  if step <= 20000:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四轮  量化  \n",
    "prune_rate = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.924600\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.125      -0.125       0.0625      0.03125     0.00390625 -0.015625\n",
      "  0.00113938  0.00154301 -0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "prune_dict=apply_inq(para_dict,one_dict,var_name,1)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.001000\n",
      "[2018-06-03 02:34:19.715179] Iteration 100, train loss = 0.163318, train accuracy = 0.992188\n",
      "[2018-06-03 02:34:43.678179] Iteration 200, train loss = 0.142647, train accuracy = 1.000000\n",
      "[2018-06-03 02:35:07.569179] Iteration 300, train loss = 0.159763, train accuracy = 0.984375\n",
      "[2018-06-03 02:35:31.548179] Iteration 400, train loss = 0.166368, train accuracy = 0.992188\n",
      "[2018-06-03 02:35:55.416179] Iteration 500, train loss = 0.173392, train accuracy = 0.992188\n",
      "Learning rate set to 0.000100\n",
      "[2018-06-03 02:36:19.293179] Iteration 600, train loss = 0.150239, train accuracy = 1.000000\n",
      "[2018-06-03 02:36:43.288179] Iteration 700, train loss = 0.155080, train accuracy = 0.992188\n",
      "[2018-06-03 02:37:07.202179] Iteration 800, train loss = 0.185840, train accuracy = 0.984375\n",
      "[2018-06-03 02:37:31.127179] Iteration 900, train loss = 0.141033, train accuracy = 1.000000\n"
     ]
    }
   ],
   "source": [
    "for step in range(1000):\n",
    "  if step <= 500:\n",
    "    _lr = 1e-3\n",
    "  else:\n",
    "    _lr = 1e-4\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune后的准确率\n",
      "Test accuracy = 0.922900\n"
     ]
    }
   ],
   "source": [
    "print('prune后的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.125      -0.125       0.0625      0.03125     0.00390625 -0.015625\n",
      "  0.00097656  0.00195312 -0.03125    -0.0625    ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
