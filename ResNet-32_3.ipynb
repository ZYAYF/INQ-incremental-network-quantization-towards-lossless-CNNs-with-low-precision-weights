{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "# import ipdb\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import control_flow_ops\n",
    "os.environ['CUDA_VISIBLE_DEVICES']='0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "FLAGS = tf.app.flags.FLAGS\n",
    "tf.app.flags.DEFINE_integer('residual_net_n', 5, '')\n",
    "tf.app.flags.DEFINE_string('train_tf_path', './data/train.tf', '')\n",
    "tf.app.flags.DEFINE_string('val_tf_path', './data/test.tf', '')\n",
    "tf.app.flags.DEFINE_integer('train_batch_size', 128, '')\n",
    "tf.app.flags.DEFINE_integer('val_batch_size', 100, '')\n",
    "tf.app.flags.DEFINE_float('weight_decay', 0.0005, 'Weight decay')\n",
    "tf.app.flags.DEFINE_integer('summary_interval', 100, 'Interval for summary.')\n",
    "tf.app.flags.DEFINE_integer('val_interval', 1000, 'Interval for evaluation.')\n",
    "tf.app.flags.DEFINE_integer('max_steps', 80000, 'Maximum number of iterations.')\n",
    "tf.app.flags.DEFINE_integer('save_interval', 5000, '')\n",
    "tf.app.flags.DEFINE_string('f', '', 'kernel')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def one_hot_embedding(label, n_classes):\n",
    "  \"\"\"\n",
    "  One-hot embedding\n",
    "  Args:\n",
    "    label: int32 tensor [B]\n",
    "    n_classes: int32, number of classes\n",
    "  Return:\n",
    "    embedding: tensor [B x n_classes]\n",
    "  \"\"\"\n",
    "  embedding_params = np.eye(n_classes, dtype=np.float32)\n",
    "  with tf.device('/cpu:0'):\n",
    "    params = tf.constant(embedding_params)\n",
    "    embedding = tf.gather(params, label)\n",
    "  return embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, n_in, n_out, k, s, p='SAME', bias=False, scope='conv'):\n",
    "  with tf.variable_scope(scope):\n",
    "    kernel = tf.Variable(\n",
    "      tf.truncated_normal([k, k, n_in, n_out],\n",
    "        stddev=math.sqrt(2/(k*k*n_in))),\n",
    "      name='weight')\n",
    "    tf.add_to_collection('weights', kernel)\n",
    "    conv = tf.nn.conv2d(x, kernel, [1,s,s,1], padding=p)\n",
    "    if bias:\n",
    "      bias = tf.get_variable('bias', [n_out], initializer=tf.constant_initializer(0.0))\n",
    "      tf.add_to_collection('biases', bias)\n",
    "      conv = tf.nn.bias_add(conv, bias)\n",
    "  return conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def batch_norm(x, n_out, phase_train, scope='bn', affine=True):\n",
    "  \"\"\"\n",
    "  Batch normalization on convolutional maps.\n",
    "  Args:\n",
    "    x: Tensor, 4D BHWD input maps\n",
    "    n_out: integer, depth of input maps\n",
    "    phase_train: boolean tf.Variable, true indicates training phase\n",
    "    scope: string, variable scope\n",
    "    affine: whether to affine-transform outputs\n",
    "  Return:\n",
    "    normed: batch-normalized maps\n",
    "  \"\"\"\n",
    "  with tf.variable_scope(scope):\n",
    "    beta = tf.Variable(tf.constant(0.0, shape=[n_out]),\n",
    "      name='beta', trainable=True)\n",
    "    gamma = tf.Variable(tf.constant(1.0, shape=[n_out]),\n",
    "      name='gamma', trainable=affine)\n",
    "    tf.add_to_collection('biases', beta)\n",
    "    tf.add_to_collection('weights', gamma)\n",
    "\n",
    "    batch_mean, batch_var = tf.nn.moments(x, [0,1,2], name='moments')\n",
    "    ema = tf.train.ExponentialMovingAverage(decay=0.99)\n",
    "\n",
    "    def mean_var_with_update():\n",
    "      ema_apply_op = ema.apply([batch_mean, batch_var])\n",
    "      with tf.control_dependencies([ema_apply_op]):\n",
    "        return tf.identity(batch_mean), tf.identity(batch_var)\n",
    "    mean, var = control_flow_ops.cond(phase_train,\n",
    "      mean_var_with_update,\n",
    "      lambda: (ema.average(batch_mean), ema.average(batch_var)))\n",
    "\n",
    "    normed = tf.nn.batch_norm_with_global_normalization(x, mean, var, \n",
    "      beta, gamma, 1e-3, affine)\n",
    "  return normed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_block(x, n_in, n_out, subsample, phase_train, scope='res_block'):\n",
    "  with tf.variable_scope(scope):\n",
    "    if subsample:\n",
    "      y = conv2d(x, n_in, n_out, 3, 2, 'SAME', False, scope='conv_1')\n",
    "      shortcut = conv2d(x, n_in, n_out, 3, 2, 'SAME',\n",
    "                False, scope='shortcut')\n",
    "    else:\n",
    "      y = conv2d(x, n_in, n_out, 3, 1, 'SAME', False, scope='conv_1')\n",
    "      shortcut = tf.identity(x, name='shortcut')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_1')\n",
    "    y = tf.nn.relu(y, name='relu_1')\n",
    "    y = conv2d(y, n_out, n_out, 3, 1, 'SAME', True, scope='conv_2')\n",
    "    y = batch_norm(y, n_out, phase_train, scope='bn_2')\n",
    "    y = y + shortcut\n",
    "    y = tf.nn.relu(y, name='relu_2')\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_group(x, n_in, n_out, n, first_subsample, phase_train, scope='res_group'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = residual_block(x, n_in, n_out, first_subsample, phase_train, scope='block_1')\n",
    "    for i in range(n - 1):\n",
    "      y = residual_block(y, n_out, n_out, False, phase_train, scope='block_%d' % (i + 2))\n",
    "  return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def residual_net(x, n, n_classes, phase_train, scope='res_net'):\n",
    "  with tf.variable_scope(scope):\n",
    "    y = conv2d(x, 3, 16, 3, 1, 'SAME', False, scope='conv_init')\n",
    "    y = batch_norm(y, 16, phase_train, scope='bn_init')\n",
    "    y = tf.nn.relu(y, name='relu_init')\n",
    "    y = residual_group(y, 16, 16, n, False, phase_train, scope='group_1')\n",
    "    y = residual_group(y, 16, 32, n, True, phase_train, scope='group_2')\n",
    "    y = residual_group(y, 32, 64, n, True, phase_train, scope='group_3')\n",
    "#     y = conv2d(y, 64, n_classes, 1, 1, 'SAME', True, scope='conv_last')\n",
    "    y = tf.nn.avg_pool(y, [1, 8, 8, 1], [1, 1, 1, 1], 'VALID', name='avg_pool')\n",
    "    y = tf.reshape(y, [-1, 64])\n",
    "    w = tf.get_variable(name='weight_fc', shape=[64, n_classes], initializer=tf.contrib.layers.xavier_initializer_conv2d())\n",
    "    tf.add_to_collection('weights', w)\n",
    "    b = tf.get_variable(name='weight_biase', shape=[n_classes], initializer=tf.constant_initializer(0))\n",
    "    tf.add_to_collection('last_biases', b)\n",
    "    y = tf.matmul(y, w) + b\n",
    "#     y = tf.squeeze(y, squeeze_dims=[1, 2])\n",
    "  return y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _loss(logits, labels, scope='loss'):\n",
    "  with tf.variable_scope(scope):\n",
    "    # entropy loss\n",
    "    targets = one_hot_embedding(labels, 10)\n",
    "    entropy_loss = tf.reduce_mean(\n",
    "      tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=targets),\n",
    "      name='entropy_loss')\n",
    "    tf.add_to_collection('losses', entropy_loss)\n",
    "    # weight l2 decay loss\n",
    "    weight_l2_losses = [tf.nn.l2_loss(o) for o in tf.get_collection('weights')]\n",
    "    weight_decay_loss = FLAGS.weight_decay*tf.add_n(weight_l2_losses)\n",
    "    tf.add_to_collection('losses', weight_decay_loss)\n",
    "  # for var in tf.get_collection('losses'):\n",
    "    # tf.scalar_summary('losses/' + var.op.name, var)\n",
    "  # total loss\n",
    "  return tf.add_n(tf.get_collection('losses'), name='total_loss')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _accuracy(logits, gt_label, scope='accuracy'):\n",
    "  with tf.variable_scope(scope):\n",
    "    pred_label = tf.argmax(logits, 1)\n",
    "    acc = 1.0 - tf.nn.zero_fraction(\n",
    "      tf.cast(tf.equal(pred_label, gt_label), tf.int32))\n",
    "  return acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def _train_op(loss, global_step, learning_rate):\n",
    "  params = tf.trainable_variables()\n",
    "  gradients = tf.gradients(loss, params, name='gradients')\n",
    "  optim = tf.train.MomentumOptimizer(learning_rate, 0.9)\n",
    "  update = optim.apply_gradients(zip(gradients, params))\n",
    "  with tf.control_dependencies([update]):\n",
    "    train_op = tf.no_op(name='train_op')\n",
    "  return train_op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cifar10_input_stream(records_path):\n",
    "  reader = tf.TFRecordReader()\n",
    "  filename_queue = tf.train.string_input_producer([records_path], None)\n",
    "  _, record_value = reader.read(filename_queue)\n",
    "  features = tf.parse_single_example(record_value,\n",
    "    {\n",
    "      'image_raw': tf.FixedLenFeature([], tf.string),\n",
    "      'label': tf.FixedLenFeature([], tf.int64),\n",
    "    })\n",
    "  image = tf.decode_raw(features['image_raw'], tf.uint8)\n",
    "  image = tf.reshape(image, [32,32,3])\n",
    "  image = tf.cast(image, tf.float32)\n",
    "  label = tf.cast(features['label'], tf.int64)\n",
    "  return image, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def normalize_image(image):\n",
    "  # meanstd = joblib.load(FLAGS.mean_std_path)\n",
    "  # mean, std = meanstd['mean'], meanstd['std']\n",
    "  mean = [ 125.30690002,122.95014954,113.86599731]\n",
    "  std = [ 62.9932518,62.08860397,66.70500946]\n",
    "  normed_image = (image - mean) / std\n",
    "  return normed_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def random_distort_image(image):\n",
    "  distorted_image = image\n",
    "  distorted_image = tf.image.pad_to_bounding_box(\n",
    "    image, 4, 4, 40, 40)  # pad 4 pixels to each side\n",
    "  distorted_image = tf.random_crop(distorted_image, [32, 32, 3])\n",
    "  distorted_image = tf.image.random_flip_left_right(distorted_image)\n",
    "  return distorted_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_train_batch(train_records_path, batch_size):\n",
    "  with tf.variable_scope('train_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      train_image, train_label = cifar10_input_stream(train_records_path)\n",
    "      train_image = normalize_image(train_image)\n",
    "      train_image = random_distort_image(train_image)\n",
    "      train_image_batch, train_label_batch = tf.train.shuffle_batch(\n",
    "        [train_image, train_label], batch_size=batch_size, num_threads=4,\n",
    "        capacity=50000,\n",
    "        min_after_dequeue=1000)\n",
    "  return train_image_batch, train_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_validation_batch(test_records_path, batch_size):\n",
    "  with tf.variable_scope('evaluate_batch'):\n",
    "    with tf.device('/cpu:0'):\n",
    "      test_image, test_label = cifar10_input_stream(test_records_path)\n",
    "      test_image = normalize_image(test_image)\n",
    "      test_image_batch, test_label_batch = tf.train.batch(\n",
    "        [test_image, test_label], batch_size=batch_size, num_threads=1,\n",
    "        capacity=10000)\n",
    "  return test_image_batch, test_label_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-9-5533cdbb8cb6>:6: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "phase_train = tf.placeholder(tf.bool, name='phase_train')\n",
    "learning_rate = tf.placeholder(tf.float32, name='learning_rate')\n",
    "\n",
    "global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "\n",
    "\n",
    "train_image_batch, train_label_batch = make_train_batch(FLAGS.train_tf_path, FLAGS.train_batch_size)\n",
    "val_image_batch, val_label_batch = make_validation_batch(FLAGS.val_tf_path, FLAGS.val_batch_size)\n",
    "\n",
    "image_batch, label_batch = control_flow_ops.cond(phase_train,lambda: (train_image_batch, train_label_batch),lambda: (val_image_batch, val_label_batch))\n",
    "\n",
    "\n",
    "logits = residual_net(image_batch, FLAGS.residual_net_n, 10, phase_train)\n",
    "\n",
    "\n",
    "loss = _loss(logits, label_batch)\n",
    "accuracy = _accuracy(logits, label_batch)\n",
    "\n",
    "# train one step\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initializing...\n",
      "INFO:tensorflow:Restoring parameters from ./full_precision/res32/model/res.ckpt\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<Thread(QueueRunnerThread-train_batch/input_producer-train_batch/input_producer/input_producer_EnqueueMany, started daemon 140099526313728)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140099534706432)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140099543099136)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140099551491840)>,\n",
       " <Thread(QueueRunnerThread-train_batch/shuffle_batch/random_shuffle_queue-train_batch/shuffle_batch/random_shuffle_queue_enqueue, started daemon 140099517921024)>,\n",
       " <Thread(QueueRunnerThread-evaluate_batch/input_producer-evaluate_batch/input_producer/input_producer_EnqueueMany, started daemon 140099509528320)>,\n",
       " <Thread(QueueRunnerThread-evaluate_batch/batch/fifo_queue-evaluate_batch/batch/fifo_queue_enqueue, started daemon 140099501135616)>]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sess = tf.Session(config=tf.ConfigProto(log_device_placement=False))\n",
    "print('Initializing...')\n",
    "saver = tf.train.Saver()\n",
    "saver.restore(sess,'./full_precision/res32/model/res.ckpt')\n",
    "\n",
    "tf.train.start_queue_runners(sess=sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating...\n",
      "Test accuracy = 0.924600\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# validation\n",
    "\n",
    "print('Evaluating...')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "  fetches = [logits, label_batch, loss]\n",
    "  session_outputs = sess.run(\n",
    "    fetches, {phase_train.name: False})\n",
    "  val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "  val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "  val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(\n",
    "  pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def apply_prune_on_grads(grads_and_vars, dict_nzidx):\n",
    "    for key, nzidx in dict_nzidx.items():\n",
    "        count = 0\n",
    "        for grad, var in grads_and_vars:\n",
    "            if var.name == key:\n",
    "                nzidx_obj = tf.cast(tf.constant(sess.run(dict_nzidx[key])), tf.float32)\n",
    "                grads_and_vars[count] = (tf.multiply(nzidx_obj, grad), var)\n",
    "            count += 1\n",
    "    return grads_and_vars\n",
    "\n",
    "def apply_inq(weights, inq_dict, var_name, prune_rate):  \n",
    "    for target in var_name:\n",
    "        wl = target\n",
    "        bit = 8\n",
    "\n",
    "        weight_obj = weights[wl]\n",
    "        weight_arr = sess.run(weight_obj)\n",
    "\n",
    "        weight_rest = np.reshape(weight_arr, [-1])\n",
    "        dic_tem = np.reshape(sess.run(inq_dict[wl]), [-1])\n",
    "        idx_rest = np.flip(np.argsort(abs(np.reshape(weight_rest, [-1]))), 0)\n",
    "        num_prune = int(len(weight_rest) * prune_rate)\n",
    "        weight_toINQ = weight_rest[idx_rest[:num_prune]]\n",
    "\n",
    "        n1 = (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        n2 = n1 + 1 - bit / 4\n",
    "        upper_bound = 2 ** (np.floor(np.log2(max(abs(np.reshape(weight_arr, [-1]))) * 4 / 3)))\n",
    "        lower_bound = 2 ** (n1 + 1 - bit / 4)\n",
    "\n",
    "        weight_toINQ[abs(weight_toINQ) < lower_bound] = 0\n",
    "        weight_toINQ[weight_toINQ != 0] = 2 ** (np.floor(np.log2(abs(weight_toINQ[weight_toINQ != 0] * 4 / 3)))) * np.sign(weight_toINQ[weight_toINQ != 0])\n",
    "\n",
    "        weight_rest[idx_rest[:num_prune]] = weight_toINQ\n",
    "        weight_arr = np.reshape(weight_rest, np.shape(weight_arr))\n",
    "        dic_tem[idx_rest[:num_prune]] = np.zeros_like(dic_tem[idx_rest[:num_prune]])\n",
    "        inq_dict[wl] = tf.cast(np.reshape(dic_tem, np.shape(sess.run(inq_dict[wl]))), tf.float32)\n",
    "        sess.run(weights[wl].assign(weight_arr))\n",
    "    return inq_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 开始压缩"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一轮  量化\n",
    "prune_rate =0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.924600\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.02709051 -0.08279994  0.01822233  0.11905745 -0.00915685 -0.03560071\n",
      "  0.01750009  0.03826123  0.00807592 -0.10065438]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate = 0时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.5)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 16:44:12.339893] Iteration 100, train loss = 1.775847, train accuracy = 0.328125\n",
      "[2018-07-16 16:44:18.248466] Iteration 200, train loss = 1.720166, train accuracy = 0.414062\n",
      "[2018-07-16 16:44:23.982442] Iteration 300, train loss = 1.443589, train accuracy = 0.453125\n",
      "[2018-07-16 16:44:29.593198] Iteration 400, train loss = 1.378587, train accuracy = 0.554688\n",
      "[2018-07-16 16:44:35.065295] Iteration 500, train loss = 1.179707, train accuracy = 0.585938\n",
      "[2018-07-16 16:44:40.555774] Iteration 600, train loss = 1.090788, train accuracy = 0.648438\n",
      "[2018-07-16 16:44:46.078564] Iteration 700, train loss = 1.278406, train accuracy = 0.554688\n",
      "[2018-07-16 16:44:51.812867] Iteration 800, train loss = 1.191014, train accuracy = 0.539062\n",
      "[2018-07-16 16:44:57.586451] Iteration 900, train loss = 0.941286, train accuracy = 0.695312\n",
      "[2018-07-16 16:45:03.417143] Iteration 1000, train loss = 0.932861, train accuracy = 0.656250\n",
      "Evaluating...\n",
      "Test accuracy = 0.673600\n",
      "[2018-07-16 16:45:10.845721] Iteration 1100, train loss = 0.992374, train accuracy = 0.656250\n",
      "[2018-07-16 16:45:16.585269] Iteration 1200, train loss = 0.805829, train accuracy = 0.718750\n",
      "[2018-07-16 16:45:22.142186] Iteration 1300, train loss = 0.753246, train accuracy = 0.765625\n",
      "[2018-07-16 16:45:27.731900] Iteration 1400, train loss = 0.680584, train accuracy = 0.789062\n",
      "[2018-07-16 16:45:33.327551] Iteration 1500, train loss = 0.882691, train accuracy = 0.718750\n",
      "[2018-07-16 16:45:38.945583] Iteration 1600, train loss = 0.539469, train accuracy = 0.812500\n",
      "[2018-07-16 16:45:44.538102] Iteration 1700, train loss = 0.646717, train accuracy = 0.789062\n",
      "[2018-07-16 16:45:50.195727] Iteration 1800, train loss = 0.947810, train accuracy = 0.679688\n",
      "[2018-07-16 16:45:55.958709] Iteration 1900, train loss = 0.685902, train accuracy = 0.804688\n",
      "[2018-07-16 16:46:01.732894] Iteration 2000, train loss = 0.666907, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.528300\n",
      "[2018-07-16 16:46:09.219884] Iteration 2100, train loss = 0.655903, train accuracy = 0.789062\n",
      "[2018-07-16 16:46:14.905247] Iteration 2200, train loss = 0.598685, train accuracy = 0.812500\n",
      "[2018-07-16 16:46:20.767040] Iteration 2300, train loss = 0.569472, train accuracy = 0.843750\n",
      "[2018-07-16 16:46:26.622474] Iteration 2400, train loss = 0.670067, train accuracy = 0.789062\n",
      "[2018-07-16 16:46:32.511373] Iteration 2500, train loss = 0.471726, train accuracy = 0.859375\n",
      "[2018-07-16 16:46:38.378497] Iteration 2600, train loss = 0.611814, train accuracy = 0.812500\n",
      "[2018-07-16 16:46:44.365613] Iteration 2700, train loss = 0.569020, train accuracy = 0.812500\n",
      "[2018-07-16 16:46:50.244746] Iteration 2800, train loss = 0.699203, train accuracy = 0.773438\n",
      "[2018-07-16 16:46:56.080590] Iteration 2900, train loss = 0.669685, train accuracy = 0.804688\n",
      "[2018-07-16 16:47:01.995298] Iteration 3000, train loss = 0.492697, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.770100\n",
      "[2018-07-16 16:47:09.864153] Iteration 3100, train loss = 0.431295, train accuracy = 0.882812\n",
      "[2018-07-16 16:47:16.095458] Iteration 3200, train loss = 0.583423, train accuracy = 0.843750\n",
      "[2018-07-16 16:47:22.464380] Iteration 3300, train loss = 0.742774, train accuracy = 0.765625\n",
      "[2018-07-16 16:47:28.536746] Iteration 3400, train loss = 0.459151, train accuracy = 0.867188\n",
      "[2018-07-16 16:47:34.616145] Iteration 3500, train loss = 0.599172, train accuracy = 0.812500\n",
      "[2018-07-16 16:47:41.031750] Iteration 3600, train loss = 0.450143, train accuracy = 0.875000\n",
      "[2018-07-16 16:47:47.202296] Iteration 3700, train loss = 0.505318, train accuracy = 0.820312\n",
      "[2018-07-16 16:47:53.229074] Iteration 3800, train loss = 0.570778, train accuracy = 0.843750\n",
      "[2018-07-16 16:47:59.327121] Iteration 3900, train loss = 0.460966, train accuracy = 0.875000\n",
      "[2018-07-16 16:48:05.500847] Iteration 4000, train loss = 0.539669, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.823400\n",
      "[2018-07-16 16:48:13.501519] Iteration 4100, train loss = 0.374494, train accuracy = 0.882812\n",
      "[2018-07-16 16:48:19.622304] Iteration 4200, train loss = 0.378901, train accuracy = 0.882812\n",
      "[2018-07-16 16:48:25.748950] Iteration 4300, train loss = 0.366895, train accuracy = 0.906250\n",
      "[2018-07-16 16:48:31.855594] Iteration 4400, train loss = 0.650932, train accuracy = 0.789062\n",
      "[2018-07-16 16:48:37.902599] Iteration 4500, train loss = 0.493462, train accuracy = 0.859375\n",
      "[2018-07-16 16:48:44.034132] Iteration 4600, train loss = 0.329217, train accuracy = 0.890625\n",
      "[2018-07-16 16:48:50.098785] Iteration 4700, train loss = 0.381281, train accuracy = 0.882812\n",
      "[2018-07-16 16:48:56.236333] Iteration 4800, train loss = 0.401975, train accuracy = 0.890625\n",
      "[2018-07-16 16:49:02.299275] Iteration 4900, train loss = 0.397161, train accuracy = 0.875000\n",
      "[2018-07-16 16:49:08.516074] Iteration 5000, train loss = 0.536488, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.771100\n",
      "[2018-07-16 16:49:16.521470] Iteration 5100, train loss = 0.513564, train accuracy = 0.804688\n",
      "[2018-07-16 16:49:22.600010] Iteration 5200, train loss = 0.394926, train accuracy = 0.867188\n",
      "[2018-07-16 16:49:28.675078] Iteration 5300, train loss = 0.556205, train accuracy = 0.867188\n",
      "[2018-07-16 16:49:34.769767] Iteration 5400, train loss = 0.411012, train accuracy = 0.867188\n",
      "[2018-07-16 16:49:40.955147] Iteration 5500, train loss = 0.417812, train accuracy = 0.867188\n",
      "[2018-07-16 16:49:47.063867] Iteration 5600, train loss = 0.447538, train accuracy = 0.882812\n",
      "[2018-07-16 16:49:53.445085] Iteration 5700, train loss = 0.464591, train accuracy = 0.898438\n",
      "[2018-07-16 16:49:59.574026] Iteration 5800, train loss = 0.404954, train accuracy = 0.875000\n",
      "[2018-07-16 16:50:05.741981] Iteration 5900, train loss = 0.358231, train accuracy = 0.906250\n",
      "[2018-07-16 16:50:12.018217] Iteration 6000, train loss = 0.425013, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.836800\n",
      "[2018-07-16 16:50:20.221730] Iteration 6100, train loss = 0.382254, train accuracy = 0.890625\n",
      "[2018-07-16 16:50:26.535553] Iteration 6200, train loss = 0.373289, train accuracy = 0.914062\n",
      "[2018-07-16 16:50:32.663035] Iteration 6300, train loss = 0.374603, train accuracy = 0.906250\n",
      "[2018-07-16 16:50:38.946341] Iteration 6400, train loss = 0.544433, train accuracy = 0.835938\n",
      "[2018-07-16 16:50:45.209440] Iteration 6500, train loss = 0.331017, train accuracy = 0.921875\n",
      "[2018-07-16 16:50:51.301257] Iteration 6600, train loss = 0.477992, train accuracy = 0.828125\n",
      "[2018-07-16 16:50:57.445222] Iteration 6700, train loss = 0.300741, train accuracy = 0.921875\n",
      "[2018-07-16 16:51:03.595506] Iteration 6800, train loss = 0.302469, train accuracy = 0.929688\n",
      "[2018-07-16 16:51:09.993172] Iteration 6900, train loss = 0.422213, train accuracy = 0.875000\n",
      "[2018-07-16 16:51:16.169710] Iteration 7000, train loss = 0.436954, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.844100\n",
      "[2018-07-16 16:51:24.205380] Iteration 7100, train loss = 0.438129, train accuracy = 0.875000\n",
      "[2018-07-16 16:51:30.305262] Iteration 7200, train loss = 0.405983, train accuracy = 0.882812\n",
      "[2018-07-16 16:51:36.449969] Iteration 7300, train loss = 0.426461, train accuracy = 0.890625\n",
      "[2018-07-16 16:51:42.565866] Iteration 7400, train loss = 0.298883, train accuracy = 0.929688\n",
      "[2018-07-16 16:51:48.647177] Iteration 7500, train loss = 0.365419, train accuracy = 0.875000\n",
      "[2018-07-16 16:51:54.831221] Iteration 7600, train loss = 0.459116, train accuracy = 0.843750\n",
      "[2018-07-16 16:52:00.913471] Iteration 7700, train loss = 0.373768, train accuracy = 0.906250\n",
      "[2018-07-16 16:52:07.023661] Iteration 7800, train loss = 0.514071, train accuracy = 0.820312\n",
      "[2018-07-16 16:52:13.131678] Iteration 7900, train loss = 0.355543, train accuracy = 0.906250\n",
      "[2018-07-16 16:52:19.246016] Iteration 8000, train loss = 0.417275, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.835900\n",
      "[2018-07-16 16:52:27.205379] Iteration 8100, train loss = 0.363435, train accuracy = 0.921875\n",
      "[2018-07-16 16:52:33.280640] Iteration 8200, train loss = 0.382574, train accuracy = 0.906250\n",
      "[2018-07-16 16:52:39.385859] Iteration 8300, train loss = 0.379233, train accuracy = 0.898438\n",
      "[2018-07-16 16:52:45.467256] Iteration 8400, train loss = 0.338342, train accuracy = 0.929688\n",
      "[2018-07-16 16:52:51.565246] Iteration 8500, train loss = 0.437128, train accuracy = 0.867188\n",
      "[2018-07-16 16:52:57.618328] Iteration 8600, train loss = 0.259196, train accuracy = 0.945312\n",
      "[2018-07-16 16:53:03.706048] Iteration 8700, train loss = 0.397904, train accuracy = 0.859375\n",
      "[2018-07-16 16:53:09.787318] Iteration 8800, train loss = 0.378608, train accuracy = 0.882812\n",
      "[2018-07-16 16:53:15.898328] Iteration 8900, train loss = 0.415684, train accuracy = 0.882812\n",
      "[2018-07-16 16:53:22.030455] Iteration 9000, train loss = 0.393788, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.789500\n",
      "[2018-07-16 16:53:29.977304] Iteration 9100, train loss = 0.269660, train accuracy = 0.921875\n",
      "[2018-07-16 16:53:36.035899] Iteration 9200, train loss = 0.373305, train accuracy = 0.890625\n",
      "[2018-07-16 16:53:42.116943] Iteration 9300, train loss = 0.436019, train accuracy = 0.882812\n",
      "[2018-07-16 16:53:48.204822] Iteration 9400, train loss = 0.342446, train accuracy = 0.882812\n",
      "[2018-07-16 16:53:54.303792] Iteration 9500, train loss = 0.376384, train accuracy = 0.890625\n",
      "[2018-07-16 16:54:00.390455] Iteration 9600, train loss = 0.285133, train accuracy = 0.937500\n",
      "[2018-07-16 16:54:06.434540] Iteration 9700, train loss = 0.291299, train accuracy = 0.906250\n",
      "[2018-07-16 16:54:12.473931] Iteration 9800, train loss = 0.495836, train accuracy = 0.890625\n",
      "[2018-07-16 16:54:18.541549] Iteration 9900, train loss = 0.319611, train accuracy = 0.906250\n",
      "[2018-07-16 16:54:24.595353] Iteration 10000, train loss = 0.389950, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.775800\n",
      "[2018-07-16 16:54:32.506209] Iteration 10100, train loss = 0.367736, train accuracy = 0.867188\n",
      "[2018-07-16 16:54:38.596196] Iteration 10200, train loss = 0.400776, train accuracy = 0.867188\n",
      "[2018-07-16 16:54:44.704242] Iteration 10300, train loss = 0.411215, train accuracy = 0.898438\n",
      "[2018-07-16 16:54:50.720706] Iteration 10400, train loss = 0.331179, train accuracy = 0.906250\n",
      "[2018-07-16 16:54:56.791513] Iteration 10500, train loss = 0.427618, train accuracy = 0.851562\n",
      "[2018-07-16 16:55:02.902292] Iteration 10600, train loss = 0.259943, train accuracy = 0.937500\n",
      "[2018-07-16 16:55:09.030135] Iteration 10700, train loss = 0.494015, train accuracy = 0.828125\n",
      "[2018-07-16 16:55:15.132395] Iteration 10800, train loss = 0.411574, train accuracy = 0.882812\n",
      "[2018-07-16 16:55:21.194296] Iteration 10900, train loss = 0.394946, train accuracy = 0.906250\n",
      "[2018-07-16 16:55:27.253678] Iteration 11000, train loss = 0.302608, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.778000\n",
      "[2018-07-16 16:55:35.094405] Iteration 11100, train loss = 0.232117, train accuracy = 0.929688\n",
      "[2018-07-16 16:55:41.167771] Iteration 11200, train loss = 0.340607, train accuracy = 0.921875\n",
      "[2018-07-16 16:55:47.235253] Iteration 11300, train loss = 0.406323, train accuracy = 0.882812\n",
      "[2018-07-16 16:55:53.327334] Iteration 11400, train loss = 0.299023, train accuracy = 0.929688\n",
      "[2018-07-16 16:55:59.427541] Iteration 11500, train loss = 0.267207, train accuracy = 0.945312\n",
      "[2018-07-16 16:56:05.510036] Iteration 11600, train loss = 0.339468, train accuracy = 0.921875\n",
      "[2018-07-16 16:56:11.625648] Iteration 11700, train loss = 0.377728, train accuracy = 0.867188\n",
      "[2018-07-16 16:56:17.739087] Iteration 11800, train loss = 0.371067, train accuracy = 0.875000\n",
      "[2018-07-16 16:56:23.839612] Iteration 11900, train loss = 0.316971, train accuracy = 0.898438\n",
      "[2018-07-16 16:56:30.114997] Iteration 12000, train loss = 0.262663, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.836400\n",
      "[2018-07-16 16:56:38.398496] Iteration 12100, train loss = 0.256582, train accuracy = 0.937500\n",
      "[2018-07-16 16:56:44.685344] Iteration 12200, train loss = 0.376852, train accuracy = 0.882812\n",
      "[2018-07-16 16:56:50.884180] Iteration 12300, train loss = 0.383325, train accuracy = 0.859375\n",
      "[2018-07-16 16:56:57.268667] Iteration 12400, train loss = 0.347696, train accuracy = 0.906250\n",
      "[2018-07-16 16:57:03.737522] Iteration 12500, train loss = 0.323584, train accuracy = 0.882812\n",
      "[2018-07-16 16:57:10.147060] Iteration 12600, train loss = 0.342259, train accuracy = 0.921875\n",
      "[2018-07-16 16:57:16.470413] Iteration 12700, train loss = 0.331055, train accuracy = 0.929688\n",
      "[2018-07-16 16:57:22.617039] Iteration 12800, train loss = 0.254560, train accuracy = 0.945312\n",
      "[2018-07-16 16:57:29.058088] Iteration 12900, train loss = 0.382552, train accuracy = 0.867188\n",
      "[2018-07-16 16:57:35.497104] Iteration 13000, train loss = 0.284016, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.854400\n",
      "[2018-07-16 16:57:43.783744] Iteration 13100, train loss = 0.266423, train accuracy = 0.914062\n",
      "[2018-07-16 16:57:50.113441] Iteration 13200, train loss = 0.219274, train accuracy = 0.953125\n",
      "[2018-07-16 16:57:56.321129] Iteration 13300, train loss = 0.336089, train accuracy = 0.898438\n",
      "[2018-07-16 16:58:02.351543] Iteration 13400, train loss = 0.239868, train accuracy = 0.953125\n",
      "[2018-07-16 16:58:08.479161] Iteration 13500, train loss = 0.430957, train accuracy = 0.859375\n",
      "[2018-07-16 16:58:14.616705] Iteration 13600, train loss = 0.202565, train accuracy = 0.960938\n",
      "[2018-07-16 16:58:20.753771] Iteration 13700, train loss = 0.254541, train accuracy = 0.945312\n",
      "[2018-07-16 16:58:26.872670] Iteration 13800, train loss = 0.218841, train accuracy = 0.953125\n",
      "[2018-07-16 16:58:32.993108] Iteration 13900, train loss = 0.244491, train accuracy = 0.921875\n",
      "[2018-07-16 16:58:39.102435] Iteration 14000, train loss = 0.198441, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.829000\n",
      "[2018-07-16 16:58:47.092655] Iteration 14100, train loss = 0.417347, train accuracy = 0.875000\n",
      "[2018-07-16 16:58:53.195226] Iteration 14200, train loss = 0.355672, train accuracy = 0.890625\n",
      "[2018-07-16 16:58:59.279710] Iteration 14300, train loss = 0.265013, train accuracy = 0.937500\n",
      "[2018-07-16 16:59:05.380291] Iteration 14400, train loss = 0.318625, train accuracy = 0.898438\n",
      "[2018-07-16 16:59:11.489101] Iteration 14500, train loss = 0.265337, train accuracy = 0.929688\n",
      "[2018-07-16 16:59:17.558046] Iteration 14600, train loss = 0.346312, train accuracy = 0.929688\n",
      "[2018-07-16 16:59:23.668497] Iteration 14700, train loss = 0.263345, train accuracy = 0.945312\n",
      "[2018-07-16 16:59:29.768135] Iteration 14800, train loss = 0.307089, train accuracy = 0.914062\n",
      "[2018-07-16 16:59:35.924775] Iteration 14900, train loss = 0.343359, train accuracy = 0.929688\n",
      "[2018-07-16 16:59:41.998913] Iteration 15000, train loss = 0.280912, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.849400\n",
      "[2018-07-16 16:59:49.929150] Iteration 15100, train loss = 0.362132, train accuracy = 0.898438\n",
      "[2018-07-16 16:59:56.055260] Iteration 15200, train loss = 0.390683, train accuracy = 0.875000\n",
      "[2018-07-16 17:00:02.188869] Iteration 15300, train loss = 0.184037, train accuracy = 0.976562\n",
      "[2018-07-16 17:00:08.264948] Iteration 15400, train loss = 0.267041, train accuracy = 0.898438\n",
      "[2018-07-16 17:00:14.348539] Iteration 15500, train loss = 0.322327, train accuracy = 0.914062\n",
      "[2018-07-16 17:00:20.460487] Iteration 15600, train loss = 0.331474, train accuracy = 0.921875\n",
      "[2018-07-16 17:00:26.563877] Iteration 15700, train loss = 0.262826, train accuracy = 0.937500\n",
      "[2018-07-16 17:00:32.634965] Iteration 15800, train loss = 0.245795, train accuracy = 0.929688\n",
      "[2018-07-16 17:00:38.772986] Iteration 15900, train loss = 0.249792, train accuracy = 0.953125\n",
      "[2018-07-16 17:00:44.871168] Iteration 16000, train loss = 0.277226, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.848000\n",
      "[2018-07-16 17:00:52.871321] Iteration 16100, train loss = 0.271277, train accuracy = 0.937500\n",
      "[2018-07-16 17:00:59.015630] Iteration 16200, train loss = 0.216440, train accuracy = 0.945312\n",
      "[2018-07-16 17:01:05.125961] Iteration 16300, train loss = 0.260016, train accuracy = 0.914062\n",
      "[2018-07-16 17:01:11.247756] Iteration 16400, train loss = 0.207456, train accuracy = 0.953125\n",
      "[2018-07-16 17:01:17.393216] Iteration 16500, train loss = 0.447032, train accuracy = 0.859375\n",
      "[2018-07-16 17:01:23.503292] Iteration 16600, train loss = 0.333735, train accuracy = 0.921875\n",
      "[2018-07-16 17:01:29.644834] Iteration 16700, train loss = 0.242173, train accuracy = 0.937500\n",
      "[2018-07-16 17:01:35.756831] Iteration 16800, train loss = 0.227085, train accuracy = 0.945312\n",
      "[2018-07-16 17:01:41.916791] Iteration 16900, train loss = 0.237281, train accuracy = 0.937500\n",
      "[2018-07-16 17:01:48.038121] Iteration 17000, train loss = 0.310491, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.833100\n",
      "[2018-07-16 17:01:56.067116] Iteration 17100, train loss = 0.302086, train accuracy = 0.929688\n",
      "[2018-07-16 17:02:02.202487] Iteration 17200, train loss = 0.244332, train accuracy = 0.921875\n",
      "[2018-07-16 17:02:08.308411] Iteration 17300, train loss = 0.214557, train accuracy = 0.960938\n",
      "[2018-07-16 17:02:14.413279] Iteration 17400, train loss = 0.196798, train accuracy = 0.984375\n",
      "[2018-07-16 17:02:20.553425] Iteration 17500, train loss = 0.221988, train accuracy = 0.945312\n",
      "[2018-07-16 17:02:26.672250] Iteration 17600, train loss = 0.237360, train accuracy = 0.960938\n",
      "[2018-07-16 17:02:32.781936] Iteration 17700, train loss = 0.278014, train accuracy = 0.929688\n",
      "[2018-07-16 17:02:38.864232] Iteration 17800, train loss = 0.221815, train accuracy = 0.929688\n",
      "[2018-07-16 17:02:45.011770] Iteration 17900, train loss = 0.438132, train accuracy = 0.859375\n",
      "[2018-07-16 17:02:51.173766] Iteration 18000, train loss = 0.164999, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.853200\n",
      "[2018-07-16 17:02:59.163019] Iteration 18100, train loss = 0.322772, train accuracy = 0.921875\n",
      "[2018-07-16 17:03:05.274309] Iteration 18200, train loss = 0.247992, train accuracy = 0.937500\n",
      "[2018-07-16 17:03:11.403882] Iteration 18300, train loss = 0.357081, train accuracy = 0.898438\n",
      "[2018-07-16 17:03:17.473925] Iteration 18400, train loss = 0.221142, train accuracy = 0.960938\n",
      "[2018-07-16 17:03:23.629569] Iteration 18500, train loss = 0.232793, train accuracy = 0.937500\n",
      "[2018-07-16 17:03:29.769669] Iteration 18600, train loss = 0.168234, train accuracy = 0.968750\n",
      "[2018-07-16 17:03:35.884929] Iteration 18700, train loss = 0.301950, train accuracy = 0.921875\n",
      "[2018-07-16 17:03:42.038097] Iteration 18800, train loss = 0.227791, train accuracy = 0.921875\n",
      "[2018-07-16 17:03:48.179218] Iteration 18900, train loss = 0.239387, train accuracy = 0.945312\n",
      "[2018-07-16 17:03:54.340396] Iteration 19000, train loss = 0.273124, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.864400\n",
      "[2018-07-16 17:04:02.284530] Iteration 19100, train loss = 0.284928, train accuracy = 0.914062\n",
      "[2018-07-16 17:04:08.428928] Iteration 19200, train loss = 0.212273, train accuracy = 0.960938\n",
      "[2018-07-16 17:04:14.556487] Iteration 19300, train loss = 0.220616, train accuracy = 0.953125\n",
      "[2018-07-16 17:04:20.693692] Iteration 19400, train loss = 0.282959, train accuracy = 0.937500\n",
      "[2018-07-16 17:04:26.819810] Iteration 19500, train loss = 0.317536, train accuracy = 0.906250\n",
      "[2018-07-16 17:04:32.955331] Iteration 19600, train loss = 0.162793, train accuracy = 0.968750\n",
      "[2018-07-16 17:04:39.084226] Iteration 19700, train loss = 0.291346, train accuracy = 0.937500\n",
      "[2018-07-16 17:04:45.170247] Iteration 19800, train loss = 0.291906, train accuracy = 0.937500\n",
      "[2018-07-16 17:04:51.322068] Iteration 19900, train loss = 0.160566, train accuracy = 0.953125\n",
      "[2018-07-16 17:04:57.471475] Iteration 20000, train loss = 0.185180, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.872800\n",
      "[2018-07-16 17:05:05.480590] Iteration 20100, train loss = 0.247927, train accuracy = 0.929688\n",
      "[2018-07-16 17:05:11.631883] Iteration 20200, train loss = 0.252094, train accuracy = 0.937500\n",
      "[2018-07-16 17:05:17.770557] Iteration 20300, train loss = 0.228298, train accuracy = 0.960938\n",
      "[2018-07-16 17:05:23.882245] Iteration 20400, train loss = 0.218814, train accuracy = 0.929688\n",
      "[2018-07-16 17:05:30.011396] Iteration 20500, train loss = 0.282475, train accuracy = 0.921875\n",
      "[2018-07-16 17:05:36.146816] Iteration 20600, train loss = 0.195949, train accuracy = 0.960938\n",
      "[2018-07-16 17:05:42.264793] Iteration 20700, train loss = 0.235375, train accuracy = 0.953125\n",
      "[2018-07-16 17:05:48.370793] Iteration 20800, train loss = 0.222099, train accuracy = 0.937500\n",
      "[2018-07-16 17:05:54.487719] Iteration 20900, train loss = 0.247786, train accuracy = 0.929688\n",
      "[2018-07-16 17:06:00.555046] Iteration 21000, train loss = 0.301870, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.870800\n",
      "[2018-07-16 17:06:08.550556] Iteration 21100, train loss = 0.271794, train accuracy = 0.937500\n",
      "[2018-07-16 17:06:14.639941] Iteration 21200, train loss = 0.182793, train accuracy = 0.976562\n",
      "[2018-07-16 17:06:20.771285] Iteration 21300, train loss = 0.262437, train accuracy = 0.953125\n",
      "[2018-07-16 17:06:26.871998] Iteration 21400, train loss = 0.182524, train accuracy = 0.968750\n",
      "[2018-07-16 17:06:32.999185] Iteration 21500, train loss = 0.316297, train accuracy = 0.906250\n",
      "[2018-07-16 17:06:39.050154] Iteration 21600, train loss = 0.256655, train accuracy = 0.945312\n",
      "[2018-07-16 17:06:45.163313] Iteration 21700, train loss = 0.247743, train accuracy = 0.960938\n",
      "[2018-07-16 17:06:51.276129] Iteration 21800, train loss = 0.202105, train accuracy = 0.953125\n",
      "[2018-07-16 17:06:57.417335] Iteration 21900, train loss = 0.227044, train accuracy = 0.929688\n",
      "[2018-07-16 17:07:03.575445] Iteration 22000, train loss = 0.206240, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.853500\n",
      "[2018-07-16 17:07:11.547410] Iteration 22100, train loss = 0.218817, train accuracy = 0.945312\n",
      "[2018-07-16 17:07:17.681854] Iteration 22200, train loss = 0.216147, train accuracy = 0.945312\n",
      "[2018-07-16 17:07:23.751560] Iteration 22300, train loss = 0.285515, train accuracy = 0.914062\n",
      "[2018-07-16 17:07:29.892579] Iteration 22400, train loss = 0.167775, train accuracy = 0.984375\n",
      "[2018-07-16 17:07:36.015295] Iteration 22500, train loss = 0.289319, train accuracy = 0.914062\n",
      "[2018-07-16 17:07:42.150566] Iteration 22600, train loss = 0.209765, train accuracy = 0.937500\n",
      "[2018-07-16 17:07:48.298005] Iteration 22700, train loss = 0.197780, train accuracy = 0.960938\n",
      "[2018-07-16 17:07:54.437842] Iteration 22800, train loss = 0.266871, train accuracy = 0.914062\n",
      "[2018-07-16 17:08:00.560938] Iteration 22900, train loss = 0.231610, train accuracy = 0.937500\n",
      "[2018-07-16 17:08:06.694939] Iteration 23000, train loss = 0.244502, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.865800\n",
      "[2018-07-16 17:08:14.685315] Iteration 23100, train loss = 0.230210, train accuracy = 0.937500\n",
      "[2018-07-16 17:08:20.756458] Iteration 23200, train loss = 0.269507, train accuracy = 0.929688\n",
      "[2018-07-16 17:08:26.887386] Iteration 23300, train loss = 0.260236, train accuracy = 0.929688\n",
      "[2018-07-16 17:08:33.025967] Iteration 23400, train loss = 0.283688, train accuracy = 0.898438\n",
      "[2018-07-16 17:08:39.121912] Iteration 23500, train loss = 0.248083, train accuracy = 0.921875\n",
      "[2018-07-16 17:08:45.267540] Iteration 23600, train loss = 0.260736, train accuracy = 0.953125\n",
      "[2018-07-16 17:08:51.414424] Iteration 23700, train loss = 0.201020, train accuracy = 0.937500\n",
      "[2018-07-16 17:08:57.524907] Iteration 23800, train loss = 0.142606, train accuracy = 0.960938\n",
      "[2018-07-16 17:09:03.613119] Iteration 23900, train loss = 0.261385, train accuracy = 0.937500\n",
      "[2018-07-16 17:09:09.759324] Iteration 24000, train loss = 0.233840, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.854200\n",
      "[2018-07-16 17:09:17.778533] Iteration 24100, train loss = 0.248619, train accuracy = 0.937500\n",
      "[2018-07-16 17:09:23.860065] Iteration 24200, train loss = 0.198016, train accuracy = 0.953125\n",
      "[2018-07-16 17:09:29.974662] Iteration 24300, train loss = 0.248626, train accuracy = 0.929688\n",
      "[2018-07-16 17:09:36.118939] Iteration 24400, train loss = 0.226297, train accuracy = 0.945312\n",
      "[2018-07-16 17:09:42.265075] Iteration 24500, train loss = 0.223811, train accuracy = 0.960938\n",
      "[2018-07-16 17:09:48.425777] Iteration 24600, train loss = 0.203522, train accuracy = 0.945312\n",
      "[2018-07-16 17:09:54.571292] Iteration 24700, train loss = 0.201017, train accuracy = 0.953125\n",
      "[2018-07-16 17:10:00.696262] Iteration 24800, train loss = 0.177559, train accuracy = 0.968750\n",
      "[2018-07-16 17:10:06.805246] Iteration 24900, train loss = 0.280640, train accuracy = 0.937500\n",
      "[2018-07-16 17:10:12.965961] Iteration 25000, train loss = 0.123237, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.864700\n",
      "[2018-07-16 17:10:20.905329] Iteration 25100, train loss = 0.152664, train accuracy = 0.976562\n",
      "[2018-07-16 17:10:27.021683] Iteration 25200, train loss = 0.217325, train accuracy = 0.945312\n",
      "[2018-07-16 17:10:33.144375] Iteration 25300, train loss = 0.198361, train accuracy = 0.953125\n",
      "[2018-07-16 17:10:39.268723] Iteration 25400, train loss = 0.206261, train accuracy = 0.960938\n",
      "[2018-07-16 17:10:45.424896] Iteration 25500, train loss = 0.256646, train accuracy = 0.953125\n",
      "[2018-07-16 17:10:51.583114] Iteration 25600, train loss = 0.149975, train accuracy = 0.968750\n",
      "[2018-07-16 17:10:57.672267] Iteration 25700, train loss = 0.178142, train accuracy = 0.960938\n",
      "[2018-07-16 17:11:03.758011] Iteration 25800, train loss = 0.291266, train accuracy = 0.921875\n",
      "[2018-07-16 17:11:09.888806] Iteration 25900, train loss = 0.253218, train accuracy = 0.945312\n",
      "[2018-07-16 17:11:16.029413] Iteration 26000, train loss = 0.280373, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.848900\n",
      "[2018-07-16 17:11:24.012862] Iteration 26100, train loss = 0.172682, train accuracy = 0.968750\n",
      "[2018-07-16 17:11:30.146471] Iteration 26200, train loss = 0.226237, train accuracy = 0.937500\n",
      "[2018-07-16 17:11:36.263371] Iteration 26300, train loss = 0.235484, train accuracy = 0.960938\n",
      "[2018-07-16 17:11:42.389619] Iteration 26400, train loss = 0.177150, train accuracy = 0.960938\n",
      "[2018-07-16 17:11:48.515796] Iteration 26500, train loss = 0.239328, train accuracy = 0.937500\n",
      "[2018-07-16 17:11:54.585279] Iteration 26600, train loss = 0.230651, train accuracy = 0.929688\n",
      "[2018-07-16 17:12:00.720359] Iteration 26700, train loss = 0.228334, train accuracy = 0.953125\n",
      "[2018-07-16 17:12:06.832174] Iteration 26800, train loss = 0.224678, train accuracy = 0.960938\n",
      "[2018-07-16 17:12:12.953328] Iteration 26900, train loss = 0.242623, train accuracy = 0.937500\n",
      "[2018-07-16 17:12:19.105412] Iteration 27000, train loss = 0.182556, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.854400\n",
      "[2018-07-16 17:12:27.077804] Iteration 27100, train loss = 0.228607, train accuracy = 0.945312\n",
      "[2018-07-16 17:12:33.182481] Iteration 27200, train loss = 0.216389, train accuracy = 0.953125\n",
      "[2018-07-16 17:12:39.318698] Iteration 27300, train loss = 0.241372, train accuracy = 0.929688\n",
      "[2018-07-16 17:12:45.448623] Iteration 27400, train loss = 0.276245, train accuracy = 0.929688\n",
      "[2018-07-16 17:12:51.570526] Iteration 27500, train loss = 0.230357, train accuracy = 0.953125\n",
      "[2018-07-16 17:12:57.715037] Iteration 27600, train loss = 0.229919, train accuracy = 0.953125\n",
      "[2018-07-16 17:13:03.831860] Iteration 27700, train loss = 0.273835, train accuracy = 0.914062\n",
      "[2018-07-16 17:13:09.952782] Iteration 27800, train loss = 0.273246, train accuracy = 0.945312\n",
      "[2018-07-16 17:13:16.046159] Iteration 27900, train loss = 0.244464, train accuracy = 0.945312\n",
      "[2018-07-16 17:13:22.151436] Iteration 28000, train loss = 0.210334, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.852600\n",
      "[2018-07-16 17:13:30.168527] Iteration 28100, train loss = 0.173059, train accuracy = 0.968750\n",
      "[2018-07-16 17:13:36.274378] Iteration 28200, train loss = 0.153276, train accuracy = 0.968750\n",
      "[2018-07-16 17:13:42.412679] Iteration 28300, train loss = 0.169587, train accuracy = 0.968750\n",
      "[2018-07-16 17:13:48.551370] Iteration 28400, train loss = 0.150309, train accuracy = 0.984375\n",
      "[2018-07-16 17:13:54.694931] Iteration 28500, train loss = 0.198197, train accuracy = 0.968750\n",
      "[2018-07-16 17:14:00.822564] Iteration 28600, train loss = 0.214673, train accuracy = 0.953125\n",
      "[2018-07-16 17:14:06.939357] Iteration 28700, train loss = 0.168450, train accuracy = 0.960938\n",
      "[2018-07-16 17:14:13.021882] Iteration 28800, train loss = 0.199537, train accuracy = 0.953125\n",
      "[2018-07-16 17:14:19.146433] Iteration 28900, train loss = 0.223300, train accuracy = 0.945312\n",
      "[2018-07-16 17:14:25.265720] Iteration 29000, train loss = 0.152436, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.829500\n",
      "[2018-07-16 17:14:33.273505] Iteration 29100, train loss = 0.148140, train accuracy = 0.976562\n",
      "[2018-07-16 17:14:39.405557] Iteration 29200, train loss = 0.184033, train accuracy = 0.976562\n",
      "[2018-07-16 17:14:45.527845] Iteration 29300, train loss = 0.264042, train accuracy = 0.921875\n",
      "[2018-07-16 17:14:51.624652] Iteration 29400, train loss = 0.224295, train accuracy = 0.953125\n",
      "[2018-07-16 17:14:57.772942] Iteration 29500, train loss = 0.171328, train accuracy = 0.984375\n",
      "[2018-07-16 17:15:03.849746] Iteration 29600, train loss = 0.209163, train accuracy = 0.945312\n",
      "[2018-07-16 17:15:09.972221] Iteration 29700, train loss = 0.191533, train accuracy = 0.945312\n",
      "[2018-07-16 17:15:16.091803] Iteration 29800, train loss = 0.177102, train accuracy = 0.968750\n",
      "[2018-07-16 17:15:22.239775] Iteration 29900, train loss = 0.199656, train accuracy = 0.968750\n",
      "[2018-07-16 17:15:28.412576] Iteration 30000, train loss = 0.172808, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.859200\n",
      "[2018-07-16 17:15:36.350757] Iteration 30100, train loss = 0.169540, train accuracy = 0.976562\n",
      "[2018-07-16 17:15:42.449256] Iteration 30200, train loss = 0.214335, train accuracy = 0.960938\n",
      "[2018-07-16 17:15:48.548223] Iteration 30300, train loss = 0.190821, train accuracy = 0.960938\n",
      "[2018-07-16 17:15:54.638471] Iteration 30400, train loss = 0.214851, train accuracy = 0.968750\n",
      "[2018-07-16 17:16:00.754732] Iteration 30500, train loss = 0.181956, train accuracy = 0.976562\n",
      "[2018-07-16 17:16:06.842024] Iteration 30600, train loss = 0.213046, train accuracy = 0.960938\n",
      "[2018-07-16 17:16:12.937359] Iteration 30700, train loss = 0.239896, train accuracy = 0.945312\n",
      "[2018-07-16 17:16:19.072732] Iteration 30800, train loss = 0.222661, train accuracy = 0.945312\n",
      "[2018-07-16 17:16:25.163214] Iteration 30900, train loss = 0.222591, train accuracy = 0.937500\n",
      "[2018-07-16 17:16:31.307084] Iteration 31000, train loss = 0.201254, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.876400\n",
      "[2018-07-16 17:16:39.311433] Iteration 31100, train loss = 0.193337, train accuracy = 0.960938\n",
      "[2018-07-16 17:16:45.422501] Iteration 31200, train loss = 0.191792, train accuracy = 0.945312\n",
      "[2018-07-16 17:16:51.557926] Iteration 31300, train loss = 0.210118, train accuracy = 0.953125\n",
      "[2018-07-16 17:16:57.681036] Iteration 31400, train loss = 0.137935, train accuracy = 0.984375\n",
      "[2018-07-16 17:17:03.800892] Iteration 31500, train loss = 0.219644, train accuracy = 0.937500\n",
      "[2018-07-16 17:17:09.902879] Iteration 31600, train loss = 0.151268, train accuracy = 0.992188\n",
      "[2018-07-16 17:17:16.051050] Iteration 31700, train loss = 0.210292, train accuracy = 0.960938\n",
      "[2018-07-16 17:17:22.153644] Iteration 31800, train loss = 0.179557, train accuracy = 0.960938\n",
      "[2018-07-16 17:17:28.285720] Iteration 31900, train loss = 0.135848, train accuracy = 0.976562\n",
      "[2018-07-16 17:17:34.423794] Iteration 32000, train loss = 0.220002, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.881700\n",
      "[2018-07-16 17:17:42.366732] Iteration 32100, train loss = 0.176292, train accuracy = 0.976562\n",
      "[2018-07-16 17:17:48.493582] Iteration 32200, train loss = 0.226386, train accuracy = 0.937500\n",
      "[2018-07-16 17:17:54.614301] Iteration 32300, train loss = 0.184354, train accuracy = 0.960938\n",
      "[2018-07-16 17:18:00.756868] Iteration 32400, train loss = 0.168000, train accuracy = 0.976562\n",
      "[2018-07-16 17:18:06.877303] Iteration 32500, train loss = 0.129140, train accuracy = 0.984375\n",
      "[2018-07-16 17:18:12.997826] Iteration 32600, train loss = 0.156875, train accuracy = 0.968750\n",
      "[2018-07-16 17:18:19.076838] Iteration 32700, train loss = 0.235395, train accuracy = 0.953125\n",
      "[2018-07-16 17:18:25.208705] Iteration 32800, train loss = 0.210916, train accuracy = 0.953125\n",
      "[2018-07-16 17:18:31.358035] Iteration 32900, train loss = 0.289213, train accuracy = 0.921875\n",
      "[2018-07-16 17:18:37.396985] Iteration 33000, train loss = 0.239990, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.860400\n",
      "[2018-07-16 17:18:45.410981] Iteration 33100, train loss = 0.178639, train accuracy = 0.960938\n",
      "[2018-07-16 17:18:51.537810] Iteration 33200, train loss = 0.167791, train accuracy = 0.976562\n",
      "[2018-07-16 17:18:57.618995] Iteration 33300, train loss = 0.154809, train accuracy = 0.984375\n",
      "[2018-07-16 17:19:03.730224] Iteration 33400, train loss = 0.183704, train accuracy = 0.960938\n",
      "[2018-07-16 17:19:09.828513] Iteration 33500, train loss = 0.165108, train accuracy = 0.976562\n",
      "[2018-07-16 17:19:15.967673] Iteration 33600, train loss = 0.163691, train accuracy = 0.960938\n",
      "[2018-07-16 17:19:22.120543] Iteration 33700, train loss = 0.214893, train accuracy = 0.937500\n",
      "[2018-07-16 17:19:28.252740] Iteration 33800, train loss = 0.190844, train accuracy = 0.960938\n",
      "[2018-07-16 17:19:34.362997] Iteration 33900, train loss = 0.200155, train accuracy = 0.960938\n",
      "[2018-07-16 17:19:40.436027] Iteration 34000, train loss = 0.154151, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.874800\n",
      "[2018-07-16 17:19:48.427353] Iteration 34100, train loss = 0.197475, train accuracy = 0.968750\n",
      "[2018-07-16 17:19:54.602007] Iteration 34200, train loss = 0.148133, train accuracy = 0.976562\n",
      "[2018-07-16 17:20:00.731009] Iteration 34300, train loss = 0.132568, train accuracy = 0.984375\n",
      "[2018-07-16 17:20:06.812566] Iteration 34400, train loss = 0.168785, train accuracy = 0.968750\n",
      "[2018-07-16 17:20:12.885858] Iteration 34500, train loss = 0.214502, train accuracy = 0.953125\n",
      "[2018-07-16 17:20:18.989893] Iteration 34600, train loss = 0.214958, train accuracy = 0.960938\n",
      "[2018-07-16 17:20:25.060352] Iteration 34700, train loss = 0.235815, train accuracy = 0.937500\n",
      "[2018-07-16 17:20:30.990519] Iteration 34800, train loss = 0.162536, train accuracy = 0.968750\n",
      "[2018-07-16 17:20:36.947215] Iteration 34900, train loss = 0.187641, train accuracy = 0.960938\n",
      "[2018-07-16 17:20:42.927408] Iteration 35000, train loss = 0.177694, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.860200\n",
      "[2018-07-16 17:20:50.729678] Iteration 35100, train loss = 0.135975, train accuracy = 0.984375\n",
      "[2018-07-16 17:20:56.700006] Iteration 35200, train loss = 0.267758, train accuracy = 0.921875\n",
      "[2018-07-16 17:21:02.647339] Iteration 35300, train loss = 0.135921, train accuracy = 0.992188\n",
      "[2018-07-16 17:21:08.632672] Iteration 35400, train loss = 0.145064, train accuracy = 0.984375\n",
      "[2018-07-16 17:21:14.559159] Iteration 35500, train loss = 0.158924, train accuracy = 0.976562\n",
      "[2018-07-16 17:21:20.558772] Iteration 35600, train loss = 0.207669, train accuracy = 0.960938\n",
      "[2018-07-16 17:21:26.583108] Iteration 35700, train loss = 0.173089, train accuracy = 0.960938\n",
      "[2018-07-16 17:21:32.551731] Iteration 35800, train loss = 0.128755, train accuracy = 0.984375\n",
      "[2018-07-16 17:21:38.552651] Iteration 35900, train loss = 0.197255, train accuracy = 0.953125\n",
      "[2018-07-16 17:21:44.533003] Iteration 36000, train loss = 0.201531, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.830500\n",
      "[2018-07-16 17:21:52.377112] Iteration 36100, train loss = 0.181253, train accuracy = 0.960938\n",
      "[2018-07-16 17:21:58.354001] Iteration 36200, train loss = 0.197598, train accuracy = 0.953125\n",
      "[2018-07-16 17:22:04.374996] Iteration 36300, train loss = 0.151677, train accuracy = 0.976562\n",
      "[2018-07-16 17:22:10.439802] Iteration 36400, train loss = 0.161186, train accuracy = 0.968750\n",
      "[2018-07-16 17:22:16.471397] Iteration 36500, train loss = 0.193552, train accuracy = 0.968750\n",
      "[2018-07-16 17:22:22.481590] Iteration 36600, train loss = 0.175592, train accuracy = 0.960938\n",
      "[2018-07-16 17:22:28.400355] Iteration 36700, train loss = 0.185717, train accuracy = 0.960938\n",
      "[2018-07-16 17:22:34.474524] Iteration 36800, train loss = 0.186476, train accuracy = 0.968750\n",
      "[2018-07-16 17:22:40.528494] Iteration 36900, train loss = 0.129877, train accuracy = 0.984375\n",
      "[2018-07-16 17:22:46.629030] Iteration 37000, train loss = 0.151695, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.862900\n",
      "[2018-07-16 17:22:54.629748] Iteration 37100, train loss = 0.187657, train accuracy = 0.953125\n",
      "[2018-07-16 17:23:00.608204] Iteration 37200, train loss = 0.126456, train accuracy = 0.992188\n",
      "[2018-07-16 17:23:06.729319] Iteration 37300, train loss = 0.190581, train accuracy = 0.968750\n",
      "[2018-07-16 17:23:12.832351] Iteration 37400, train loss = 0.265465, train accuracy = 0.937500\n",
      "[2018-07-16 17:23:18.896507] Iteration 37500, train loss = 0.208144, train accuracy = 0.953125\n",
      "[2018-07-16 17:23:25.025413] Iteration 37600, train loss = 0.139610, train accuracy = 0.984375\n",
      "[2018-07-16 17:23:31.102398] Iteration 37700, train loss = 0.183304, train accuracy = 0.945312\n",
      "[2018-07-16 17:23:37.217898] Iteration 37800, train loss = 0.203951, train accuracy = 0.960938\n",
      "[2018-07-16 17:23:43.273811] Iteration 37900, train loss = 0.198626, train accuracy = 0.953125\n",
      "[2018-07-16 17:23:49.340632] Iteration 38000, train loss = 0.194691, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.883900\n",
      "[2018-07-16 17:23:57.303809] Iteration 38100, train loss = 0.170651, train accuracy = 0.960938\n",
      "[2018-07-16 17:24:03.420455] Iteration 38200, train loss = 0.154339, train accuracy = 0.976562\n",
      "[2018-07-16 17:24:09.553354] Iteration 38300, train loss = 0.111143, train accuracy = 0.984375\n",
      "[2018-07-16 17:24:15.632823] Iteration 38400, train loss = 0.189413, train accuracy = 0.960938\n",
      "[2018-07-16 17:24:21.717990] Iteration 38500, train loss = 0.188009, train accuracy = 0.960938\n",
      "[2018-07-16 17:24:27.864890] Iteration 38600, train loss = 0.197579, train accuracy = 0.929688\n",
      "[2018-07-16 17:24:33.951880] Iteration 38700, train loss = 0.184159, train accuracy = 0.976562\n",
      "[2018-07-16 17:24:40.067015] Iteration 38800, train loss = 0.184089, train accuracy = 0.960938\n",
      "[2018-07-16 17:24:46.190513] Iteration 38900, train loss = 0.241545, train accuracy = 0.929688\n",
      "[2018-07-16 17:24:52.289417] Iteration 39000, train loss = 0.210884, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.870700\n",
      "[2018-07-16 17:25:00.278569] Iteration 39100, train loss = 0.187211, train accuracy = 0.960938\n",
      "[2018-07-16 17:25:06.435681] Iteration 39200, train loss = 0.210456, train accuracy = 0.945312\n",
      "[2018-07-16 17:25:12.531329] Iteration 39300, train loss = 0.175458, train accuracy = 0.968750\n",
      "[2018-07-16 17:25:18.623204] Iteration 39400, train loss = 0.236820, train accuracy = 0.953125\n",
      "[2018-07-16 17:25:24.769305] Iteration 39500, train loss = 0.172562, train accuracy = 0.968750\n",
      "[2018-07-16 17:25:30.919164] Iteration 39600, train loss = 0.161095, train accuracy = 0.968750\n",
      "[2018-07-16 17:25:37.066133] Iteration 39700, train loss = 0.238338, train accuracy = 0.960938\n",
      "[2018-07-16 17:25:43.212826] Iteration 39800, train loss = 0.155061, train accuracy = 0.976562\n",
      "[2018-07-16 17:25:49.374334] Iteration 39900, train loss = 0.167030, train accuracy = 0.968750\n"
     ]
    }
   ],
   "source": [
    "curr_lr = 0\n",
    "for step in range(40000):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二轮 量化\n",
    "prune_rate = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.859900\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.08658446 -0.0625      0.07791188  0.125      -0.01751886  0.\n",
      "  0.05940842  0.         -0.11898945 -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.5时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.75)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2018-07-16 17:26:54.821419] Iteration 100, train loss = 1.492028, train accuracy = 0.531250\n",
      "[2018-07-16 17:27:00.700498] Iteration 200, train loss = 1.448729, train accuracy = 0.468750\n",
      "[2018-07-16 17:27:06.600673] Iteration 300, train loss = 1.259122, train accuracy = 0.609375\n",
      "[2018-07-16 17:27:12.399976] Iteration 400, train loss = 0.942946, train accuracy = 0.726562\n",
      "[2018-07-16 17:27:18.207807] Iteration 500, train loss = 0.855860, train accuracy = 0.718750\n",
      "[2018-07-16 17:27:24.045334] Iteration 600, train loss = 0.902651, train accuracy = 0.695312\n",
      "[2018-07-16 17:27:29.877048] Iteration 700, train loss = 0.877396, train accuracy = 0.671875\n",
      "[2018-07-16 17:27:35.851180] Iteration 800, train loss = 0.702269, train accuracy = 0.765625\n",
      "[2018-07-16 17:27:41.811352] Iteration 900, train loss = 0.880765, train accuracy = 0.710938\n",
      "[2018-07-16 17:27:47.769359] Iteration 1000, train loss = 0.974040, train accuracy = 0.695312\n",
      "Evaluating...\n",
      "Test accuracy = 0.709000\n",
      "[2018-07-16 17:27:55.747391] Iteration 1100, train loss = 0.771052, train accuracy = 0.734375\n",
      "[2018-07-16 17:28:01.711449] Iteration 1200, train loss = 0.640366, train accuracy = 0.804688\n",
      "[2018-07-16 17:28:07.745481] Iteration 1300, train loss = 0.752482, train accuracy = 0.750000\n",
      "[2018-07-16 17:28:13.777232] Iteration 1400, train loss = 0.598738, train accuracy = 0.796875\n",
      "[2018-07-16 17:28:19.839364] Iteration 1500, train loss = 0.664573, train accuracy = 0.773438\n",
      "[2018-07-16 17:28:25.924232] Iteration 1600, train loss = 0.661657, train accuracy = 0.789062\n",
      "[2018-07-16 17:28:32.028102] Iteration 1700, train loss = 0.639214, train accuracy = 0.781250\n",
      "[2018-07-16 17:28:38.106844] Iteration 1800, train loss = 0.734831, train accuracy = 0.750000\n",
      "[2018-07-16 17:28:44.244602] Iteration 1900, train loss = 0.609617, train accuracy = 0.820312\n",
      "[2018-07-16 17:28:50.347715] Iteration 2000, train loss = 0.662112, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.653000\n",
      "[2018-07-16 17:28:58.387600] Iteration 2100, train loss = 0.684407, train accuracy = 0.820312\n",
      "[2018-07-16 17:29:04.548560] Iteration 2200, train loss = 0.659572, train accuracy = 0.812500\n",
      "[2018-07-16 17:29:10.724587] Iteration 2300, train loss = 0.598821, train accuracy = 0.835938\n",
      "[2018-07-16 17:29:16.906258] Iteration 2400, train loss = 0.479920, train accuracy = 0.843750\n",
      "[2018-07-16 17:29:23.075221] Iteration 2500, train loss = 0.608681, train accuracy = 0.828125\n",
      "[2018-07-16 17:29:29.203264] Iteration 2600, train loss = 0.706778, train accuracy = 0.773438\n",
      "[2018-07-16 17:29:35.381642] Iteration 2700, train loss = 0.578257, train accuracy = 0.843750\n",
      "[2018-07-16 17:29:41.552835] Iteration 2800, train loss = 0.561860, train accuracy = 0.835938\n",
      "[2018-07-16 17:29:47.722020] Iteration 2900, train loss = 0.646717, train accuracy = 0.796875\n",
      "[2018-07-16 17:29:53.831438] Iteration 3000, train loss = 0.619325, train accuracy = 0.820312\n",
      "Evaluating...\n",
      "Test accuracy = 0.709400\n",
      "[2018-07-16 17:30:01.883885] Iteration 3100, train loss = 0.460282, train accuracy = 0.851562\n",
      "[2018-07-16 17:30:07.990850] Iteration 3200, train loss = 0.529867, train accuracy = 0.859375\n",
      "[2018-07-16 17:30:14.180919] Iteration 3300, train loss = 0.514331, train accuracy = 0.882812\n",
      "[2018-07-16 17:30:20.337244] Iteration 3400, train loss = 0.413853, train accuracy = 0.890625\n",
      "[2018-07-16 17:30:26.508732] Iteration 3500, train loss = 0.606515, train accuracy = 0.820312\n",
      "[2018-07-16 17:30:32.704609] Iteration 3600, train loss = 0.652418, train accuracy = 0.796875\n",
      "[2018-07-16 17:30:38.870861] Iteration 3700, train loss = 0.465289, train accuracy = 0.859375\n",
      "[2018-07-16 17:30:45.068931] Iteration 3800, train loss = 0.548980, train accuracy = 0.804688\n",
      "[2018-07-16 17:30:51.240725] Iteration 3900, train loss = 0.468726, train accuracy = 0.875000\n",
      "[2018-07-16 17:30:57.429072] Iteration 4000, train loss = 0.412263, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.753600\n",
      "[2018-07-16 17:31:05.434169] Iteration 4100, train loss = 0.555632, train accuracy = 0.835938\n",
      "[2018-07-16 17:31:11.578386] Iteration 4200, train loss = 0.483840, train accuracy = 0.851562\n",
      "[2018-07-16 17:31:17.763054] Iteration 4300, train loss = 0.426366, train accuracy = 0.851562\n",
      "[2018-07-16 17:31:23.934774] Iteration 4400, train loss = 0.510767, train accuracy = 0.851562\n",
      "[2018-07-16 17:31:30.092761] Iteration 4500, train loss = 0.513381, train accuracy = 0.812500\n",
      "[2018-07-16 17:31:36.267844] Iteration 4600, train loss = 0.440108, train accuracy = 0.867188\n",
      "[2018-07-16 17:31:42.398810] Iteration 4700, train loss = 0.530299, train accuracy = 0.851562\n",
      "[2018-07-16 17:31:48.606007] Iteration 4800, train loss = 0.454969, train accuracy = 0.867188\n",
      "[2018-07-16 17:31:54.774906] Iteration 4900, train loss = 0.517249, train accuracy = 0.851562\n",
      "[2018-07-16 17:32:00.974431] Iteration 5000, train loss = 0.505417, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.813000\n",
      "[2018-07-16 17:32:08.988462] Iteration 5100, train loss = 0.601298, train accuracy = 0.773438\n",
      "[2018-07-16 17:32:15.136631] Iteration 5200, train loss = 0.423569, train accuracy = 0.898438\n",
      "[2018-07-16 17:32:21.313041] Iteration 5300, train loss = 0.464924, train accuracy = 0.851562\n",
      "[2018-07-16 17:32:27.505747] Iteration 5400, train loss = 0.395029, train accuracy = 0.875000\n",
      "[2018-07-16 17:32:33.674703] Iteration 5500, train loss = 0.426872, train accuracy = 0.875000\n",
      "[2018-07-16 17:32:39.840703] Iteration 5600, train loss = 0.468820, train accuracy = 0.867188\n",
      "[2018-07-16 17:32:45.996401] Iteration 5700, train loss = 0.411500, train accuracy = 0.867188\n",
      "[2018-07-16 17:32:52.150093] Iteration 5800, train loss = 0.369719, train accuracy = 0.898438\n",
      "[2018-07-16 17:32:58.321341] Iteration 5900, train loss = 0.544378, train accuracy = 0.851562\n",
      "[2018-07-16 17:33:04.479122] Iteration 6000, train loss = 0.405913, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.787100\n",
      "[2018-07-16 17:33:12.419062] Iteration 6100, train loss = 0.385336, train accuracy = 0.890625\n",
      "[2018-07-16 17:33:18.588623] Iteration 6200, train loss = 0.409384, train accuracy = 0.867188\n",
      "[2018-07-16 17:33:24.759267] Iteration 6300, train loss = 0.394154, train accuracy = 0.882812\n",
      "[2018-07-16 17:33:30.880367] Iteration 6400, train loss = 0.312038, train accuracy = 0.914062\n",
      "[2018-07-16 17:33:37.042688] Iteration 6500, train loss = 0.417628, train accuracy = 0.867188\n",
      "[2018-07-16 17:33:43.268305] Iteration 6600, train loss = 0.358605, train accuracy = 0.914062\n",
      "[2018-07-16 17:33:49.463219] Iteration 6700, train loss = 0.320540, train accuracy = 0.890625\n",
      "[2018-07-16 17:33:55.623558] Iteration 6800, train loss = 0.441413, train accuracy = 0.851562\n",
      "[2018-07-16 17:34:01.800964] Iteration 6900, train loss = 0.426965, train accuracy = 0.882812\n",
      "[2018-07-16 17:34:07.986331] Iteration 7000, train loss = 0.419864, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.782600\n",
      "[2018-07-16 17:34:15.992120] Iteration 7100, train loss = 0.497484, train accuracy = 0.843750\n",
      "[2018-07-16 17:34:22.113270] Iteration 7200, train loss = 0.369860, train accuracy = 0.882812\n",
      "[2018-07-16 17:34:28.294296] Iteration 7300, train loss = 0.440699, train accuracy = 0.867188\n",
      "[2018-07-16 17:34:34.456742] Iteration 7400, train loss = 0.475654, train accuracy = 0.843750\n",
      "[2018-07-16 17:34:40.639616] Iteration 7500, train loss = 0.468955, train accuracy = 0.859375\n",
      "[2018-07-16 17:34:46.822351] Iteration 7600, train loss = 0.425258, train accuracy = 0.890625\n",
      "[2018-07-16 17:34:52.954318] Iteration 7700, train loss = 0.344574, train accuracy = 0.906250\n",
      "[2018-07-16 17:34:59.132778] Iteration 7800, train loss = 0.430332, train accuracy = 0.859375\n",
      "[2018-07-16 17:35:05.303079] Iteration 7900, train loss = 0.449455, train accuracy = 0.875000\n",
      "[2018-07-16 17:35:11.438914] Iteration 8000, train loss = 0.402298, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.813300\n",
      "[2018-07-16 17:35:19.503590] Iteration 8100, train loss = 0.269174, train accuracy = 0.945312\n",
      "[2018-07-16 17:35:25.685909] Iteration 8200, train loss = 0.390958, train accuracy = 0.882812\n",
      "[2018-07-16 17:35:31.866389] Iteration 8300, train loss = 0.349227, train accuracy = 0.906250\n",
      "[2018-07-16 17:35:38.038534] Iteration 8400, train loss = 0.376404, train accuracy = 0.914062\n",
      "[2018-07-16 17:35:44.180393] Iteration 8500, train loss = 0.595895, train accuracy = 0.812500\n",
      "[2018-07-16 17:35:50.328482] Iteration 8600, train loss = 0.415775, train accuracy = 0.882812\n",
      "[2018-07-16 17:35:56.468682] Iteration 8700, train loss = 0.439951, train accuracy = 0.875000\n",
      "[2018-07-16 17:36:02.609445] Iteration 8800, train loss = 0.297382, train accuracy = 0.929688\n",
      "[2018-07-16 17:36:08.763916] Iteration 8900, train loss = 0.388201, train accuracy = 0.875000\n",
      "[2018-07-16 17:36:14.933108] Iteration 9000, train loss = 0.389066, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.823100\n",
      "[2018-07-16 17:36:22.871252] Iteration 9100, train loss = 0.582962, train accuracy = 0.828125\n",
      "[2018-07-16 17:36:29.041079] Iteration 9200, train loss = 0.298510, train accuracy = 0.921875\n",
      "[2018-07-16 17:36:35.221254] Iteration 9300, train loss = 0.432875, train accuracy = 0.875000\n",
      "[2018-07-16 17:36:41.385237] Iteration 9400, train loss = 0.404757, train accuracy = 0.875000\n",
      "[2018-07-16 17:36:47.544971] Iteration 9500, train loss = 0.354994, train accuracy = 0.906250\n",
      "[2018-07-16 17:36:53.701909] Iteration 9600, train loss = 0.371515, train accuracy = 0.890625\n",
      "[2018-07-16 17:36:59.836557] Iteration 9700, train loss = 0.241164, train accuracy = 0.968750\n",
      "[2018-07-16 17:37:05.987030] Iteration 9800, train loss = 0.392726, train accuracy = 0.898438\n",
      "[2018-07-16 17:37:12.148107] Iteration 9900, train loss = 0.291414, train accuracy = 0.937500\n",
      "[2018-07-16 17:37:18.349902] Iteration 10000, train loss = 0.425246, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.830800\n",
      "[2018-07-16 17:37:26.381827] Iteration 10100, train loss = 0.382082, train accuracy = 0.867188\n",
      "[2018-07-16 17:37:32.429438] Iteration 10200, train loss = 0.396790, train accuracy = 0.882812\n",
      "[2018-07-16 17:37:38.565039] Iteration 10300, train loss = 0.384685, train accuracy = 0.898438\n",
      "[2018-07-16 17:37:44.680660] Iteration 10400, train loss = 0.257294, train accuracy = 0.937500\n",
      "[2018-07-16 17:37:50.855282] Iteration 10500, train loss = 0.391750, train accuracy = 0.835938\n",
      "[2018-07-16 17:37:57.059426] Iteration 10600, train loss = 0.420911, train accuracy = 0.867188\n",
      "[2018-07-16 17:38:03.224191] Iteration 10700, train loss = 0.307471, train accuracy = 0.929688\n",
      "[2018-07-16 17:38:09.366404] Iteration 10800, train loss = 0.278578, train accuracy = 0.953125\n",
      "[2018-07-16 17:38:15.493474] Iteration 10900, train loss = 0.398001, train accuracy = 0.898438\n",
      "[2018-07-16 17:38:21.692229] Iteration 11000, train loss = 0.380847, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.815800\n",
      "[2018-07-16 17:38:29.713421] Iteration 11100, train loss = 0.340569, train accuracy = 0.929688\n",
      "[2018-07-16 17:38:35.901292] Iteration 11200, train loss = 0.494698, train accuracy = 0.835938\n",
      "[2018-07-16 17:38:42.022092] Iteration 11300, train loss = 0.304518, train accuracy = 0.914062\n",
      "[2018-07-16 17:38:48.157119] Iteration 11400, train loss = 0.549578, train accuracy = 0.828125\n",
      "[2018-07-16 17:38:54.264428] Iteration 11500, train loss = 0.315478, train accuracy = 0.914062\n",
      "[2018-07-16 17:39:00.446236] Iteration 11600, train loss = 0.257894, train accuracy = 0.937500\n",
      "[2018-07-16 17:39:06.629611] Iteration 11700, train loss = 0.398966, train accuracy = 0.882812\n",
      "[2018-07-16 17:39:12.807103] Iteration 11800, train loss = 0.349889, train accuracy = 0.914062\n",
      "[2018-07-16 17:39:18.957297] Iteration 11900, train loss = 0.307771, train accuracy = 0.898438\n",
      "[2018-07-16 17:39:25.136021] Iteration 12000, train loss = 0.461455, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.831300\n",
      "[2018-07-16 17:39:33.157954] Iteration 12100, train loss = 0.378196, train accuracy = 0.867188\n",
      "[2018-07-16 17:39:39.305836] Iteration 12200, train loss = 0.273135, train accuracy = 0.929688\n",
      "[2018-07-16 17:39:45.479045] Iteration 12300, train loss = 0.358659, train accuracy = 0.859375\n",
      "[2018-07-16 17:39:51.645969] Iteration 12400, train loss = 0.328167, train accuracy = 0.914062\n",
      "[2018-07-16 17:39:57.794619] Iteration 12500, train loss = 0.367974, train accuracy = 0.875000\n",
      "[2018-07-16 17:40:03.952425] Iteration 12600, train loss = 0.298409, train accuracy = 0.937500\n",
      "[2018-07-16 17:40:10.100551] Iteration 12700, train loss = 0.321079, train accuracy = 0.906250\n",
      "[2018-07-16 17:40:16.265723] Iteration 12800, train loss = 0.221618, train accuracy = 0.953125\n",
      "[2018-07-16 17:40:22.381351] Iteration 12900, train loss = 0.325540, train accuracy = 0.921875\n",
      "[2018-07-16 17:40:28.570266] Iteration 13000, train loss = 0.375461, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.819200\n",
      "[2018-07-16 17:40:36.710703] Iteration 13100, train loss = 0.285373, train accuracy = 0.921875\n",
      "[2018-07-16 17:40:42.897667] Iteration 13200, train loss = 0.382582, train accuracy = 0.875000\n",
      "[2018-07-16 17:40:49.026551] Iteration 13300, train loss = 0.283301, train accuracy = 0.914062\n",
      "[2018-07-16 17:40:55.170434] Iteration 13400, train loss = 0.323288, train accuracy = 0.921875\n",
      "[2018-07-16 17:41:01.312340] Iteration 13500, train loss = 0.375056, train accuracy = 0.914062\n",
      "[2018-07-16 17:41:07.447291] Iteration 13600, train loss = 0.279172, train accuracy = 0.937500\n",
      "[2018-07-16 17:41:13.601328] Iteration 13700, train loss = 0.376620, train accuracy = 0.898438\n",
      "[2018-07-16 17:41:19.791161] Iteration 13800, train loss = 0.466455, train accuracy = 0.867188\n",
      "[2018-07-16 17:41:25.930421] Iteration 13900, train loss = 0.329511, train accuracy = 0.929688\n",
      "[2018-07-16 17:41:32.134335] Iteration 14000, train loss = 0.241969, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.822100\n",
      "[2018-07-16 17:41:40.127220] Iteration 14100, train loss = 0.287832, train accuracy = 0.953125\n",
      "[2018-07-16 17:41:46.283736] Iteration 14200, train loss = 0.365946, train accuracy = 0.898438\n",
      "[2018-07-16 17:41:52.449990] Iteration 14300, train loss = 0.298874, train accuracy = 0.914062\n",
      "[2018-07-16 17:41:58.585830] Iteration 14400, train loss = 0.339226, train accuracy = 0.906250\n",
      "[2018-07-16 17:42:04.761065] Iteration 14500, train loss = 0.374056, train accuracy = 0.914062\n",
      "[2018-07-16 17:42:10.869710] Iteration 14600, train loss = 0.255688, train accuracy = 0.945312\n",
      "[2018-07-16 17:42:17.026265] Iteration 14700, train loss = 0.441324, train accuracy = 0.882812\n",
      "[2018-07-16 17:42:23.196124] Iteration 14800, train loss = 0.468436, train accuracy = 0.898438\n",
      "[2018-07-16 17:42:29.333264] Iteration 14900, train loss = 0.372546, train accuracy = 0.882812\n",
      "[2018-07-16 17:42:35.477535] Iteration 15000, train loss = 0.244071, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.846700\n",
      "[2018-07-16 17:42:43.496777] Iteration 15100, train loss = 0.256499, train accuracy = 0.921875\n",
      "[2018-07-16 17:42:49.589355] Iteration 15200, train loss = 0.355114, train accuracy = 0.890625\n",
      "[2018-07-16 17:42:55.753394] Iteration 15300, train loss = 0.269202, train accuracy = 0.890625\n",
      "[2018-07-16 17:43:01.904548] Iteration 15400, train loss = 0.379528, train accuracy = 0.906250\n",
      "[2018-07-16 17:43:08.077582] Iteration 15500, train loss = 0.295618, train accuracy = 0.929688\n",
      "[2018-07-16 17:43:14.237619] Iteration 15600, train loss = 0.348039, train accuracy = 0.914062\n",
      "[2018-07-16 17:43:20.416863] Iteration 15700, train loss = 0.344746, train accuracy = 0.890625\n",
      "[2018-07-16 17:43:26.635623] Iteration 15800, train loss = 0.548841, train accuracy = 0.828125\n",
      "[2018-07-16 17:43:32.783931] Iteration 15900, train loss = 0.256008, train accuracy = 0.921875\n",
      "[2018-07-16 17:43:38.916826] Iteration 16000, train loss = 0.355207, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.838400\n",
      "[2018-07-16 17:43:46.949058] Iteration 16100, train loss = 0.202970, train accuracy = 0.968750\n",
      "[2018-07-16 17:43:53.081167] Iteration 16200, train loss = 0.314102, train accuracy = 0.906250\n",
      "[2018-07-16 17:43:59.250962] Iteration 16300, train loss = 0.442621, train accuracy = 0.867188\n",
      "[2018-07-16 17:44:05.371402] Iteration 16400, train loss = 0.294213, train accuracy = 0.921875\n",
      "[2018-07-16 17:44:11.539493] Iteration 16500, train loss = 0.379450, train accuracy = 0.921875\n",
      "[2018-07-16 17:44:17.696379] Iteration 16600, train loss = 0.348261, train accuracy = 0.906250\n",
      "[2018-07-16 17:44:23.791997] Iteration 16700, train loss = 0.306412, train accuracy = 0.921875\n",
      "[2018-07-16 17:44:29.919521] Iteration 16800, train loss = 0.265737, train accuracy = 0.929688\n",
      "[2018-07-16 17:44:36.121775] Iteration 16900, train loss = 0.469321, train accuracy = 0.843750\n",
      "[2018-07-16 17:44:42.228975] Iteration 17000, train loss = 0.372498, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.837100\n",
      "[2018-07-16 17:44:50.246368] Iteration 17100, train loss = 0.298314, train accuracy = 0.898438\n",
      "[2018-07-16 17:44:56.392414] Iteration 17200, train loss = 0.378361, train accuracy = 0.914062\n",
      "[2018-07-16 17:45:02.551721] Iteration 17300, train loss = 0.332397, train accuracy = 0.921875\n",
      "[2018-07-16 17:45:08.696336] Iteration 17400, train loss = 0.343685, train accuracy = 0.914062\n",
      "[2018-07-16 17:45:14.825445] Iteration 17500, train loss = 0.324459, train accuracy = 0.906250\n",
      "[2018-07-16 17:45:21.013451] Iteration 17600, train loss = 0.409117, train accuracy = 0.867188\n",
      "[2018-07-16 17:45:27.130921] Iteration 17700, train loss = 0.290211, train accuracy = 0.921875\n",
      "[2018-07-16 17:45:33.287043] Iteration 17800, train loss = 0.373770, train accuracy = 0.890625\n",
      "[2018-07-16 17:45:39.302132] Iteration 17900, train loss = 0.251194, train accuracy = 0.953125\n",
      "[2018-07-16 17:45:45.455884] Iteration 18000, train loss = 0.399120, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.830400\n",
      "[2018-07-16 17:45:53.436297] Iteration 18100, train loss = 0.248513, train accuracy = 0.945312\n",
      "[2018-07-16 17:45:59.598727] Iteration 18200, train loss = 0.324180, train accuracy = 0.890625\n",
      "[2018-07-16 17:46:05.742199] Iteration 18300, train loss = 0.287448, train accuracy = 0.937500\n",
      "[2018-07-16 17:46:11.863127] Iteration 18400, train loss = 0.227223, train accuracy = 0.945312\n",
      "[2018-07-16 17:46:18.042809] Iteration 18500, train loss = 0.228679, train accuracy = 0.945312\n",
      "[2018-07-16 17:46:24.197967] Iteration 18600, train loss = 0.356343, train accuracy = 0.882812\n",
      "[2018-07-16 17:46:30.368711] Iteration 18700, train loss = 0.301404, train accuracy = 0.921875\n",
      "[2018-07-16 17:46:36.508314] Iteration 18800, train loss = 0.318806, train accuracy = 0.906250\n",
      "[2018-07-16 17:46:42.651567] Iteration 18900, train loss = 0.316841, train accuracy = 0.929688\n",
      "[2018-07-16 17:46:48.805198] Iteration 19000, train loss = 0.303302, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.829500\n",
      "[2018-07-16 17:46:56.813236] Iteration 19100, train loss = 0.362944, train accuracy = 0.882812\n",
      "[2018-07-16 17:47:02.930247] Iteration 19200, train loss = 0.386541, train accuracy = 0.898438\n",
      "[2018-07-16 17:47:09.083808] Iteration 19300, train loss = 0.351427, train accuracy = 0.890625\n",
      "[2018-07-16 17:47:15.221627] Iteration 19400, train loss = 0.303863, train accuracy = 0.890625\n",
      "[2018-07-16 17:47:21.396164] Iteration 19500, train loss = 0.254991, train accuracy = 0.937500\n",
      "[2018-07-16 17:47:27.571347] Iteration 19600, train loss = 0.428953, train accuracy = 0.890625\n",
      "[2018-07-16 17:47:33.730821] Iteration 19700, train loss = 0.264127, train accuracy = 0.953125\n",
      "[2018-07-16 17:47:39.871622] Iteration 19800, train loss = 0.304116, train accuracy = 0.914062\n",
      "[2018-07-16 17:47:46.007720] Iteration 19900, train loss = 0.291825, train accuracy = 0.929688\n",
      "[2018-07-16 17:47:52.112075] Iteration 20000, train loss = 0.321195, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.808300\n",
      "[2018-07-16 17:48:00.149266] Iteration 20100, train loss = 0.404258, train accuracy = 0.882812\n",
      "[2018-07-16 17:48:06.282865] Iteration 20200, train loss = 0.309028, train accuracy = 0.921875\n",
      "[2018-07-16 17:48:12.385059] Iteration 20300, train loss = 0.359047, train accuracy = 0.875000\n",
      "[2018-07-16 17:48:18.593972] Iteration 20400, train loss = 0.326239, train accuracy = 0.898438\n",
      "[2018-07-16 17:48:24.710520] Iteration 20500, train loss = 0.311003, train accuracy = 0.914062\n",
      "[2018-07-16 17:48:30.860797] Iteration 20600, train loss = 0.341805, train accuracy = 0.921875\n",
      "[2018-07-16 17:48:37.005327] Iteration 20700, train loss = 0.386956, train accuracy = 0.890625\n",
      "[2018-07-16 17:48:43.144453] Iteration 20800, train loss = 0.243350, train accuracy = 0.945312\n",
      "[2018-07-16 17:48:49.271488] Iteration 20900, train loss = 0.274162, train accuracy = 0.945312\n",
      "[2018-07-16 17:48:55.362708] Iteration 21000, train loss = 0.303972, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.829000\n",
      "[2018-07-16 17:49:03.370176] Iteration 21100, train loss = 0.280548, train accuracy = 0.945312\n",
      "[2018-07-16 17:49:09.505960] Iteration 21200, train loss = 0.294514, train accuracy = 0.921875\n",
      "[2018-07-16 17:49:15.638352] Iteration 21300, train loss = 0.440355, train accuracy = 0.875000\n",
      "[2018-07-16 17:49:21.776671] Iteration 21400, train loss = 0.306736, train accuracy = 0.921875\n",
      "[2018-07-16 17:49:27.910228] Iteration 21500, train loss = 0.303401, train accuracy = 0.953125\n",
      "[2018-07-16 17:49:34.086016] Iteration 21600, train loss = 0.337702, train accuracy = 0.937500\n",
      "[2018-07-16 17:49:40.165126] Iteration 21700, train loss = 0.176353, train accuracy = 0.960938\n",
      "[2018-07-16 17:49:46.253631] Iteration 21800, train loss = 0.322449, train accuracy = 0.914062\n",
      "[2018-07-16 17:49:52.434354] Iteration 21900, train loss = 0.268240, train accuracy = 0.898438\n",
      "[2018-07-16 17:49:58.608256] Iteration 22000, train loss = 0.318332, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.806800\n",
      "[2018-07-16 17:50:06.614262] Iteration 22100, train loss = 0.379679, train accuracy = 0.898438\n",
      "[2018-07-16 17:50:12.758516] Iteration 22200, train loss = 0.289859, train accuracy = 0.906250\n",
      "[2018-07-16 17:50:18.860672] Iteration 22300, train loss = 0.376593, train accuracy = 0.875000\n",
      "[2018-07-16 17:50:25.006192] Iteration 22400, train loss = 0.305445, train accuracy = 0.921875\n",
      "[2018-07-16 17:50:31.143601] Iteration 22500, train loss = 0.369632, train accuracy = 0.914062\n",
      "[2018-07-16 17:50:37.215797] Iteration 22600, train loss = 0.382174, train accuracy = 0.859375\n",
      "[2018-07-16 17:50:43.332474] Iteration 22700, train loss = 0.249058, train accuracy = 0.929688\n",
      "[2018-07-16 17:50:49.509473] Iteration 22800, train loss = 0.272148, train accuracy = 0.921875\n",
      "[2018-07-16 17:50:55.662551] Iteration 22900, train loss = 0.275954, train accuracy = 0.929688\n",
      "[2018-07-16 17:51:01.829771] Iteration 23000, train loss = 0.290173, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.853500\n",
      "[2018-07-16 17:51:09.784820] Iteration 23100, train loss = 0.319409, train accuracy = 0.906250\n",
      "[2018-07-16 17:51:15.911439] Iteration 23200, train loss = 0.263337, train accuracy = 0.937500\n",
      "[2018-07-16 17:51:22.069124] Iteration 23300, train loss = 0.229046, train accuracy = 0.953125\n",
      "[2018-07-16 17:51:28.202687] Iteration 23400, train loss = 0.332651, train accuracy = 0.929688\n",
      "[2018-07-16 17:51:34.330675] Iteration 23500, train loss = 0.371658, train accuracy = 0.898438\n",
      "[2018-07-16 17:51:40.458884] Iteration 23600, train loss = 0.334180, train accuracy = 0.914062\n",
      "[2018-07-16 17:51:46.593196] Iteration 23700, train loss = 0.309826, train accuracy = 0.921875\n",
      "[2018-07-16 17:51:52.722227] Iteration 23800, train loss = 0.186861, train accuracy = 0.960938\n",
      "[2018-07-16 17:51:58.849085] Iteration 23900, train loss = 0.236467, train accuracy = 0.929688\n",
      "[2018-07-16 17:52:04.997431] Iteration 24000, train loss = 0.331054, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.853900\n",
      "[2018-07-16 17:52:12.969939] Iteration 24100, train loss = 0.288444, train accuracy = 0.914062\n",
      "[2018-07-16 17:52:19.117721] Iteration 24200, train loss = 0.219998, train accuracy = 0.945312\n",
      "[2018-07-16 17:52:25.272182] Iteration 24300, train loss = 0.265229, train accuracy = 0.929688\n",
      "[2018-07-16 17:52:31.402900] Iteration 24400, train loss = 0.252283, train accuracy = 0.945312\n",
      "[2018-07-16 17:52:37.507201] Iteration 24500, train loss = 0.341176, train accuracy = 0.898438\n",
      "[2018-07-16 17:52:43.653721] Iteration 24600, train loss = 0.354140, train accuracy = 0.859375\n",
      "[2018-07-16 17:52:49.821556] Iteration 24700, train loss = 0.225561, train accuracy = 0.937500\n",
      "[2018-07-16 17:52:55.974268] Iteration 24800, train loss = 0.174904, train accuracy = 0.960938\n",
      "[2018-07-16 17:53:02.139612] Iteration 24900, train loss = 0.317079, train accuracy = 0.921875\n",
      "[2018-07-16 17:53:08.274845] Iteration 25000, train loss = 0.245749, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.860700\n",
      "[2018-07-16 17:53:16.310361] Iteration 25100, train loss = 0.254003, train accuracy = 0.914062\n",
      "[2018-07-16 17:53:22.444348] Iteration 25200, train loss = 0.275951, train accuracy = 0.921875\n",
      "[2018-07-16 17:53:28.577243] Iteration 25300, train loss = 0.344487, train accuracy = 0.898438\n",
      "[2018-07-16 17:53:34.713768] Iteration 25400, train loss = 0.300364, train accuracy = 0.906250\n",
      "[2018-07-16 17:53:40.845389] Iteration 25500, train loss = 0.306182, train accuracy = 0.914062\n",
      "[2018-07-16 17:53:46.985886] Iteration 25600, train loss = 0.276167, train accuracy = 0.937500\n",
      "[2018-07-16 17:53:53.083123] Iteration 25700, train loss = 0.317457, train accuracy = 0.898438\n",
      "[2018-07-16 17:53:59.177903] Iteration 25800, train loss = 0.334792, train accuracy = 0.906250\n",
      "[2018-07-16 17:54:05.349555] Iteration 25900, train loss = 0.254170, train accuracy = 0.937500\n",
      "[2018-07-16 17:54:11.511284] Iteration 26000, train loss = 0.292414, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.824600\n",
      "[2018-07-16 17:54:19.508363] Iteration 26100, train loss = 0.263900, train accuracy = 0.960938\n",
      "[2018-07-16 17:54:25.661200] Iteration 26200, train loss = 0.222951, train accuracy = 0.945312\n",
      "[2018-07-16 17:54:31.731252] Iteration 26300, train loss = 0.326199, train accuracy = 0.914062\n",
      "[2018-07-16 17:54:37.896389] Iteration 26400, train loss = 0.281872, train accuracy = 0.937500\n",
      "[2018-07-16 17:54:44.046298] Iteration 26500, train loss = 0.321882, train accuracy = 0.929688\n",
      "[2018-07-16 17:54:50.229775] Iteration 26600, train loss = 0.250211, train accuracy = 0.945312\n",
      "[2018-07-16 17:54:56.379955] Iteration 26700, train loss = 0.226235, train accuracy = 0.937500\n",
      "[2018-07-16 17:55:02.576458] Iteration 26800, train loss = 0.262561, train accuracy = 0.921875\n",
      "[2018-07-16 17:55:08.755618] Iteration 26900, train loss = 0.238829, train accuracy = 0.953125\n",
      "[2018-07-16 17:55:14.920655] Iteration 27000, train loss = 0.201064, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.832500\n",
      "[2018-07-16 17:55:22.900151] Iteration 27100, train loss = 0.271928, train accuracy = 0.960938\n",
      "[2018-07-16 17:55:29.024460] Iteration 27200, train loss = 0.218267, train accuracy = 0.960938\n",
      "[2018-07-16 17:55:35.198148] Iteration 27300, train loss = 0.258294, train accuracy = 0.929688\n",
      "[2018-07-16 17:55:41.344308] Iteration 27400, train loss = 0.329457, train accuracy = 0.937500\n",
      "[2018-07-16 17:55:47.496770] Iteration 27500, train loss = 0.288164, train accuracy = 0.929688\n",
      "[2018-07-16 17:55:53.661428] Iteration 27600, train loss = 0.246300, train accuracy = 0.953125\n",
      "[2018-07-16 17:55:59.865440] Iteration 27700, train loss = 0.216219, train accuracy = 0.945312\n",
      "[2018-07-16 17:56:06.010919] Iteration 27800, train loss = 0.350524, train accuracy = 0.914062\n",
      "[2018-07-16 17:56:12.168094] Iteration 27900, train loss = 0.295446, train accuracy = 0.906250\n",
      "[2018-07-16 17:56:18.335369] Iteration 28000, train loss = 0.304940, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.862800\n",
      "[2018-07-16 17:56:26.378597] Iteration 28100, train loss = 0.205749, train accuracy = 0.929688\n",
      "[2018-07-16 17:56:32.535792] Iteration 28200, train loss = 0.448949, train accuracy = 0.882812\n",
      "[2018-07-16 17:56:38.599405] Iteration 28300, train loss = 0.256385, train accuracy = 0.921875\n",
      "[2018-07-16 17:56:44.755412] Iteration 28400, train loss = 0.273125, train accuracy = 0.929688\n",
      "[2018-07-16 17:56:50.860517] Iteration 28500, train loss = 0.281509, train accuracy = 0.945312\n",
      "[2018-07-16 17:56:57.061206] Iteration 28600, train loss = 0.279016, train accuracy = 0.914062\n",
      "[2018-07-16 17:57:03.260640] Iteration 28700, train loss = 0.300968, train accuracy = 0.921875\n",
      "[2018-07-16 17:57:09.430375] Iteration 28800, train loss = 0.246410, train accuracy = 0.929688\n",
      "[2018-07-16 17:57:15.621649] Iteration 28900, train loss = 0.271242, train accuracy = 0.953125\n",
      "[2018-07-16 17:57:21.799299] Iteration 29000, train loss = 0.233305, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.819300\n",
      "[2018-07-16 17:57:29.851336] Iteration 29100, train loss = 0.338083, train accuracy = 0.890625\n",
      "[2018-07-16 17:57:35.993622] Iteration 29200, train loss = 0.265408, train accuracy = 0.929688\n",
      "[2018-07-16 17:57:42.174885] Iteration 29300, train loss = 0.268838, train accuracy = 0.937500\n",
      "[2018-07-16 17:57:48.359935] Iteration 29400, train loss = 0.286441, train accuracy = 0.914062\n",
      "[2018-07-16 17:57:54.559914] Iteration 29500, train loss = 0.264870, train accuracy = 0.921875\n",
      "[2018-07-16 17:58:00.663945] Iteration 29600, train loss = 0.219162, train accuracy = 0.960938\n",
      "[2018-07-16 17:58:06.791840] Iteration 29700, train loss = 0.303601, train accuracy = 0.914062\n",
      "[2018-07-16 17:58:12.978324] Iteration 29800, train loss = 0.248052, train accuracy = 0.945312\n",
      "[2018-07-16 17:58:19.146008] Iteration 29900, train loss = 0.223615, train accuracy = 0.953125\n",
      "[2018-07-16 17:58:25.323053] Iteration 30000, train loss = 0.272098, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.870300\n",
      "[2018-07-16 17:58:33.346169] Iteration 30100, train loss = 0.340157, train accuracy = 0.921875\n",
      "[2018-07-16 17:58:39.509884] Iteration 30200, train loss = 0.402632, train accuracy = 0.867188\n",
      "[2018-07-16 17:58:45.673480] Iteration 30300, train loss = 0.291140, train accuracy = 0.914062\n",
      "[2018-07-16 17:58:51.881866] Iteration 30400, train loss = 0.312388, train accuracy = 0.914062\n",
      "[2018-07-16 17:58:58.080963] Iteration 30500, train loss = 0.235995, train accuracy = 0.945312\n",
      "[2018-07-16 17:59:04.283551] Iteration 30600, train loss = 0.300648, train accuracy = 0.929688\n",
      "[2018-07-16 17:59:10.453239] Iteration 30700, train loss = 0.249599, train accuracy = 0.937500\n",
      "[2018-07-16 17:59:16.601170] Iteration 30800, train loss = 0.221276, train accuracy = 0.953125\n",
      "[2018-07-16 17:59:22.791370] Iteration 30900, train loss = 0.334020, train accuracy = 0.914062\n",
      "[2018-07-16 17:59:28.976230] Iteration 31000, train loss = 0.330739, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.863500\n",
      "[2018-07-16 17:59:36.944311] Iteration 31100, train loss = 0.283438, train accuracy = 0.921875\n",
      "[2018-07-16 17:59:43.100414] Iteration 31200, train loss = 0.255686, train accuracy = 0.921875\n",
      "[2018-07-16 17:59:49.266874] Iteration 31300, train loss = 0.278785, train accuracy = 0.937500\n",
      "[2018-07-16 17:59:55.459679] Iteration 31400, train loss = 0.252900, train accuracy = 0.937500\n",
      "[2018-07-16 18:00:01.620437] Iteration 31500, train loss = 0.377255, train accuracy = 0.898438\n",
      "[2018-07-16 18:00:07.823000] Iteration 31600, train loss = 0.262246, train accuracy = 0.945312\n",
      "[2018-07-16 18:00:13.958692] Iteration 31700, train loss = 0.328210, train accuracy = 0.906250\n",
      "[2018-07-16 18:00:20.066668] Iteration 31800, train loss = 0.213291, train accuracy = 0.937500\n",
      "[2018-07-16 18:00:26.223286] Iteration 31900, train loss = 0.229400, train accuracy = 0.937500\n",
      "[2018-07-16 18:00:32.401288] Iteration 32000, train loss = 0.248071, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.863100\n",
      "[2018-07-16 18:00:40.439868] Iteration 32100, train loss = 0.294975, train accuracy = 0.914062\n",
      "[2018-07-16 18:00:46.624050] Iteration 32200, train loss = 0.287170, train accuracy = 0.929688\n",
      "[2018-07-16 18:00:52.841261] Iteration 32300, train loss = 0.197823, train accuracy = 0.976562\n",
      "[2018-07-16 18:00:58.988682] Iteration 32400, train loss = 0.280881, train accuracy = 0.937500\n",
      "[2018-07-16 18:01:05.179487] Iteration 32500, train loss = 0.247293, train accuracy = 0.960938\n",
      "[2018-07-16 18:01:11.357032] Iteration 32600, train loss = 0.348363, train accuracy = 0.898438\n",
      "[2018-07-16 18:01:17.555742] Iteration 32700, train loss = 0.207464, train accuracy = 0.968750\n",
      "[2018-07-16 18:01:23.742895] Iteration 32800, train loss = 0.360657, train accuracy = 0.937500\n",
      "[2018-07-16 18:01:29.933472] Iteration 32900, train loss = 0.241429, train accuracy = 0.937500\n",
      "[2018-07-16 18:01:36.135888] Iteration 33000, train loss = 0.210904, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.842000\n",
      "[2018-07-16 18:01:44.173455] Iteration 33100, train loss = 0.238456, train accuracy = 0.937500\n",
      "[2018-07-16 18:01:50.296846] Iteration 33200, train loss = 0.270942, train accuracy = 0.937500\n",
      "[2018-07-16 18:01:56.408141] Iteration 33300, train loss = 0.229944, train accuracy = 0.937500\n",
      "[2018-07-16 18:02:02.596256] Iteration 33400, train loss = 0.255163, train accuracy = 0.921875\n",
      "[2018-07-16 18:02:08.756247] Iteration 33500, train loss = 0.273082, train accuracy = 0.937500\n",
      "[2018-07-16 18:02:14.887799] Iteration 33600, train loss = 0.304725, train accuracy = 0.914062\n",
      "[2018-07-16 18:02:21.045476] Iteration 33700, train loss = 0.282543, train accuracy = 0.937500\n",
      "[2018-07-16 18:02:27.237804] Iteration 33800, train loss = 0.240858, train accuracy = 0.937500\n",
      "[2018-07-16 18:02:33.449084] Iteration 33900, train loss = 0.264811, train accuracy = 0.937500\n",
      "[2018-07-16 18:02:39.623273] Iteration 34000, train loss = 0.343616, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.838800\n",
      "[2018-07-16 18:02:47.630330] Iteration 34100, train loss = 0.198534, train accuracy = 0.960938\n",
      "[2018-07-16 18:02:53.800652] Iteration 34200, train loss = 0.239918, train accuracy = 0.945312\n",
      "[2018-07-16 18:02:59.989548] Iteration 34300, train loss = 0.332545, train accuracy = 0.929688\n",
      "[2018-07-16 18:03:06.133855] Iteration 34400, train loss = 0.286349, train accuracy = 0.945312\n",
      "[2018-07-16 18:03:12.306653] Iteration 34500, train loss = 0.314804, train accuracy = 0.890625\n",
      "[2018-07-16 18:03:18.491620] Iteration 34600, train loss = 0.315835, train accuracy = 0.914062\n",
      "[2018-07-16 18:03:24.623065] Iteration 34700, train loss = 0.217514, train accuracy = 0.953125\n",
      "[2018-07-16 18:03:30.780150] Iteration 34800, train loss = 0.212779, train accuracy = 0.976562\n",
      "[2018-07-16 18:03:36.981664] Iteration 34900, train loss = 0.212924, train accuracy = 0.968750\n",
      "[2018-07-16 18:03:43.188976] Iteration 35000, train loss = 0.246725, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.858000\n",
      "[2018-07-16 18:03:51.172113] Iteration 35100, train loss = 0.376223, train accuracy = 0.890625\n",
      "[2018-07-16 18:03:57.349402] Iteration 35200, train loss = 0.275984, train accuracy = 0.929688\n",
      "[2018-07-16 18:04:03.533897] Iteration 35300, train loss = 0.281806, train accuracy = 0.914062\n",
      "[2018-07-16 18:04:09.715192] Iteration 35400, train loss = 0.279633, train accuracy = 0.937500\n",
      "[2018-07-16 18:04:15.832389] Iteration 35500, train loss = 0.206079, train accuracy = 0.945312\n",
      "[2018-07-16 18:04:22.020410] Iteration 35600, train loss = 0.326564, train accuracy = 0.906250\n",
      "[2018-07-16 18:04:28.199645] Iteration 35700, train loss = 0.254361, train accuracy = 0.945312\n",
      "[2018-07-16 18:04:34.387358] Iteration 35800, train loss = 0.232967, train accuracy = 0.929688\n",
      "[2018-07-16 18:04:40.594740] Iteration 35900, train loss = 0.264464, train accuracy = 0.945312\n",
      "[2018-07-16 18:04:46.736786] Iteration 36000, train loss = 0.216221, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.862000\n",
      "[2018-07-16 18:04:54.763335] Iteration 36100, train loss = 0.235210, train accuracy = 0.929688\n",
      "[2018-07-16 18:05:00.942926] Iteration 36200, train loss = 0.209361, train accuracy = 0.953125\n",
      "[2018-07-16 18:05:07.089716] Iteration 36300, train loss = 0.246840, train accuracy = 0.945312\n",
      "[2018-07-16 18:05:13.291787] Iteration 36400, train loss = 0.213314, train accuracy = 0.945312\n",
      "[2018-07-16 18:05:19.482620] Iteration 36500, train loss = 0.288446, train accuracy = 0.929688\n",
      "[2018-07-16 18:05:25.684050] Iteration 36600, train loss = 0.235368, train accuracy = 0.945312\n",
      "[2018-07-16 18:05:31.879524] Iteration 36700, train loss = 0.470394, train accuracy = 0.843750\n",
      "[2018-07-16 18:05:38.072008] Iteration 36800, train loss = 0.300630, train accuracy = 0.921875\n",
      "[2018-07-16 18:05:44.243202] Iteration 36900, train loss = 0.324128, train accuracy = 0.906250\n",
      "[2018-07-16 18:05:50.391116] Iteration 37000, train loss = 0.236858, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.849400\n",
      "[2018-07-16 18:05:58.439015] Iteration 37100, train loss = 0.200754, train accuracy = 0.968750\n",
      "[2018-07-16 18:06:04.643263] Iteration 37200, train loss = 0.237817, train accuracy = 0.937500\n",
      "[2018-07-16 18:06:10.810304] Iteration 37300, train loss = 0.232573, train accuracy = 0.953125\n",
      "[2018-07-16 18:06:17.008379] Iteration 37400, train loss = 0.282796, train accuracy = 0.914062\n",
      "[2018-07-16 18:06:23.188767] Iteration 37500, train loss = 0.257532, train accuracy = 0.914062\n",
      "[2018-07-16 18:06:29.359677] Iteration 37600, train loss = 0.216737, train accuracy = 0.953125\n",
      "[2018-07-16 18:06:35.567425] Iteration 37700, train loss = 0.179894, train accuracy = 0.984375\n",
      "[2018-07-16 18:06:41.753005] Iteration 37800, train loss = 0.220665, train accuracy = 0.953125\n",
      "[2018-07-16 18:06:47.953476] Iteration 37900, train loss = 0.173947, train accuracy = 0.960938\n",
      "[2018-07-16 18:06:54.095693] Iteration 38000, train loss = 0.265974, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.842200\n",
      "[2018-07-16 18:07:02.157323] Iteration 38100, train loss = 0.225505, train accuracy = 0.945312\n",
      "[2018-07-16 18:07:08.365740] Iteration 38200, train loss = 0.310278, train accuracy = 0.914062\n",
      "[2018-07-16 18:07:14.575028] Iteration 38300, train loss = 0.275614, train accuracy = 0.945312\n",
      "[2018-07-16 18:07:20.748139] Iteration 38400, train loss = 0.264868, train accuracy = 0.937500\n",
      "[2018-07-16 18:07:26.949816] Iteration 38500, train loss = 0.313693, train accuracy = 0.945312\n",
      "[2018-07-16 18:07:33.124978] Iteration 38600, train loss = 0.290896, train accuracy = 0.929688\n",
      "[2018-07-16 18:07:39.337968] Iteration 38700, train loss = 0.217437, train accuracy = 0.953125\n",
      "[2018-07-16 18:07:45.514431] Iteration 38800, train loss = 0.265550, train accuracy = 0.921875\n",
      "[2018-07-16 18:07:51.714749] Iteration 38900, train loss = 0.194480, train accuracy = 0.968750\n",
      "[2018-07-16 18:07:57.922967] Iteration 39000, train loss = 0.275629, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.852300\n",
      "[2018-07-16 18:08:06.004114] Iteration 39100, train loss = 0.281537, train accuracy = 0.914062\n",
      "[2018-07-16 18:08:12.209534] Iteration 39200, train loss = 0.258680, train accuracy = 0.937500\n",
      "[2018-07-16 18:08:18.416479] Iteration 39300, train loss = 0.378210, train accuracy = 0.906250\n",
      "[2018-07-16 18:08:24.621325] Iteration 39400, train loss = 0.272335, train accuracy = 0.906250\n",
      "[2018-07-16 18:08:30.776512] Iteration 39500, train loss = 0.228530, train accuracy = 0.937500\n",
      "[2018-07-16 18:08:37.015733] Iteration 39600, train loss = 0.236122, train accuracy = 0.960938\n",
      "[2018-07-16 18:08:43.177649] Iteration 39700, train loss = 0.217349, train accuracy = 0.929688\n",
      "[2018-07-16 18:08:49.397403] Iteration 39800, train loss = 0.277630, train accuracy = 0.921875\n",
      "[2018-07-16 18:08:55.492566] Iteration 39900, train loss = 0.172478, train accuracy = 0.960938\n",
      "[2018-07-16 18:09:01.688879] Iteration 40000, train loss = 0.212919, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.838400\n",
      "[2018-07-16 18:09:09.771959] Iteration 40100, train loss = 0.184887, train accuracy = 0.953125\n",
      "[2018-07-16 18:09:15.980940] Iteration 40200, train loss = 0.241491, train accuracy = 0.945312\n",
      "[2018-07-16 18:09:22.178431] Iteration 40300, train loss = 0.265909, train accuracy = 0.945312\n",
      "[2018-07-16 18:09:28.394894] Iteration 40400, train loss = 0.171411, train accuracy = 0.960938\n",
      "[2018-07-16 18:09:34.641421] Iteration 40500, train loss = 0.262327, train accuracy = 0.953125\n",
      "[2018-07-16 18:09:40.854521] Iteration 40600, train loss = 0.302612, train accuracy = 0.921875\n",
      "[2018-07-16 18:09:47.046894] Iteration 40700, train loss = 0.210619, train accuracy = 0.968750\n",
      "[2018-07-16 18:09:53.237897] Iteration 40800, train loss = 0.220396, train accuracy = 0.960938\n",
      "[2018-07-16 18:09:59.446625] Iteration 40900, train loss = 0.198832, train accuracy = 0.960938\n",
      "[2018-07-16 18:10:05.632632] Iteration 41000, train loss = 0.279545, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.842600\n",
      "[2018-07-16 18:10:13.714642] Iteration 41100, train loss = 0.326848, train accuracy = 0.882812\n",
      "[2018-07-16 18:10:19.921544] Iteration 41200, train loss = 0.276275, train accuracy = 0.929688\n",
      "[2018-07-16 18:10:26.117107] Iteration 41300, train loss = 0.261896, train accuracy = 0.937500\n",
      "[2018-07-16 18:10:32.358848] Iteration 41400, train loss = 0.214898, train accuracy = 0.937500\n",
      "[2018-07-16 18:10:38.520254] Iteration 41500, train loss = 0.213585, train accuracy = 0.945312\n",
      "[2018-07-16 18:10:44.719092] Iteration 41600, train loss = 0.208941, train accuracy = 0.937500\n",
      "[2018-07-16 18:10:50.918631] Iteration 41700, train loss = 0.274933, train accuracy = 0.914062\n",
      "[2018-07-16 18:10:57.077741] Iteration 41800, train loss = 0.221858, train accuracy = 0.937500\n",
      "[2018-07-16 18:11:03.296462] Iteration 41900, train loss = 0.186234, train accuracy = 0.960938\n",
      "[2018-07-16 18:11:09.515137] Iteration 42000, train loss = 0.277426, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.872000\n",
      "[2018-07-16 18:11:17.576991] Iteration 42100, train loss = 0.169681, train accuracy = 0.976562\n",
      "[2018-07-16 18:11:23.772900] Iteration 42200, train loss = 0.245445, train accuracy = 0.945312\n",
      "[2018-07-16 18:11:29.997180] Iteration 42300, train loss = 0.193777, train accuracy = 0.968750\n",
      "[2018-07-16 18:11:36.178838] Iteration 42400, train loss = 0.248642, train accuracy = 0.953125\n",
      "[2018-07-16 18:11:42.379835] Iteration 42500, train loss = 0.361491, train accuracy = 0.945312\n",
      "[2018-07-16 18:11:48.519276] Iteration 42600, train loss = 0.169937, train accuracy = 0.976562\n",
      "[2018-07-16 18:11:54.718749] Iteration 42700, train loss = 0.250948, train accuracy = 0.921875\n",
      "[2018-07-16 18:12:00.920798] Iteration 42800, train loss = 0.336852, train accuracy = 0.890625\n",
      "[2018-07-16 18:12:07.124612] Iteration 42900, train loss = 0.248964, train accuracy = 0.945312\n",
      "[2018-07-16 18:12:13.309860] Iteration 43000, train loss = 0.241432, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.848800\n",
      "[2018-07-16 18:12:21.387407] Iteration 43100, train loss = 0.171771, train accuracy = 0.976562\n",
      "[2018-07-16 18:12:27.616380] Iteration 43200, train loss = 0.254020, train accuracy = 0.945312\n",
      "[2018-07-16 18:12:33.801667] Iteration 43300, train loss = 0.307397, train accuracy = 0.898438\n",
      "[2018-07-16 18:12:40.003285] Iteration 43400, train loss = 0.228918, train accuracy = 0.953125\n",
      "[2018-07-16 18:12:46.171235] Iteration 43500, train loss = 0.227306, train accuracy = 0.921875\n",
      "[2018-07-16 18:12:52.374585] Iteration 43600, train loss = 0.237820, train accuracy = 0.960938\n",
      "[2018-07-16 18:12:58.556461] Iteration 43700, train loss = 0.231173, train accuracy = 0.953125\n",
      "[2018-07-16 18:13:04.714209] Iteration 43800, train loss = 0.241757, train accuracy = 0.945312\n",
      "[2018-07-16 18:13:10.918754] Iteration 43900, train loss = 0.307775, train accuracy = 0.937500\n",
      "[2018-07-16 18:13:17.106995] Iteration 44000, train loss = 0.251197, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.865800\n",
      "[2018-07-16 18:13:25.167476] Iteration 44100, train loss = 0.286984, train accuracy = 0.929688\n",
      "[2018-07-16 18:13:31.360231] Iteration 44200, train loss = 0.234258, train accuracy = 0.937500\n",
      "[2018-07-16 18:13:37.528353] Iteration 44300, train loss = 0.274072, train accuracy = 0.937500\n",
      "[2018-07-16 18:13:43.710439] Iteration 44400, train loss = 0.274780, train accuracy = 0.945312\n",
      "[2018-07-16 18:13:49.895296] Iteration 44500, train loss = 0.258318, train accuracy = 0.921875\n",
      "[2018-07-16 18:13:56.087980] Iteration 44600, train loss = 0.152743, train accuracy = 0.992188\n",
      "[2018-07-16 18:14:02.285461] Iteration 44700, train loss = 0.217228, train accuracy = 0.945312\n",
      "[2018-07-16 18:14:08.492232] Iteration 44800, train loss = 0.273415, train accuracy = 0.945312\n",
      "[2018-07-16 18:14:14.661900] Iteration 44900, train loss = 0.297429, train accuracy = 0.929688\n",
      "[2018-07-16 18:14:20.861107] Iteration 45000, train loss = 0.219138, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.837500\n",
      "[2018-07-16 18:14:28.900292] Iteration 45100, train loss = 0.216926, train accuracy = 0.960938\n",
      "[2018-07-16 18:14:35.066256] Iteration 45200, train loss = 0.315296, train accuracy = 0.890625\n",
      "[2018-07-16 18:14:41.271176] Iteration 45300, train loss = 0.291773, train accuracy = 0.914062\n",
      "[2018-07-16 18:14:47.464176] Iteration 45400, train loss = 0.225971, train accuracy = 0.953125\n",
      "[2018-07-16 18:14:53.644825] Iteration 45500, train loss = 0.241479, train accuracy = 0.929688\n",
      "[2018-07-16 18:14:59.804241] Iteration 45600, train loss = 0.255796, train accuracy = 0.945312\n",
      "[2018-07-16 18:15:05.938347] Iteration 45700, train loss = 0.241131, train accuracy = 0.953125\n",
      "[2018-07-16 18:15:12.096255] Iteration 45800, train loss = 0.212896, train accuracy = 0.953125\n",
      "[2018-07-16 18:15:18.302351] Iteration 45900, train loss = 0.294029, train accuracy = 0.921875\n",
      "[2018-07-16 18:15:24.518095] Iteration 46000, train loss = 0.267873, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.853100\n",
      "[2018-07-16 18:15:32.574315] Iteration 46100, train loss = 0.190429, train accuracy = 0.953125\n",
      "[2018-07-16 18:15:38.769067] Iteration 46200, train loss = 0.286747, train accuracy = 0.921875\n",
      "[2018-07-16 18:15:44.942649] Iteration 46300, train loss = 0.236895, train accuracy = 0.945312\n",
      "[2018-07-16 18:15:51.116674] Iteration 46400, train loss = 0.267634, train accuracy = 0.929688\n",
      "[2018-07-16 18:15:57.186781] Iteration 46500, train loss = 0.166288, train accuracy = 0.976562\n",
      "[2018-07-16 18:16:03.336949] Iteration 46600, train loss = 0.181271, train accuracy = 0.953125\n",
      "[2018-07-16 18:16:09.371084] Iteration 46700, train loss = 0.223339, train accuracy = 0.945312\n",
      "[2018-07-16 18:16:15.538246] Iteration 46800, train loss = 0.167280, train accuracy = 0.976562\n",
      "[2018-07-16 18:16:21.728433] Iteration 46900, train loss = 0.191624, train accuracy = 0.968750\n",
      "[2018-07-16 18:16:27.930041] Iteration 47000, train loss = 0.206764, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.839000\n",
      "[2018-07-16 18:16:35.995608] Iteration 47100, train loss = 0.239746, train accuracy = 0.929688\n",
      "[2018-07-16 18:16:42.134907] Iteration 47200, train loss = 0.244036, train accuracy = 0.929688\n",
      "[2018-07-16 18:16:48.316194] Iteration 47300, train loss = 0.284538, train accuracy = 0.914062\n",
      "[2018-07-16 18:16:54.463065] Iteration 47400, train loss = 0.266152, train accuracy = 0.914062\n",
      "[2018-07-16 18:17:00.660485] Iteration 47500, train loss = 0.200960, train accuracy = 0.960938\n",
      "[2018-07-16 18:17:06.813970] Iteration 47600, train loss = 0.177801, train accuracy = 0.968750\n",
      "[2018-07-16 18:17:12.991178] Iteration 47700, train loss = 0.293922, train accuracy = 0.906250\n",
      "[2018-07-16 18:17:19.128085] Iteration 47800, train loss = 0.223599, train accuracy = 0.945312\n",
      "[2018-07-16 18:17:25.292874] Iteration 47900, train loss = 0.238651, train accuracy = 0.945312\n",
      "[2018-07-16 18:17:31.505530] Iteration 48000, train loss = 0.237561, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.826800\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 18:17:39.526617] Iteration 48100, train loss = 0.219789, train accuracy = 0.960938\n",
      "[2018-07-16 18:17:45.698823] Iteration 48200, train loss = 0.216024, train accuracy = 0.953125\n",
      "[2018-07-16 18:17:51.850411] Iteration 48300, train loss = 0.199277, train accuracy = 0.960938\n",
      "[2018-07-16 18:17:58.032020] Iteration 48400, train loss = 0.234324, train accuracy = 0.945312\n",
      "[2018-07-16 18:18:04.170392] Iteration 48500, train loss = 0.198967, train accuracy = 0.960938\n",
      "[2018-07-16 18:18:10.305893] Iteration 48600, train loss = 0.231907, train accuracy = 0.953125\n",
      "[2018-07-16 18:18:16.501348] Iteration 48700, train loss = 0.141869, train accuracy = 0.992188\n",
      "[2018-07-16 18:18:22.678495] Iteration 48800, train loss = 0.263124, train accuracy = 0.953125\n",
      "[2018-07-16 18:18:28.855847] Iteration 48900, train loss = 0.208196, train accuracy = 0.960938\n",
      "[2018-07-16 18:18:35.059538] Iteration 49000, train loss = 0.309266, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.892300\n",
      "[2018-07-16 18:18:43.066329] Iteration 49100, train loss = 0.210229, train accuracy = 0.968750\n",
      "[2018-07-16 18:18:49.225831] Iteration 49200, train loss = 0.231477, train accuracy = 0.945312\n",
      "[2018-07-16 18:18:55.384554] Iteration 49300, train loss = 0.196464, train accuracy = 0.953125\n",
      "[2018-07-16 18:19:01.568338] Iteration 49400, train loss = 0.240875, train accuracy = 0.953125\n",
      "[2018-07-16 18:19:07.734490] Iteration 49500, train loss = 0.190502, train accuracy = 0.968750\n",
      "[2018-07-16 18:19:13.889925] Iteration 49600, train loss = 0.210681, train accuracy = 0.937500\n",
      "[2018-07-16 18:19:20.071164] Iteration 49700, train loss = 0.188816, train accuracy = 0.968750\n",
      "[2018-07-16 18:19:26.175408] Iteration 49800, train loss = 0.212026, train accuracy = 0.953125\n",
      "[2018-07-16 18:19:32.379049] Iteration 49900, train loss = 0.225608, train accuracy = 0.960938\n",
      "[2018-07-16 18:19:38.547651] Iteration 50000, train loss = 0.240567, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.892000\n",
      "[2018-07-16 18:19:46.539915] Iteration 50100, train loss = 0.159623, train accuracy = 0.984375\n",
      "[2018-07-16 18:19:52.694009] Iteration 50200, train loss = 0.187626, train accuracy = 0.960938\n",
      "[2018-07-16 18:19:58.886123] Iteration 50300, train loss = 0.171095, train accuracy = 0.968750\n",
      "[2018-07-16 18:20:05.011683] Iteration 50400, train loss = 0.152147, train accuracy = 0.984375\n",
      "[2018-07-16 18:20:11.159489] Iteration 50500, train loss = 0.178806, train accuracy = 0.953125\n",
      "[2018-07-16 18:20:17.331733] Iteration 50600, train loss = 0.256243, train accuracy = 0.953125\n",
      "[2018-07-16 18:20:23.481379] Iteration 50700, train loss = 0.218974, train accuracy = 0.945312\n",
      "[2018-07-16 18:20:29.608729] Iteration 50800, train loss = 0.182056, train accuracy = 0.968750\n",
      "[2018-07-16 18:20:35.758722] Iteration 50900, train loss = 0.206900, train accuracy = 0.968750\n",
      "[2018-07-16 18:20:41.967809] Iteration 51000, train loss = 0.179494, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.893100\n",
      "[2018-07-16 18:20:50.023677] Iteration 51100, train loss = 0.206633, train accuracy = 0.937500\n",
      "[2018-07-16 18:20:56.215442] Iteration 51200, train loss = 0.197839, train accuracy = 0.960938\n",
      "[2018-07-16 18:21:02.423706] Iteration 51300, train loss = 0.230120, train accuracy = 0.921875\n",
      "[2018-07-16 18:21:08.600363] Iteration 51400, train loss = 0.205884, train accuracy = 0.937500\n",
      "[2018-07-16 18:21:14.793937] Iteration 51500, train loss = 0.243672, train accuracy = 0.921875\n",
      "[2018-07-16 18:21:20.977327] Iteration 51600, train loss = 0.183833, train accuracy = 0.968750\n",
      "[2018-07-16 18:21:27.125446] Iteration 51700, train loss = 0.277867, train accuracy = 0.929688\n",
      "[2018-07-16 18:21:33.330322] Iteration 51800, train loss = 0.225711, train accuracy = 0.945312\n",
      "[2018-07-16 18:21:39.509879] Iteration 51900, train loss = 0.198669, train accuracy = 0.976562\n",
      "[2018-07-16 18:21:45.694245] Iteration 52000, train loss = 0.238174, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.893100\n",
      "[2018-07-16 18:21:53.735150] Iteration 52100, train loss = 0.283675, train accuracy = 0.921875\n",
      "[2018-07-16 18:21:59.910814] Iteration 52200, train loss = 0.256215, train accuracy = 0.937500\n",
      "[2018-07-16 18:22:06.116445] Iteration 52300, train loss = 0.165873, train accuracy = 0.953125\n",
      "[2018-07-16 18:22:12.309636] Iteration 52400, train loss = 0.201742, train accuracy = 0.960938\n",
      "[2018-07-16 18:22:18.507527] Iteration 52500, train loss = 0.258092, train accuracy = 0.937500\n",
      "[2018-07-16 18:22:24.670750] Iteration 52600, train loss = 0.156565, train accuracy = 0.968750\n",
      "[2018-07-16 18:22:30.861728] Iteration 52700, train loss = 0.204342, train accuracy = 0.960938\n",
      "[2018-07-16 18:22:37.039686] Iteration 52800, train loss = 0.184941, train accuracy = 0.968750\n",
      "[2018-07-16 18:22:43.241888] Iteration 52900, train loss = 0.199919, train accuracy = 0.953125\n",
      "[2018-07-16 18:22:49.420143] Iteration 53000, train loss = 0.190996, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.890500\n",
      "[2018-07-16 18:22:57.456734] Iteration 53100, train loss = 0.188377, train accuracy = 0.960938\n",
      "[2018-07-16 18:23:03.667392] Iteration 53200, train loss = 0.188653, train accuracy = 0.960938\n",
      "[2018-07-16 18:23:09.817398] Iteration 53300, train loss = 0.194932, train accuracy = 0.960938\n",
      "[2018-07-16 18:23:16.001763] Iteration 53400, train loss = 0.248217, train accuracy = 0.914062\n",
      "[2018-07-16 18:23:22.195333] Iteration 53500, train loss = 0.247483, train accuracy = 0.937500\n",
      "[2018-07-16 18:23:28.373572] Iteration 53600, train loss = 0.208130, train accuracy = 0.945312\n",
      "[2018-07-16 18:23:34.568078] Iteration 53700, train loss = 0.132584, train accuracy = 0.976562\n",
      "[2018-07-16 18:23:40.755822] Iteration 53800, train loss = 0.175233, train accuracy = 0.968750\n",
      "[2018-07-16 18:23:46.916218] Iteration 53900, train loss = 0.165637, train accuracy = 0.976562\n",
      "[2018-07-16 18:23:53.067673] Iteration 54000, train loss = 0.230671, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.893400\n",
      "[2018-07-16 18:24:01.070280] Iteration 54100, train loss = 0.148858, train accuracy = 0.976562\n",
      "[2018-07-16 18:24:07.194632] Iteration 54200, train loss = 0.174393, train accuracy = 0.984375\n",
      "[2018-07-16 18:24:13.375244] Iteration 54300, train loss = 0.154277, train accuracy = 0.976562\n",
      "[2018-07-16 18:24:19.529215] Iteration 54400, train loss = 0.169723, train accuracy = 0.968750\n",
      "[2018-07-16 18:24:25.677770] Iteration 54500, train loss = 0.197249, train accuracy = 0.945312\n",
      "[2018-07-16 18:24:31.865211] Iteration 54600, train loss = 0.151408, train accuracy = 0.968750\n",
      "[2018-07-16 18:24:38.067745] Iteration 54700, train loss = 0.165953, train accuracy = 0.968750\n",
      "[2018-07-16 18:24:44.230333] Iteration 54800, train loss = 0.153516, train accuracy = 0.976562\n",
      "[2018-07-16 18:24:50.414275] Iteration 54900, train loss = 0.138993, train accuracy = 0.992188\n",
      "[2018-07-16 18:24:56.573811] Iteration 55000, train loss = 0.170735, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.891200\n",
      "[2018-07-16 18:25:04.647945] Iteration 55100, train loss = 0.176118, train accuracy = 0.968750\n",
      "[2018-07-16 18:25:10.817567] Iteration 55200, train loss = 0.144037, train accuracy = 0.984375\n",
      "[2018-07-16 18:25:16.943916] Iteration 55300, train loss = 0.138976, train accuracy = 0.984375\n",
      "[2018-07-16 18:25:23.133203] Iteration 55400, train loss = 0.177351, train accuracy = 0.968750\n",
      "[2018-07-16 18:25:29.281133] Iteration 55500, train loss = 0.174712, train accuracy = 0.976562\n",
      "[2018-07-16 18:25:35.476266] Iteration 55600, train loss = 0.141976, train accuracy = 0.984375\n",
      "[2018-07-16 18:25:41.635233] Iteration 55700, train loss = 0.165070, train accuracy = 0.976562\n",
      "[2018-07-16 18:25:47.779754] Iteration 55800, train loss = 0.202423, train accuracy = 0.960938\n",
      "[2018-07-16 18:25:53.904239] Iteration 55900, train loss = 0.193621, train accuracy = 0.953125\n",
      "[2018-07-16 18:26:00.113883] Iteration 56000, train loss = 0.191822, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.890900\n",
      "[2018-07-16 18:26:08.171420] Iteration 56100, train loss = 0.164774, train accuracy = 0.976562\n",
      "[2018-07-16 18:26:14.353682] Iteration 56200, train loss = 0.151104, train accuracy = 0.976562\n",
      "[2018-07-16 18:26:20.526952] Iteration 56300, train loss = 0.259763, train accuracy = 0.937500\n",
      "[2018-07-16 18:26:26.681535] Iteration 56400, train loss = 0.156317, train accuracy = 0.960938\n",
      "[2018-07-16 18:26:32.770821] Iteration 56500, train loss = 0.167400, train accuracy = 0.968750\n",
      "[2018-07-16 18:26:38.950979] Iteration 56600, train loss = 0.140950, train accuracy = 0.992188\n",
      "[2018-07-16 18:26:45.139285] Iteration 56700, train loss = 0.152485, train accuracy = 0.960938\n",
      "[2018-07-16 18:26:51.306804] Iteration 56800, train loss = 0.159716, train accuracy = 0.968750\n",
      "[2018-07-16 18:26:57.472973] Iteration 56900, train loss = 0.173066, train accuracy = 0.976562\n",
      "[2018-07-16 18:27:03.646011] Iteration 57000, train loss = 0.158494, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.891400\n",
      "[2018-07-16 18:27:11.715366] Iteration 57100, train loss = 0.180172, train accuracy = 0.960938\n",
      "[2018-07-16 18:27:17.812818] Iteration 57200, train loss = 0.163552, train accuracy = 0.968750\n",
      "[2018-07-16 18:27:23.963905] Iteration 57300, train loss = 0.154905, train accuracy = 0.960938\n",
      "[2018-07-16 18:27:30.091306] Iteration 57400, train loss = 0.147235, train accuracy = 0.976562\n",
      "[2018-07-16 18:27:36.246697] Iteration 57500, train loss = 0.162311, train accuracy = 0.984375\n",
      "[2018-07-16 18:27:42.373392] Iteration 57600, train loss = 0.235132, train accuracy = 0.953125\n",
      "[2018-07-16 18:27:48.532518] Iteration 57700, train loss = 0.282952, train accuracy = 0.906250\n",
      "[2018-07-16 18:27:54.675737] Iteration 57800, train loss = 0.168532, train accuracy = 0.984375\n",
      "[2018-07-16 18:28:00.823700] Iteration 57900, train loss = 0.258069, train accuracy = 0.953125\n",
      "[2018-07-16 18:28:06.953554] Iteration 58000, train loss = 0.207772, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.891500\n",
      "[2018-07-16 18:28:14.916858] Iteration 58100, train loss = 0.133678, train accuracy = 0.976562\n",
      "[2018-07-16 18:28:21.070491] Iteration 58200, train loss = 0.229181, train accuracy = 0.929688\n",
      "[2018-07-16 18:28:27.200880] Iteration 58300, train loss = 0.162021, train accuracy = 0.976562\n",
      "[2018-07-16 18:28:33.317386] Iteration 58400, train loss = 0.204342, train accuracy = 0.953125\n",
      "[2018-07-16 18:28:39.498411] Iteration 58500, train loss = 0.155757, train accuracy = 0.984375\n",
      "[2018-07-16 18:28:45.627485] Iteration 58600, train loss = 0.167658, train accuracy = 0.968750\n",
      "[2018-07-16 18:28:51.768482] Iteration 58700, train loss = 0.177034, train accuracy = 0.953125\n",
      "[2018-07-16 18:28:57.940470] Iteration 58800, train loss = 0.170477, train accuracy = 0.945312\n",
      "[2018-07-16 18:29:04.100565] Iteration 58900, train loss = 0.171803, train accuracy = 0.968750\n",
      "[2018-07-16 18:29:10.275462] Iteration 59000, train loss = 0.165370, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.892900\n",
      "[2018-07-16 18:29:18.179390] Iteration 59100, train loss = 0.172928, train accuracy = 0.976562\n",
      "[2018-07-16 18:29:24.348944] Iteration 59200, train loss = 0.193548, train accuracy = 0.960938\n",
      "[2018-07-16 18:29:30.500930] Iteration 59300, train loss = 0.167736, train accuracy = 0.960938\n",
      "[2018-07-16 18:29:36.669882] Iteration 59400, train loss = 0.138751, train accuracy = 0.992188\n",
      "[2018-07-16 18:29:42.812083] Iteration 59500, train loss = 0.179154, train accuracy = 0.960938\n",
      "[2018-07-16 18:29:48.934576] Iteration 59600, train loss = 0.123196, train accuracy = 0.992188\n",
      "[2018-07-16 18:29:55.126373] Iteration 59700, train loss = 0.197078, train accuracy = 0.960938\n",
      "[2018-07-16 18:30:01.272970] Iteration 59800, train loss = 0.196989, train accuracy = 0.960938\n",
      "[2018-07-16 18:30:07.456844] Iteration 59900, train loss = 0.190414, train accuracy = 0.960938\n",
      "[2018-07-16 18:30:13.579486] Iteration 60000, train loss = 0.116256, train accuracy = 1.000000\n",
      "Evaluating...\n",
      "Test accuracy = 0.892600\n",
      "[2018-07-16 18:30:21.599867] Iteration 60100, train loss = 0.167418, train accuracy = 0.960938\n",
      "[2018-07-16 18:30:27.706516] Iteration 60200, train loss = 0.195137, train accuracy = 0.968750\n",
      "[2018-07-16 18:30:33.873941] Iteration 60300, train loss = 0.172146, train accuracy = 0.968750\n",
      "[2018-07-16 18:30:40.030270] Iteration 60400, train loss = 0.233632, train accuracy = 0.937500\n",
      "[2018-07-16 18:30:46.200327] Iteration 60500, train loss = 0.183617, train accuracy = 0.953125\n",
      "[2018-07-16 18:30:52.357409] Iteration 60600, train loss = 0.183631, train accuracy = 0.960938\n",
      "[2018-07-16 18:30:58.522492] Iteration 60700, train loss = 0.205135, train accuracy = 0.960938\n",
      "[2018-07-16 18:31:04.710105] Iteration 60800, train loss = 0.233973, train accuracy = 0.953125\n",
      "[2018-07-16 18:31:10.829479] Iteration 60900, train loss = 0.182063, train accuracy = 0.976562\n",
      "[2018-07-16 18:31:16.956067] Iteration 61000, train loss = 0.145464, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891200\n",
      "[2018-07-16 18:31:24.923542] Iteration 61100, train loss = 0.160348, train accuracy = 0.968750\n",
      "[2018-07-16 18:31:31.039096] Iteration 61200, train loss = 0.248524, train accuracy = 0.953125\n",
      "[2018-07-16 18:31:37.192069] Iteration 61300, train loss = 0.192859, train accuracy = 0.960938\n",
      "[2018-07-16 18:31:43.374860] Iteration 61400, train loss = 0.229108, train accuracy = 0.937500\n",
      "[2018-07-16 18:31:49.551175] Iteration 61500, train loss = 0.205971, train accuracy = 0.937500\n",
      "[2018-07-16 18:31:55.671906] Iteration 61600, train loss = 0.226600, train accuracy = 0.945312\n",
      "[2018-07-16 18:32:01.859071] Iteration 61700, train loss = 0.172044, train accuracy = 0.968750\n",
      "[2018-07-16 18:32:07.979514] Iteration 61800, train loss = 0.135577, train accuracy = 0.984375\n",
      "[2018-07-16 18:32:14.147197] Iteration 61900, train loss = 0.255237, train accuracy = 0.937500\n",
      "[2018-07-16 18:32:20.332231] Iteration 62000, train loss = 0.156266, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.891700\n",
      "[2018-07-16 18:32:28.337097] Iteration 62100, train loss = 0.162262, train accuracy = 0.976562\n",
      "[2018-07-16 18:32:34.453355] Iteration 62200, train loss = 0.123535, train accuracy = 0.992188\n",
      "[2018-07-16 18:32:40.613026] Iteration 62300, train loss = 0.156157, train accuracy = 0.976562\n",
      "[2018-07-16 18:32:46.754575] Iteration 62400, train loss = 0.183559, train accuracy = 0.960938\n",
      "[2018-07-16 18:32:52.915444] Iteration 62500, train loss = 0.188802, train accuracy = 0.976562\n",
      "[2018-07-16 18:32:59.107034] Iteration 62600, train loss = 0.238397, train accuracy = 0.953125\n",
      "[2018-07-16 18:33:05.267303] Iteration 62700, train loss = 0.206098, train accuracy = 0.945312\n",
      "[2018-07-16 18:33:11.435240] Iteration 62800, train loss = 0.150848, train accuracy = 0.968750\n",
      "[2018-07-16 18:33:17.592066] Iteration 62900, train loss = 0.187839, train accuracy = 0.953125\n",
      "[2018-07-16 18:33:23.741117] Iteration 63000, train loss = 0.171875, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891400\n",
      "[2018-07-16 18:33:31.712719] Iteration 63100, train loss = 0.142285, train accuracy = 0.984375\n",
      "[2018-07-16 18:33:37.856204] Iteration 63200, train loss = 0.123983, train accuracy = 1.000000\n",
      "[2018-07-16 18:33:43.998330] Iteration 63300, train loss = 0.137813, train accuracy = 0.984375\n",
      "[2018-07-16 18:33:50.194156] Iteration 63400, train loss = 0.230043, train accuracy = 0.953125\n",
      "[2018-07-16 18:33:56.374453] Iteration 63500, train loss = 0.128934, train accuracy = 0.984375\n",
      "[2018-07-16 18:34:02.554759] Iteration 63600, train loss = 0.166622, train accuracy = 0.984375\n",
      "[2018-07-16 18:34:08.688899] Iteration 63700, train loss = 0.262601, train accuracy = 0.929688\n",
      "[2018-07-16 18:34:14.820227] Iteration 63800, train loss = 0.159556, train accuracy = 0.976562\n",
      "[2018-07-16 18:34:20.987690] Iteration 63900, train loss = 0.143108, train accuracy = 0.984375\n",
      "[2018-07-16 18:34:27.154113] Iteration 64000, train loss = 0.131737, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.889700\n",
      "[2018-07-16 18:34:35.150444] Iteration 64100, train loss = 0.210974, train accuracy = 0.945312\n",
      "[2018-07-16 18:34:41.294977] Iteration 64200, train loss = 0.194873, train accuracy = 0.953125\n",
      "[2018-07-16 18:34:47.468718] Iteration 64300, train loss = 0.143990, train accuracy = 0.992188\n",
      "[2018-07-16 18:34:53.644475] Iteration 64400, train loss = 0.161589, train accuracy = 0.968750\n",
      "[2018-07-16 18:34:59.763854] Iteration 64500, train loss = 0.125563, train accuracy = 1.000000\n",
      "[2018-07-16 18:35:05.906182] Iteration 64600, train loss = 0.151150, train accuracy = 0.976562\n",
      "[2018-07-16 18:35:12.099581] Iteration 64700, train loss = 0.195130, train accuracy = 0.960938\n",
      "[2018-07-16 18:35:18.228285] Iteration 64800, train loss = 0.157592, train accuracy = 0.968750\n",
      "[2018-07-16 18:35:24.407635] Iteration 64900, train loss = 0.163815, train accuracy = 0.968750\n",
      "[2018-07-16 18:35:30.565972] Iteration 65000, train loss = 0.151425, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.891500\n",
      "[2018-07-16 18:35:38.623887] Iteration 65100, train loss = 0.192491, train accuracy = 0.968750\n",
      "[2018-07-16 18:35:44.796282] Iteration 65200, train loss = 0.153082, train accuracy = 0.976562\n",
      "[2018-07-16 18:35:50.947418] Iteration 65300, train loss = 0.191978, train accuracy = 0.960938\n",
      "[2018-07-16 18:35:57.075350] Iteration 65400, train loss = 0.243942, train accuracy = 0.945312\n",
      "[2018-07-16 18:36:03.253898] Iteration 65500, train loss = 0.128141, train accuracy = 1.000000\n",
      "[2018-07-16 18:36:09.432612] Iteration 65600, train loss = 0.218489, train accuracy = 0.937500\n",
      "[2018-07-16 18:36:15.606090] Iteration 65700, train loss = 0.135880, train accuracy = 0.976562\n",
      "[2018-07-16 18:36:21.803128] Iteration 65800, train loss = 0.147384, train accuracy = 0.976562\n",
      "[2018-07-16 18:36:27.988553] Iteration 65900, train loss = 0.154949, train accuracy = 0.976562\n",
      "[2018-07-16 18:36:34.166782] Iteration 66000, train loss = 0.152950, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.889500\n",
      "[2018-07-16 18:36:42.236374] Iteration 66100, train loss = 0.157540, train accuracy = 0.976562\n",
      "[2018-07-16 18:36:48.417169] Iteration 66200, train loss = 0.166269, train accuracy = 0.984375\n",
      "[2018-07-16 18:36:54.616296] Iteration 66300, train loss = 0.307064, train accuracy = 0.945312\n",
      "[2018-07-16 18:37:00.784474] Iteration 66400, train loss = 0.172559, train accuracy = 0.953125\n",
      "[2018-07-16 18:37:06.909582] Iteration 66500, train loss = 0.199452, train accuracy = 0.960938\n",
      "[2018-07-16 18:37:13.096572] Iteration 66600, train loss = 0.137164, train accuracy = 1.000000\n",
      "[2018-07-16 18:37:19.307081] Iteration 66700, train loss = 0.154326, train accuracy = 0.960938\n",
      "[2018-07-16 18:37:25.484054] Iteration 66800, train loss = 0.156096, train accuracy = 0.968750\n",
      "[2018-07-16 18:37:31.576637] Iteration 66900, train loss = 0.168789, train accuracy = 0.968750\n",
      "[2018-07-16 18:37:37.757131] Iteration 67000, train loss = 0.161497, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.889000\n",
      "[2018-07-16 18:37:45.837245] Iteration 67100, train loss = 0.193252, train accuracy = 0.968750\n",
      "[2018-07-16 18:37:52.043848] Iteration 67200, train loss = 0.167009, train accuracy = 0.960938\n",
      "[2018-07-16 18:37:58.260736] Iteration 67300, train loss = 0.170575, train accuracy = 0.960938\n",
      "[2018-07-16 18:38:04.425949] Iteration 67400, train loss = 0.123504, train accuracy = 1.000000\n",
      "[2018-07-16 18:38:10.561877] Iteration 67500, train loss = 0.180207, train accuracy = 0.960938\n",
      "[2018-07-16 18:38:16.736841] Iteration 67600, train loss = 0.197158, train accuracy = 0.968750\n",
      "[2018-07-16 18:38:22.910456] Iteration 67700, train loss = 0.171235, train accuracy = 0.968750\n",
      "[2018-07-16 18:38:28.990022] Iteration 67800, train loss = 0.125266, train accuracy = 1.000000\n",
      "[2018-07-16 18:38:35.048119] Iteration 67900, train loss = 0.121217, train accuracy = 0.984375\n",
      "[2018-07-16 18:38:41.141962] Iteration 68000, train loss = 0.149811, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891400\n",
      "[2018-07-16 18:38:48.993560] Iteration 68100, train loss = 0.153004, train accuracy = 0.968750\n",
      "[2018-07-16 18:38:55.005488] Iteration 68200, train loss = 0.163366, train accuracy = 0.968750\n",
      "[2018-07-16 18:39:00.965836] Iteration 68300, train loss = 0.141774, train accuracy = 0.984375\n",
      "[2018-07-16 18:39:06.973613] Iteration 68400, train loss = 0.189901, train accuracy = 0.960938\n",
      "[2018-07-16 18:39:12.932391] Iteration 68500, train loss = 0.129708, train accuracy = 1.000000\n",
      "[2018-07-16 18:39:18.912927] Iteration 68600, train loss = 0.175278, train accuracy = 0.968750\n",
      "[2018-07-16 18:39:24.882047] Iteration 68700, train loss = 0.121680, train accuracy = 1.000000\n",
      "[2018-07-16 18:39:30.863126] Iteration 68800, train loss = 0.196637, train accuracy = 0.960938\n",
      "[2018-07-16 18:39:36.802612] Iteration 68900, train loss = 0.247612, train accuracy = 0.921875\n",
      "[2018-07-16 18:39:42.707288] Iteration 69000, train loss = 0.207716, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.893800\n",
      "[2018-07-16 18:39:50.558299] Iteration 69100, train loss = 0.194787, train accuracy = 0.937500\n",
      "[2018-07-16 18:39:56.516409] Iteration 69200, train loss = 0.128886, train accuracy = 0.992188\n",
      "[2018-07-16 18:40:02.481617] Iteration 69300, train loss = 0.187920, train accuracy = 0.960938\n",
      "[2018-07-16 18:40:08.432787] Iteration 69400, train loss = 0.142353, train accuracy = 0.984375\n",
      "[2018-07-16 18:40:14.422174] Iteration 69500, train loss = 0.191583, train accuracy = 0.945312\n",
      "[2018-07-16 18:40:20.360375] Iteration 69600, train loss = 0.170300, train accuracy = 0.984375\n",
      "[2018-07-16 18:40:26.307792] Iteration 69700, train loss = 0.239162, train accuracy = 0.953125\n",
      "[2018-07-16 18:40:32.294143] Iteration 69800, train loss = 0.127485, train accuracy = 0.984375\n",
      "[2018-07-16 18:40:38.312096] Iteration 69900, train loss = 0.158112, train accuracy = 0.960938\n",
      "[2018-07-16 18:40:44.400754] Iteration 70000, train loss = 0.149694, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891700\n",
      "[2018-07-16 18:40:52.390576] Iteration 70100, train loss = 0.167180, train accuracy = 0.953125\n",
      "[2018-07-16 18:40:58.373641] Iteration 70200, train loss = 0.167364, train accuracy = 0.984375\n",
      "[2018-07-16 18:41:04.350789] Iteration 70300, train loss = 0.109491, train accuracy = 0.992188\n",
      "[2018-07-16 18:41:10.392977] Iteration 70400, train loss = 0.141481, train accuracy = 0.976562\n",
      "[2018-07-16 18:41:16.431759] Iteration 70500, train loss = 0.183254, train accuracy = 0.953125\n",
      "[2018-07-16 18:41:22.452711] Iteration 70600, train loss = 0.192188, train accuracy = 0.960938\n",
      "[2018-07-16 18:41:28.497957] Iteration 70700, train loss = 0.174432, train accuracy = 0.953125\n",
      "[2018-07-16 18:41:34.545442] Iteration 70800, train loss = 0.183286, train accuracy = 0.960938\n",
      "[2018-07-16 18:41:40.638034] Iteration 70900, train loss = 0.157401, train accuracy = 0.976562\n",
      "[2018-07-16 18:41:46.743518] Iteration 71000, train loss = 0.166756, train accuracy = 0.976562\n",
      "Evaluating...\n",
      "Test accuracy = 0.891600\n",
      "[2018-07-16 18:41:54.737734] Iteration 71100, train loss = 0.169450, train accuracy = 0.976562\n",
      "[2018-07-16 18:42:00.798117] Iteration 71200, train loss = 0.151968, train accuracy = 0.976562\n",
      "[2018-07-16 18:42:06.891959] Iteration 71300, train loss = 0.160928, train accuracy = 0.976562\n",
      "[2018-07-16 18:42:12.998281] Iteration 71400, train loss = 0.121201, train accuracy = 0.992188\n",
      "[2018-07-16 18:42:19.094211] Iteration 71500, train loss = 0.150168, train accuracy = 0.960938\n",
      "[2018-07-16 18:42:25.213333] Iteration 71600, train loss = 0.233909, train accuracy = 0.953125\n",
      "[2018-07-16 18:42:31.311990] Iteration 71700, train loss = 0.201854, train accuracy = 0.953125\n",
      "[2018-07-16 18:42:37.371823] Iteration 71800, train loss = 0.141741, train accuracy = 0.992188\n",
      "[2018-07-16 18:42:43.521348] Iteration 71900, train loss = 0.234584, train accuracy = 0.953125\n",
      "[2018-07-16 18:42:49.696816] Iteration 72000, train loss = 0.195538, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.890400\n",
      "[2018-07-16 18:42:57.697549] Iteration 72100, train loss = 0.188827, train accuracy = 0.945312\n",
      "[2018-07-16 18:43:03.849306] Iteration 72200, train loss = 0.189058, train accuracy = 0.953125\n",
      "[2018-07-16 18:43:09.933191] Iteration 72300, train loss = 0.162396, train accuracy = 0.968750\n",
      "[2018-07-16 18:43:16.035119] Iteration 72400, train loss = 0.160080, train accuracy = 0.968750\n",
      "[2018-07-16 18:43:22.232524] Iteration 72500, train loss = 0.141090, train accuracy = 1.000000\n",
      "[2018-07-16 18:43:28.375435] Iteration 72600, train loss = 0.150720, train accuracy = 0.976562\n",
      "[2018-07-16 18:43:34.524522] Iteration 72700, train loss = 0.178543, train accuracy = 0.968750\n",
      "[2018-07-16 18:43:40.740500] Iteration 72800, train loss = 0.146655, train accuracy = 0.968750\n",
      "[2018-07-16 18:43:46.937142] Iteration 72900, train loss = 0.163695, train accuracy = 0.968750\n",
      "[2018-07-16 18:43:53.131319] Iteration 73000, train loss = 0.132210, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.890900\n",
      "[2018-07-16 18:44:01.157685] Iteration 73100, train loss = 0.158578, train accuracy = 0.968750\n",
      "[2018-07-16 18:44:07.256567] Iteration 73200, train loss = 0.173009, train accuracy = 0.960938\n",
      "[2018-07-16 18:44:13.411451] Iteration 73300, train loss = 0.164497, train accuracy = 0.960938\n",
      "[2018-07-16 18:44:19.522535] Iteration 73400, train loss = 0.155012, train accuracy = 0.976562\n",
      "[2018-07-16 18:44:25.724916] Iteration 73500, train loss = 0.171442, train accuracy = 0.968750\n",
      "[2018-07-16 18:44:31.902161] Iteration 73600, train loss = 0.243069, train accuracy = 0.945312\n",
      "[2018-07-16 18:44:38.074992] Iteration 73700, train loss = 0.195594, train accuracy = 0.953125\n",
      "[2018-07-16 18:44:44.231793] Iteration 73800, train loss = 0.180226, train accuracy = 0.960938\n",
      "[2018-07-16 18:44:50.412931] Iteration 73900, train loss = 0.212651, train accuracy = 0.960938\n",
      "[2018-07-16 18:44:56.575041] Iteration 74000, train loss = 0.156668, train accuracy = 0.968750\n",
      "Evaluating...\n",
      "Test accuracy = 0.893100\n",
      "[2018-07-16 18:45:04.604816] Iteration 74100, train loss = 0.265995, train accuracy = 0.914062\n",
      "[2018-07-16 18:45:10.789949] Iteration 74200, train loss = 0.204067, train accuracy = 0.968750\n",
      "[2018-07-16 18:45:16.953657] Iteration 74300, train loss = 0.156317, train accuracy = 0.968750\n",
      "[2018-07-16 18:45:23.125053] Iteration 74400, train loss = 0.165354, train accuracy = 0.968750\n",
      "[2018-07-16 18:45:29.287728] Iteration 74500, train loss = 0.169755, train accuracy = 0.960938\n",
      "[2018-07-16 18:45:35.440603] Iteration 74600, train loss = 0.254694, train accuracy = 0.921875\n",
      "[2018-07-16 18:45:41.575463] Iteration 74700, train loss = 0.202166, train accuracy = 0.953125\n",
      "[2018-07-16 18:45:47.754598] Iteration 74800, train loss = 0.156455, train accuracy = 0.968750\n",
      "[2018-07-16 18:45:53.931923] Iteration 74900, train loss = 0.212019, train accuracy = 0.945312\n",
      "[2018-07-16 18:46:00.099264] Iteration 75000, train loss = 0.108779, train accuracy = 0.992188\n",
      "Evaluating...\n",
      "Test accuracy = 0.891200\n",
      "[2018-07-16 18:46:08.078856] Iteration 75100, train loss = 0.184851, train accuracy = 0.968750\n",
      "[2018-07-16 18:46:14.258107] Iteration 75200, train loss = 0.204465, train accuracy = 0.968750\n",
      "[2018-07-16 18:46:20.416638] Iteration 75300, train loss = 0.146502, train accuracy = 0.992188\n",
      "[2018-07-16 18:46:26.602589] Iteration 75400, train loss = 0.175670, train accuracy = 0.960938\n",
      "[2018-07-16 18:46:32.717341] Iteration 75500, train loss = 0.134670, train accuracy = 0.984375\n",
      "[2018-07-16 18:46:38.904697] Iteration 75600, train loss = 0.144971, train accuracy = 0.976562\n",
      "[2018-07-16 18:46:45.068490] Iteration 75700, train loss = 0.117102, train accuracy = 1.000000\n",
      "[2018-07-16 18:46:51.254040] Iteration 75800, train loss = 0.154464, train accuracy = 0.976562\n",
      "[2018-07-16 18:46:57.448330] Iteration 75900, train loss = 0.133089, train accuracy = 0.984375\n",
      "[2018-07-16 18:47:03.599913] Iteration 76000, train loss = 0.144254, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891300\n",
      "[2018-07-16 18:47:11.623895] Iteration 76100, train loss = 0.112428, train accuracy = 0.992188\n",
      "[2018-07-16 18:47:17.783894] Iteration 76200, train loss = 0.189571, train accuracy = 0.960938\n",
      "[2018-07-16 18:47:23.951087] Iteration 76300, train loss = 0.154349, train accuracy = 0.968750\n",
      "[2018-07-16 18:47:30.046160] Iteration 76400, train loss = 0.148262, train accuracy = 0.968750\n",
      "[2018-07-16 18:47:36.213831] Iteration 76500, train loss = 0.208215, train accuracy = 0.945312\n",
      "[2018-07-16 18:47:42.376267] Iteration 76600, train loss = 0.184036, train accuracy = 0.968750\n",
      "[2018-07-16 18:47:48.505135] Iteration 76700, train loss = 0.177785, train accuracy = 0.960938\n",
      "[2018-07-16 18:47:54.621781] Iteration 76800, train loss = 0.184225, train accuracy = 0.945312\n",
      "[2018-07-16 18:48:00.776203] Iteration 76900, train loss = 0.163772, train accuracy = 0.976562\n",
      "[2018-07-16 18:48:06.943329] Iteration 77000, train loss = 0.197927, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.892100\n",
      "[2018-07-16 18:48:14.949814] Iteration 77100, train loss = 0.163174, train accuracy = 0.976562\n",
      "[2018-07-16 18:48:21.104436] Iteration 77200, train loss = 0.194131, train accuracy = 0.960938\n",
      "[2018-07-16 18:48:27.154537] Iteration 77300, train loss = 0.237359, train accuracy = 0.953125\n",
      "[2018-07-16 18:48:33.340688] Iteration 77400, train loss = 0.204988, train accuracy = 0.953125\n",
      "[2018-07-16 18:48:39.440682] Iteration 77500, train loss = 0.227776, train accuracy = 0.960938\n",
      "[2018-07-16 18:48:45.568455] Iteration 77600, train loss = 0.232458, train accuracy = 0.929688\n",
      "[2018-07-16 18:48:51.751452] Iteration 77700, train loss = 0.143501, train accuracy = 0.976562\n",
      "[2018-07-16 18:48:57.879510] Iteration 77800, train loss = 0.184886, train accuracy = 0.968750\n",
      "[2018-07-16 18:49:03.982289] Iteration 77900, train loss = 0.148440, train accuracy = 0.976562\n",
      "[2018-07-16 18:49:10.143946] Iteration 78000, train loss = 0.127284, train accuracy = 0.984375\n",
      "Evaluating...\n",
      "Test accuracy = 0.891900\n",
      "[2018-07-16 18:49:18.078251] Iteration 78100, train loss = 0.177836, train accuracy = 0.968750\n",
      "[2018-07-16 18:49:24.195536] Iteration 78200, train loss = 0.170893, train accuracy = 0.960938\n",
      "[2018-07-16 18:49:30.314126] Iteration 78300, train loss = 0.150876, train accuracy = 0.968750\n",
      "[2018-07-16 18:49:36.382581] Iteration 78400, train loss = 0.172021, train accuracy = 0.968750\n",
      "[2018-07-16 18:49:42.528353] Iteration 78500, train loss = 0.164150, train accuracy = 0.984375\n",
      "[2018-07-16 18:49:48.643585] Iteration 78600, train loss = 0.157159, train accuracy = 0.968750\n",
      "[2018-07-16 18:49:54.786588] Iteration 78700, train loss = 0.144869, train accuracy = 0.976562\n",
      "[2018-07-16 18:50:00.936882] Iteration 78800, train loss = 0.211119, train accuracy = 0.960938\n",
      "[2018-07-16 18:50:07.062608] Iteration 78900, train loss = 0.202902, train accuracy = 0.976562\n",
      "[2018-07-16 18:50:13.219826] Iteration 79000, train loss = 0.162827, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.892000\n",
      "[2018-07-16 18:50:21.217372] Iteration 79100, train loss = 0.193357, train accuracy = 0.960938\n",
      "[2018-07-16 18:50:27.352021] Iteration 79200, train loss = 0.192049, train accuracy = 0.968750\n",
      "[2018-07-16 18:50:33.494303] Iteration 79300, train loss = 0.166948, train accuracy = 0.976562\n",
      "[2018-07-16 18:50:39.627275] Iteration 79400, train loss = 0.169373, train accuracy = 0.976562\n",
      "[2018-07-16 18:50:45.724187] Iteration 79500, train loss = 0.162797, train accuracy = 0.968750\n",
      "[2018-07-16 18:50:51.836007] Iteration 79600, train loss = 0.143989, train accuracy = 0.984375\n",
      "[2018-07-16 18:50:57.941720] Iteration 79700, train loss = 0.148392, train accuracy = 0.976562\n",
      "[2018-07-16 18:51:04.053547] Iteration 79800, train loss = 0.192311, train accuracy = 0.968750\n",
      "[2018-07-16 18:51:10.175378] Iteration 79900, train loss = 0.123711, train accuracy = 0.992188\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第三轮  量化\n",
    "prune_rate = 0.85"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.890600\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.0625      0.125      -0.0260509  -0.05282128\n",
      "  0.         -0.00231245 -0.125      -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.75时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,0.85)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 18:52:40.175028] Iteration 100, train loss = 1.447202, train accuracy = 0.531250\n",
      "[2018-07-16 18:52:45.799083] Iteration 200, train loss = 1.074247, train accuracy = 0.632812\n",
      "[2018-07-16 18:52:51.471280] Iteration 300, train loss = 1.077294, train accuracy = 0.687500\n",
      "[2018-07-16 18:52:57.180837] Iteration 400, train loss = 1.061212, train accuracy = 0.710938\n",
      "[2018-07-16 18:53:02.848593] Iteration 500, train loss = 0.843215, train accuracy = 0.734375\n",
      "[2018-07-16 18:53:08.503702] Iteration 600, train loss = 0.740226, train accuracy = 0.796875\n",
      "[2018-07-16 18:53:14.369119] Iteration 700, train loss = 0.857359, train accuracy = 0.726562\n",
      "[2018-07-16 18:53:20.287913] Iteration 800, train loss = 0.828355, train accuracy = 0.765625\n",
      "[2018-07-16 18:53:26.180618] Iteration 900, train loss = 0.776286, train accuracy = 0.718750\n",
      "[2018-07-16 18:53:32.000441] Iteration 1000, train loss = 0.719240, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.694800\n",
      "[2018-07-16 18:53:39.607625] Iteration 1100, train loss = 0.715761, train accuracy = 0.804688\n",
      "[2018-07-16 18:53:45.602750] Iteration 1200, train loss = 0.728137, train accuracy = 0.765625\n",
      "[2018-07-16 18:53:51.589026] Iteration 1300, train loss = 0.784973, train accuracy = 0.734375\n",
      "[2018-07-16 18:53:57.475128] Iteration 1400, train loss = 0.713875, train accuracy = 0.804688\n",
      "[2018-07-16 18:54:03.434160] Iteration 1500, train loss = 0.830933, train accuracy = 0.703125\n",
      "[2018-07-16 18:54:09.408808] Iteration 1600, train loss = 0.598863, train accuracy = 0.851562\n",
      "[2018-07-16 18:54:15.376367] Iteration 1700, train loss = 0.699710, train accuracy = 0.773438\n",
      "[2018-07-16 18:54:21.441247] Iteration 1800, train loss = 0.690446, train accuracy = 0.828125\n",
      "[2018-07-16 18:54:27.469255] Iteration 1900, train loss = 0.711774, train accuracy = 0.804688\n",
      "[2018-07-16 18:54:33.498920] Iteration 2000, train loss = 0.566306, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.786300\n",
      "[2018-07-16 18:54:41.425827] Iteration 2100, train loss = 0.662716, train accuracy = 0.789062\n",
      "[2018-07-16 18:54:47.458116] Iteration 2200, train loss = 0.736124, train accuracy = 0.757812\n",
      "[2018-07-16 18:54:53.512309] Iteration 2300, train loss = 0.530829, train accuracy = 0.835938\n",
      "[2018-07-16 18:54:59.645553] Iteration 2400, train loss = 0.687715, train accuracy = 0.789062\n",
      "[2018-07-16 18:55:05.752829] Iteration 2500, train loss = 0.599627, train accuracy = 0.828125\n",
      "[2018-07-16 18:55:11.849651] Iteration 2600, train loss = 0.604728, train accuracy = 0.781250\n",
      "[2018-07-16 18:55:17.920445] Iteration 2700, train loss = 0.574705, train accuracy = 0.820312\n",
      "[2018-07-16 18:55:24.035051] Iteration 2800, train loss = 0.615778, train accuracy = 0.820312\n",
      "[2018-07-16 18:55:30.197786] Iteration 2900, train loss = 0.731539, train accuracy = 0.765625\n",
      "[2018-07-16 18:55:36.299122] Iteration 3000, train loss = 0.608754, train accuracy = 0.812500\n",
      "Evaluating...\n",
      "Test accuracy = 0.769500\n",
      "[2018-07-16 18:55:44.205956] Iteration 3100, train loss = 0.459776, train accuracy = 0.867188\n",
      "[2018-07-16 18:55:50.279999] Iteration 3200, train loss = 0.571386, train accuracy = 0.851562\n",
      "[2018-07-16 18:55:56.349518] Iteration 3300, train loss = 0.395514, train accuracy = 0.898438\n",
      "[2018-07-16 18:56:02.494412] Iteration 3400, train loss = 0.529760, train accuracy = 0.820312\n",
      "[2018-07-16 18:56:08.594447] Iteration 3500, train loss = 0.406443, train accuracy = 0.890625\n",
      "[2018-07-16 18:56:14.736134] Iteration 3600, train loss = 0.592450, train accuracy = 0.843750\n",
      "[2018-07-16 18:56:20.868592] Iteration 3700, train loss = 0.591642, train accuracy = 0.804688\n",
      "[2018-07-16 18:56:26.995712] Iteration 3800, train loss = 0.609454, train accuracy = 0.804688\n",
      "[2018-07-16 18:56:33.135958] Iteration 3900, train loss = 0.503756, train accuracy = 0.843750\n",
      "[2018-07-16 18:56:39.254844] Iteration 4000, train loss = 0.624814, train accuracy = 0.781250\n",
      "Evaluating...\n",
      "Test accuracy = 0.790000\n",
      "[2018-07-16 18:56:47.291123] Iteration 4100, train loss = 0.591409, train accuracy = 0.828125\n",
      "[2018-07-16 18:56:53.400076] Iteration 4200, train loss = 0.579242, train accuracy = 0.812500\n",
      "[2018-07-16 18:56:59.480901] Iteration 4300, train loss = 0.530798, train accuracy = 0.851562\n",
      "[2018-07-16 18:57:05.562904] Iteration 4400, train loss = 0.581099, train accuracy = 0.820312\n",
      "[2018-07-16 18:57:11.633786] Iteration 4500, train loss = 0.671132, train accuracy = 0.789062\n",
      "[2018-07-16 18:57:17.782899] Iteration 4600, train loss = 0.607498, train accuracy = 0.820312\n",
      "[2018-07-16 18:57:23.899749] Iteration 4700, train loss = 0.587918, train accuracy = 0.820312\n",
      "[2018-07-16 18:57:30.074275] Iteration 4800, train loss = 0.520372, train accuracy = 0.851562\n",
      "[2018-07-16 18:57:36.267474] Iteration 4900, train loss = 0.651170, train accuracy = 0.789062\n",
      "[2018-07-16 18:57:42.386389] Iteration 5000, train loss = 0.494298, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.811400\n",
      "[2018-07-16 18:57:50.369294] Iteration 5100, train loss = 0.475081, train accuracy = 0.867188\n",
      "[2018-07-16 18:57:56.427269] Iteration 5200, train loss = 0.583941, train accuracy = 0.820312\n",
      "[2018-07-16 18:58:02.578765] Iteration 5300, train loss = 0.596171, train accuracy = 0.804688\n",
      "[2018-07-16 18:58:08.742517] Iteration 5400, train loss = 0.569707, train accuracy = 0.835938\n",
      "[2018-07-16 18:58:14.911532] Iteration 5500, train loss = 0.449246, train accuracy = 0.890625\n",
      "[2018-07-16 18:58:21.056498] Iteration 5600, train loss = 0.546959, train accuracy = 0.835938\n",
      "[2018-07-16 18:58:27.175880] Iteration 5700, train loss = 0.515422, train accuracy = 0.835938\n",
      "[2018-07-16 18:58:33.261692] Iteration 5800, train loss = 0.457751, train accuracy = 0.859375\n",
      "[2018-07-16 18:58:39.413648] Iteration 5900, train loss = 0.402673, train accuracy = 0.898438\n",
      "[2018-07-16 18:58:45.536203] Iteration 6000, train loss = 0.496083, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.800200\n",
      "[2018-07-16 18:58:53.537002] Iteration 6100, train loss = 0.430240, train accuracy = 0.875000\n",
      "[2018-07-16 18:58:59.617752] Iteration 6200, train loss = 0.520086, train accuracy = 0.843750\n",
      "[2018-07-16 18:59:05.718738] Iteration 6300, train loss = 0.349734, train accuracy = 0.929688\n",
      "[2018-07-16 18:59:11.805495] Iteration 6400, train loss = 0.538622, train accuracy = 0.812500\n",
      "[2018-07-16 18:59:17.956531] Iteration 6500, train loss = 0.487719, train accuracy = 0.828125\n",
      "[2018-07-16 18:59:24.103273] Iteration 6600, train loss = 0.607907, train accuracy = 0.789062\n",
      "[2018-07-16 18:59:30.182513] Iteration 6700, train loss = 0.342280, train accuracy = 0.914062\n",
      "[2018-07-16 18:59:36.312180] Iteration 6800, train loss = 0.554253, train accuracy = 0.828125\n",
      "[2018-07-16 18:59:42.429129] Iteration 6900, train loss = 0.600351, train accuracy = 0.812500\n",
      "[2018-07-16 18:59:48.591842] Iteration 7000, train loss = 0.474560, train accuracy = 0.835938\n",
      "Evaluating...\n",
      "Test accuracy = 0.806900\n",
      "[2018-07-16 18:59:56.585830] Iteration 7100, train loss = 0.465683, train accuracy = 0.828125\n",
      "[2018-07-16 19:00:02.709935] Iteration 7200, train loss = 0.489119, train accuracy = 0.882812\n",
      "[2018-07-16 19:00:08.837611] Iteration 7300, train loss = 0.433500, train accuracy = 0.914062\n",
      "[2018-07-16 19:00:14.992851] Iteration 7400, train loss = 0.619489, train accuracy = 0.789062\n",
      "[2018-07-16 19:00:21.108004] Iteration 7500, train loss = 0.566703, train accuracy = 0.843750\n",
      "[2018-07-16 19:00:27.263104] Iteration 7600, train loss = 0.538690, train accuracy = 0.843750\n",
      "[2018-07-16 19:00:33.432729] Iteration 7700, train loss = 0.532693, train accuracy = 0.828125\n",
      "[2018-07-16 19:00:39.511741] Iteration 7800, train loss = 0.414325, train accuracy = 0.875000\n",
      "[2018-07-16 19:00:45.664551] Iteration 7900, train loss = 0.496673, train accuracy = 0.835938\n",
      "[2018-07-16 19:00:51.804110] Iteration 8000, train loss = 0.501772, train accuracy = 0.843750\n",
      "Evaluating...\n",
      "Test accuracy = 0.796300\n",
      "[2018-07-16 19:00:59.824423] Iteration 8100, train loss = 0.697336, train accuracy = 0.789062\n",
      "[2018-07-16 19:01:05.909147] Iteration 8200, train loss = 0.418926, train accuracy = 0.859375\n",
      "[2018-07-16 19:01:12.037238] Iteration 8300, train loss = 0.437533, train accuracy = 0.867188\n",
      "[2018-07-16 19:01:18.090449] Iteration 8400, train loss = 0.517880, train accuracy = 0.882812\n",
      "[2018-07-16 19:01:24.192961] Iteration 8500, train loss = 0.348129, train accuracy = 0.906250\n",
      "[2018-07-16 19:01:30.351736] Iteration 8600, train loss = 0.511633, train accuracy = 0.828125\n",
      "[2018-07-16 19:01:36.517842] Iteration 8700, train loss = 0.487166, train accuracy = 0.867188\n",
      "[2018-07-16 19:01:42.724279] Iteration 8800, train loss = 0.533605, train accuracy = 0.828125\n",
      "[2018-07-16 19:01:48.869388] Iteration 8900, train loss = 0.520030, train accuracy = 0.835938\n",
      "[2018-07-16 19:01:54.919017] Iteration 9000, train loss = 0.406720, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.801400\n",
      "[2018-07-16 19:02:02.856472] Iteration 9100, train loss = 0.453187, train accuracy = 0.859375\n",
      "[2018-07-16 19:02:09.025845] Iteration 9200, train loss = 0.491872, train accuracy = 0.851562\n",
      "[2018-07-16 19:02:15.146274] Iteration 9300, train loss = 0.499240, train accuracy = 0.867188\n",
      "[2018-07-16 19:02:21.311099] Iteration 9400, train loss = 0.539110, train accuracy = 0.859375\n",
      "[2018-07-16 19:02:27.405613] Iteration 9500, train loss = 0.326391, train accuracy = 0.921875\n",
      "[2018-07-16 19:02:33.492563] Iteration 9600, train loss = 0.400240, train accuracy = 0.859375\n",
      "[2018-07-16 19:02:39.677522] Iteration 9700, train loss = 0.578571, train accuracy = 0.828125\n",
      "[2018-07-16 19:02:45.842527] Iteration 9800, train loss = 0.359439, train accuracy = 0.898438\n",
      "[2018-07-16 19:02:52.021745] Iteration 9900, train loss = 0.519545, train accuracy = 0.859375\n",
      "[2018-07-16 19:02:58.119711] Iteration 10000, train loss = 0.493740, train accuracy = 0.804688\n",
      "Evaluating...\n",
      "Test accuracy = 0.826100\n",
      "[2018-07-16 19:03:06.071111] Iteration 10100, train loss = 0.454043, train accuracy = 0.875000\n",
      "[2018-07-16 19:03:12.182807] Iteration 10200, train loss = 0.418893, train accuracy = 0.851562\n",
      "[2018-07-16 19:03:18.321166] Iteration 10300, train loss = 0.410803, train accuracy = 0.859375\n",
      "[2018-07-16 19:03:24.410575] Iteration 10400, train loss = 0.387814, train accuracy = 0.890625\n",
      "[2018-07-16 19:03:30.500407] Iteration 10500, train loss = 0.467054, train accuracy = 0.906250\n",
      "[2018-07-16 19:03:36.600980] Iteration 10600, train loss = 0.483778, train accuracy = 0.851562\n",
      "[2018-07-16 19:03:42.759384] Iteration 10700, train loss = 0.596515, train accuracy = 0.812500\n",
      "[2018-07-16 19:03:48.881429] Iteration 10800, train loss = 0.413567, train accuracy = 0.882812\n",
      "[2018-07-16 19:03:54.961094] Iteration 10900, train loss = 0.390422, train accuracy = 0.882812\n",
      "[2018-07-16 19:04:01.039808] Iteration 11000, train loss = 0.437332, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.792500\n",
      "[2018-07-16 19:04:09.040163] Iteration 11100, train loss = 0.533164, train accuracy = 0.828125\n",
      "[2018-07-16 19:04:15.137572] Iteration 11200, train loss = 0.441690, train accuracy = 0.859375\n",
      "[2018-07-16 19:04:21.229633] Iteration 11300, train loss = 0.460629, train accuracy = 0.851562\n",
      "[2018-07-16 19:04:27.287127] Iteration 11400, train loss = 0.515515, train accuracy = 0.851562\n",
      "[2018-07-16 19:04:33.360170] Iteration 11500, train loss = 0.409947, train accuracy = 0.914062\n",
      "[2018-07-16 19:04:39.491016] Iteration 11600, train loss = 0.567183, train accuracy = 0.820312\n",
      "[2018-07-16 19:04:45.535320] Iteration 11700, train loss = 0.488408, train accuracy = 0.835938\n",
      "[2018-07-16 19:04:51.592808] Iteration 11800, train loss = 0.599975, train accuracy = 0.804688\n",
      "[2018-07-16 19:04:57.705735] Iteration 11900, train loss = 0.544412, train accuracy = 0.828125\n",
      "[2018-07-16 19:05:03.822776] Iteration 12000, train loss = 0.399596, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.798100\n",
      "[2018-07-16 19:05:11.741091] Iteration 12100, train loss = 0.406637, train accuracy = 0.875000\n",
      "[2018-07-16 19:05:17.767593] Iteration 12200, train loss = 0.421569, train accuracy = 0.867188\n",
      "[2018-07-16 19:05:23.877412] Iteration 12300, train loss = 0.501243, train accuracy = 0.851562\n",
      "[2018-07-16 19:05:29.956329] Iteration 12400, train loss = 0.437622, train accuracy = 0.875000\n",
      "[2018-07-16 19:05:36.024669] Iteration 12500, train loss = 0.367693, train accuracy = 0.890625\n",
      "[2018-07-16 19:05:42.120683] Iteration 12600, train loss = 0.426374, train accuracy = 0.867188\n",
      "[2018-07-16 19:05:48.166953] Iteration 12700, train loss = 0.541372, train accuracy = 0.820312\n",
      "[2018-07-16 19:05:54.251581] Iteration 12800, train loss = 0.521717, train accuracy = 0.851562\n",
      "[2018-07-16 19:06:00.316970] Iteration 12900, train loss = 0.647352, train accuracy = 0.812500\n",
      "[2018-07-16 19:06:06.406814] Iteration 13000, train loss = 0.418123, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.833600\n",
      "[2018-07-16 19:06:14.271070] Iteration 13100, train loss = 0.349526, train accuracy = 0.906250\n",
      "[2018-07-16 19:06:20.327373] Iteration 13200, train loss = 0.452089, train accuracy = 0.867188\n",
      "[2018-07-16 19:06:26.402146] Iteration 13300, train loss = 0.445020, train accuracy = 0.882812\n",
      "[2018-07-16 19:06:32.490194] Iteration 13400, train loss = 0.427517, train accuracy = 0.882812\n",
      "[2018-07-16 19:06:38.530848] Iteration 13500, train loss = 0.481292, train accuracy = 0.875000\n",
      "[2018-07-16 19:06:44.625291] Iteration 13600, train loss = 0.373802, train accuracy = 0.898438\n",
      "[2018-07-16 19:06:50.685229] Iteration 13700, train loss = 0.409264, train accuracy = 0.867188\n",
      "[2018-07-16 19:06:56.741133] Iteration 13800, train loss = 0.521738, train accuracy = 0.835938\n",
      "[2018-07-16 19:07:02.843853] Iteration 13900, train loss = 0.446175, train accuracy = 0.843750\n",
      "[2018-07-16 19:07:08.905786] Iteration 14000, train loss = 0.376673, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.839000\n",
      "[2018-07-16 19:07:16.851971] Iteration 14100, train loss = 0.397960, train accuracy = 0.890625\n",
      "[2018-07-16 19:07:22.885182] Iteration 14200, train loss = 0.492186, train accuracy = 0.843750\n",
      "[2018-07-16 19:07:28.915301] Iteration 14300, train loss = 0.463800, train accuracy = 0.835938\n",
      "[2018-07-16 19:07:34.952380] Iteration 14400, train loss = 0.428271, train accuracy = 0.875000\n",
      "[2018-07-16 19:07:40.966227] Iteration 14500, train loss = 0.374937, train accuracy = 0.906250\n",
      "[2018-07-16 19:07:47.153419] Iteration 14600, train loss = 0.444961, train accuracy = 0.859375\n",
      "[2018-07-16 19:07:53.203037] Iteration 14700, train loss = 0.379027, train accuracy = 0.882812\n",
      "[2018-07-16 19:07:59.278706] Iteration 14800, train loss = 0.448580, train accuracy = 0.867188\n",
      "[2018-07-16 19:08:05.282233] Iteration 14900, train loss = 0.430314, train accuracy = 0.898438\n",
      "[2018-07-16 19:08:11.314857] Iteration 15000, train loss = 0.469463, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.807300\n",
      "[2018-07-16 19:08:19.288458] Iteration 15100, train loss = 0.360226, train accuracy = 0.921875\n",
      "[2018-07-16 19:08:25.310425] Iteration 15200, train loss = 0.312110, train accuracy = 0.921875\n",
      "[2018-07-16 19:08:31.349266] Iteration 15300, train loss = 0.486811, train accuracy = 0.882812\n",
      "[2018-07-16 19:08:37.415334] Iteration 15400, train loss = 0.455680, train accuracy = 0.875000\n",
      "[2018-07-16 19:08:43.486646] Iteration 15500, train loss = 0.377576, train accuracy = 0.906250\n",
      "[2018-07-16 19:08:49.513664] Iteration 15600, train loss = 0.516375, train accuracy = 0.851562\n",
      "[2018-07-16 19:08:55.580188] Iteration 15700, train loss = 0.459070, train accuracy = 0.867188\n",
      "[2018-07-16 19:09:01.567063] Iteration 15800, train loss = 0.459942, train accuracy = 0.843750\n",
      "[2018-07-16 19:09:07.650128] Iteration 15900, train loss = 0.370307, train accuracy = 0.875000\n",
      "[2018-07-16 19:09:13.722911] Iteration 16000, train loss = 0.429846, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.809700\n",
      "[2018-07-16 19:09:21.652148] Iteration 16100, train loss = 0.326579, train accuracy = 0.921875\n",
      "[2018-07-16 19:09:27.712570] Iteration 16200, train loss = 0.390850, train accuracy = 0.890625\n",
      "[2018-07-16 19:09:33.753384] Iteration 16300, train loss = 0.394179, train accuracy = 0.898438\n",
      "[2018-07-16 19:09:39.822454] Iteration 16400, train loss = 0.374434, train accuracy = 0.890625\n",
      "[2018-07-16 19:09:45.918833] Iteration 16500, train loss = 0.449862, train accuracy = 0.859375\n",
      "[2018-07-16 19:09:52.037793] Iteration 16600, train loss = 0.295695, train accuracy = 0.945312\n",
      "[2018-07-16 19:09:58.123626] Iteration 16700, train loss = 0.413517, train accuracy = 0.906250\n",
      "[2018-07-16 19:10:04.230695] Iteration 16800, train loss = 0.375977, train accuracy = 0.906250\n",
      "[2018-07-16 19:10:10.256184] Iteration 16900, train loss = 0.308593, train accuracy = 0.929688\n",
      "[2018-07-16 19:10:16.297798] Iteration 17000, train loss = 0.454282, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.822200\n",
      "[2018-07-16 19:10:24.182216] Iteration 17100, train loss = 0.437534, train accuracy = 0.867188\n",
      "[2018-07-16 19:10:30.222751] Iteration 17200, train loss = 0.207521, train accuracy = 0.960938\n",
      "[2018-07-16 19:10:36.294558] Iteration 17300, train loss = 0.325735, train accuracy = 0.898438\n",
      "[2018-07-16 19:10:42.373156] Iteration 17400, train loss = 0.506188, train accuracy = 0.835938\n",
      "[2018-07-16 19:10:48.428390] Iteration 17500, train loss = 0.470740, train accuracy = 0.859375\n",
      "[2018-07-16 19:10:54.487763] Iteration 17600, train loss = 0.386974, train accuracy = 0.906250\n",
      "[2018-07-16 19:11:00.541183] Iteration 17700, train loss = 0.299528, train accuracy = 0.960938\n",
      "[2018-07-16 19:11:06.617326] Iteration 17800, train loss = 0.395274, train accuracy = 0.875000\n",
      "[2018-07-16 19:11:12.701703] Iteration 17900, train loss = 0.360141, train accuracy = 0.867188\n",
      "[2018-07-16 19:11:18.836339] Iteration 18000, train loss = 0.362733, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.821800\n",
      "[2018-07-16 19:11:26.787660] Iteration 18100, train loss = 0.374699, train accuracy = 0.882812\n",
      "[2018-07-16 19:11:32.868054] Iteration 18200, train loss = 0.368618, train accuracy = 0.929688\n",
      "[2018-07-16 19:11:38.905120] Iteration 18300, train loss = 0.318173, train accuracy = 0.937500\n",
      "[2018-07-16 19:11:45.023390] Iteration 18400, train loss = 0.341200, train accuracy = 0.882812\n",
      "[2018-07-16 19:11:51.125236] Iteration 18500, train loss = 0.502280, train accuracy = 0.859375\n",
      "[2018-07-16 19:11:57.208177] Iteration 18600, train loss = 0.374018, train accuracy = 0.906250\n",
      "[2018-07-16 19:12:03.244805] Iteration 18700, train loss = 0.378938, train accuracy = 0.890625\n",
      "[2018-07-16 19:12:09.340874] Iteration 18800, train loss = 0.483124, train accuracy = 0.843750\n",
      "[2018-07-16 19:12:15.414785] Iteration 18900, train loss = 0.340882, train accuracy = 0.898438\n",
      "[2018-07-16 19:12:21.492695] Iteration 19000, train loss = 0.446543, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.801100\n",
      "[2018-07-16 19:12:29.452602] Iteration 19100, train loss = 0.289296, train accuracy = 0.921875\n",
      "[2018-07-16 19:12:35.536587] Iteration 19200, train loss = 0.412063, train accuracy = 0.882812\n",
      "[2018-07-16 19:12:41.660406] Iteration 19300, train loss = 0.363884, train accuracy = 0.914062\n",
      "[2018-07-16 19:12:47.809279] Iteration 19400, train loss = 0.330979, train accuracy = 0.929688\n",
      "[2018-07-16 19:12:53.900548] Iteration 19500, train loss = 0.293908, train accuracy = 0.937500\n",
      "[2018-07-16 19:13:00.003298] Iteration 19600, train loss = 0.471442, train accuracy = 0.875000\n",
      "[2018-07-16 19:13:06.134826] Iteration 19700, train loss = 0.740335, train accuracy = 0.812500\n",
      "[2018-07-16 19:13:12.261330] Iteration 19800, train loss = 0.348112, train accuracy = 0.914062\n",
      "[2018-07-16 19:13:18.369047] Iteration 19900, train loss = 0.469335, train accuracy = 0.859375\n",
      "[2018-07-16 19:13:24.510706] Iteration 20000, train loss = 0.325901, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.823500\n",
      "[2018-07-16 19:13:32.454632] Iteration 20100, train loss = 0.482001, train accuracy = 0.890625\n",
      "[2018-07-16 19:13:38.599176] Iteration 20200, train loss = 0.388343, train accuracy = 0.859375\n",
      "[2018-07-16 19:13:44.722469] Iteration 20300, train loss = 0.350616, train accuracy = 0.898438\n",
      "[2018-07-16 19:13:50.826711] Iteration 20400, train loss = 0.300643, train accuracy = 0.914062\n",
      "[2018-07-16 19:13:56.901204] Iteration 20500, train loss = 0.466186, train accuracy = 0.890625\n",
      "[2018-07-16 19:14:03.023240] Iteration 20600, train loss = 0.417796, train accuracy = 0.890625\n",
      "[2018-07-16 19:14:09.094580] Iteration 20700, train loss = 0.354183, train accuracy = 0.914062\n",
      "[2018-07-16 19:14:15.199230] Iteration 20800, train loss = 0.394433, train accuracy = 0.859375\n",
      "[2018-07-16 19:14:21.351040] Iteration 20900, train loss = 0.413969, train accuracy = 0.875000\n",
      "[2018-07-16 19:14:27.460042] Iteration 21000, train loss = 0.397914, train accuracy = 0.859375\n",
      "Evaluating...\n",
      "Test accuracy = 0.841500\n",
      "[2018-07-16 19:14:35.374951] Iteration 21100, train loss = 0.419580, train accuracy = 0.882812\n",
      "[2018-07-16 19:14:41.525242] Iteration 21200, train loss = 0.418977, train accuracy = 0.898438\n",
      "[2018-07-16 19:14:47.689701] Iteration 21300, train loss = 0.327059, train accuracy = 0.929688\n",
      "[2018-07-16 19:14:53.841055] Iteration 21400, train loss = 0.460395, train accuracy = 0.898438\n",
      "[2018-07-16 19:14:59.987722] Iteration 21500, train loss = 0.426735, train accuracy = 0.898438\n",
      "[2018-07-16 19:15:06.112839] Iteration 21600, train loss = 0.416281, train accuracy = 0.890625\n",
      "[2018-07-16 19:15:12.258655] Iteration 21700, train loss = 0.373924, train accuracy = 0.898438\n",
      "[2018-07-16 19:15:18.376341] Iteration 21800, train loss = 0.383501, train accuracy = 0.906250\n",
      "[2018-07-16 19:15:24.499778] Iteration 21900, train loss = 0.390945, train accuracy = 0.921875\n",
      "[2018-07-16 19:15:30.637614] Iteration 22000, train loss = 0.461670, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.804700\n",
      "[2018-07-16 19:15:38.551826] Iteration 22100, train loss = 0.360975, train accuracy = 0.898438\n",
      "[2018-07-16 19:15:44.692807] Iteration 22200, train loss = 0.347119, train accuracy = 0.890625\n",
      "[2018-07-16 19:15:50.799300] Iteration 22300, train loss = 0.256297, train accuracy = 0.937500\n",
      "[2018-07-16 19:15:56.885176] Iteration 22400, train loss = 0.480517, train accuracy = 0.859375\n",
      "[2018-07-16 19:16:02.956102] Iteration 22500, train loss = 0.315822, train accuracy = 0.906250\n",
      "[2018-07-16 19:16:09.024776] Iteration 22600, train loss = 0.252219, train accuracy = 0.906250\n",
      "[2018-07-16 19:16:15.143596] Iteration 22700, train loss = 0.325967, train accuracy = 0.914062\n",
      "[2018-07-16 19:16:21.247857] Iteration 22800, train loss = 0.371663, train accuracy = 0.906250\n",
      "[2018-07-16 19:16:27.336525] Iteration 22900, train loss = 0.441399, train accuracy = 0.867188\n",
      "[2018-07-16 19:16:33.458445] Iteration 23000, train loss = 0.520644, train accuracy = 0.812500\n",
      "Evaluating...\n",
      "Test accuracy = 0.837500\n",
      "[2018-07-16 19:16:41.461399] Iteration 23100, train loss = 0.437634, train accuracy = 0.851562\n",
      "[2018-07-16 19:16:47.513656] Iteration 23200, train loss = 0.342762, train accuracy = 0.898438\n",
      "[2018-07-16 19:16:53.634719] Iteration 23300, train loss = 0.338444, train accuracy = 0.929688\n",
      "[2018-07-16 19:16:59.757702] Iteration 23400, train loss = 0.425750, train accuracy = 0.859375\n",
      "[2018-07-16 19:17:05.914143] Iteration 23500, train loss = 0.420167, train accuracy = 0.906250\n",
      "[2018-07-16 19:17:12.060746] Iteration 23600, train loss = 0.369864, train accuracy = 0.921875\n",
      "[2018-07-16 19:17:18.170897] Iteration 23700, train loss = 0.409625, train accuracy = 0.882812\n",
      "[2018-07-16 19:17:24.262886] Iteration 23800, train loss = 0.304333, train accuracy = 0.914062\n",
      "[2018-07-16 19:17:30.409314] Iteration 23900, train loss = 0.263168, train accuracy = 0.945312\n",
      "[2018-07-16 19:17:36.532821] Iteration 24000, train loss = 0.406419, train accuracy = 0.867188\n",
      "Evaluating...\n",
      "Test accuracy = 0.844600\n",
      "[2018-07-16 19:17:44.559817] Iteration 24100, train loss = 0.436153, train accuracy = 0.867188\n",
      "[2018-07-16 19:17:50.657376] Iteration 24200, train loss = 0.410803, train accuracy = 0.890625\n",
      "[2018-07-16 19:17:56.755983] Iteration 24300, train loss = 0.427180, train accuracy = 0.882812\n",
      "[2018-07-16 19:18:02.822525] Iteration 24400, train loss = 0.378269, train accuracy = 0.867188\n",
      "[2018-07-16 19:18:08.875727] Iteration 24500, train loss = 0.367146, train accuracy = 0.906250\n",
      "[2018-07-16 19:18:14.989972] Iteration 24600, train loss = 0.360502, train accuracy = 0.898438\n",
      "[2018-07-16 19:18:21.057223] Iteration 24700, train loss = 0.409913, train accuracy = 0.867188\n",
      "[2018-07-16 19:18:27.178167] Iteration 24800, train loss = 0.604581, train accuracy = 0.828125\n",
      "[2018-07-16 19:18:33.224249] Iteration 24900, train loss = 0.426925, train accuracy = 0.882812\n",
      "[2018-07-16 19:18:39.398005] Iteration 25000, train loss = 0.252894, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.823700\n",
      "[2018-07-16 19:18:47.439612] Iteration 25100, train loss = 0.455560, train accuracy = 0.859375\n",
      "[2018-07-16 19:18:53.456033] Iteration 25200, train loss = 0.404194, train accuracy = 0.875000\n",
      "[2018-07-16 19:18:59.549335] Iteration 25300, train loss = 0.334662, train accuracy = 0.937500\n",
      "[2018-07-16 19:19:05.616316] Iteration 25400, train loss = 0.451207, train accuracy = 0.890625\n",
      "[2018-07-16 19:19:11.699069] Iteration 25500, train loss = 0.361446, train accuracy = 0.882812\n",
      "[2018-07-16 19:19:17.823969] Iteration 25600, train loss = 0.396772, train accuracy = 0.882812\n",
      "[2018-07-16 19:19:23.959647] Iteration 25700, train loss = 0.343417, train accuracy = 0.921875\n",
      "[2018-07-16 19:19:30.014578] Iteration 25800, train loss = 0.342445, train accuracy = 0.921875\n",
      "[2018-07-16 19:19:36.093139] Iteration 25900, train loss = 0.398881, train accuracy = 0.906250\n",
      "[2018-07-16 19:19:42.166702] Iteration 26000, train loss = 0.390348, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.838200\n",
      "[2018-07-16 19:19:50.147342] Iteration 26100, train loss = 0.524146, train accuracy = 0.828125\n",
      "[2018-07-16 19:19:56.202059] Iteration 26200, train loss = 0.351246, train accuracy = 0.890625\n",
      "[2018-07-16 19:20:02.262685] Iteration 26300, train loss = 0.280638, train accuracy = 0.937500\n",
      "[2018-07-16 19:20:08.326853] Iteration 26400, train loss = 0.377647, train accuracy = 0.890625\n",
      "[2018-07-16 19:20:14.450189] Iteration 26500, train loss = 0.260266, train accuracy = 0.945312\n",
      "[2018-07-16 19:20:20.496884] Iteration 26600, train loss = 0.395515, train accuracy = 0.875000\n",
      "[2018-07-16 19:20:26.585245] Iteration 26700, train loss = 0.290422, train accuracy = 0.945312\n",
      "[2018-07-16 19:20:32.678039] Iteration 26800, train loss = 0.483059, train accuracy = 0.859375\n",
      "[2018-07-16 19:20:38.770577] Iteration 26900, train loss = 0.421955, train accuracy = 0.898438\n",
      "[2018-07-16 19:20:44.844339] Iteration 27000, train loss = 0.268278, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.844800\n",
      "[2018-07-16 19:20:52.794341] Iteration 27100, train loss = 0.482082, train accuracy = 0.867188\n",
      "[2018-07-16 19:20:58.867298] Iteration 27200, train loss = 0.414796, train accuracy = 0.867188\n",
      "[2018-07-16 19:21:04.980638] Iteration 27300, train loss = 0.306167, train accuracy = 0.929688\n",
      "[2018-07-16 19:21:11.037547] Iteration 27400, train loss = 0.302458, train accuracy = 0.906250\n",
      "[2018-07-16 19:21:17.055022] Iteration 27500, train loss = 0.462987, train accuracy = 0.875000\n",
      "[2018-07-16 19:21:23.211450] Iteration 27600, train loss = 0.333314, train accuracy = 0.921875\n",
      "[2018-07-16 19:21:29.284377] Iteration 27700, train loss = 0.349690, train accuracy = 0.906250\n",
      "[2018-07-16 19:21:35.435432] Iteration 27800, train loss = 0.529786, train accuracy = 0.851562\n",
      "[2018-07-16 19:21:41.557433] Iteration 27900, train loss = 0.442126, train accuracy = 0.875000\n",
      "[2018-07-16 19:21:47.684088] Iteration 28000, train loss = 0.344463, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.822000\n",
      "[2018-07-16 19:21:55.632464] Iteration 28100, train loss = 0.438269, train accuracy = 0.875000\n",
      "[2018-07-16 19:22:01.719351] Iteration 28200, train loss = 0.431072, train accuracy = 0.875000\n",
      "[2018-07-16 19:22:07.849895] Iteration 28300, train loss = 0.417954, train accuracy = 0.882812\n",
      "[2018-07-16 19:22:13.916617] Iteration 28400, train loss = 0.340114, train accuracy = 0.929688\n",
      "[2018-07-16 19:22:19.957844] Iteration 28500, train loss = 0.424720, train accuracy = 0.867188\n",
      "[2018-07-16 19:22:26.093387] Iteration 28600, train loss = 0.372635, train accuracy = 0.906250\n",
      "[2018-07-16 19:22:32.200399] Iteration 28700, train loss = 0.362786, train accuracy = 0.898438\n",
      "[2018-07-16 19:22:38.339152] Iteration 28800, train loss = 0.355373, train accuracy = 0.898438\n",
      "[2018-07-16 19:22:44.470272] Iteration 28900, train loss = 0.484134, train accuracy = 0.851562\n",
      "[2018-07-16 19:22:50.533912] Iteration 29000, train loss = 0.460153, train accuracy = 0.851562\n",
      "Evaluating...\n",
      "Test accuracy = 0.819500\n",
      "[2018-07-16 19:22:58.482804] Iteration 29100, train loss = 0.393852, train accuracy = 0.882812\n",
      "[2018-07-16 19:23:04.557216] Iteration 29200, train loss = 0.347603, train accuracy = 0.929688\n",
      "[2018-07-16 19:23:10.629873] Iteration 29300, train loss = 0.446760, train accuracy = 0.859375\n",
      "[2018-07-16 19:23:16.716957] Iteration 29400, train loss = 0.379730, train accuracy = 0.890625\n",
      "[2018-07-16 19:23:22.799378] Iteration 29500, train loss = 0.364509, train accuracy = 0.937500\n",
      "[2018-07-16 19:23:28.915083] Iteration 29600, train loss = 0.398164, train accuracy = 0.890625\n",
      "[2018-07-16 19:23:35.097743] Iteration 29700, train loss = 0.362245, train accuracy = 0.914062\n",
      "[2018-07-16 19:23:41.158496] Iteration 29800, train loss = 0.365518, train accuracy = 0.914062\n",
      "[2018-07-16 19:23:47.246455] Iteration 29900, train loss = 0.306012, train accuracy = 0.898438\n",
      "[2018-07-16 19:23:53.324936] Iteration 30000, train loss = 0.307659, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.848400\n",
      "[2018-07-16 19:24:01.329283] Iteration 30100, train loss = 0.362660, train accuracy = 0.921875\n",
      "[2018-07-16 19:24:07.431414] Iteration 30200, train loss = 0.351819, train accuracy = 0.914062\n",
      "[2018-07-16 19:24:13.524241] Iteration 30300, train loss = 0.378582, train accuracy = 0.875000\n",
      "[2018-07-16 19:24:19.691083] Iteration 30400, train loss = 0.297395, train accuracy = 0.945312\n",
      "[2018-07-16 19:24:25.871522] Iteration 30500, train loss = 0.467407, train accuracy = 0.875000\n",
      "[2018-07-16 19:24:31.986478] Iteration 30600, train loss = 0.511821, train accuracy = 0.875000\n",
      "[2018-07-16 19:24:38.086218] Iteration 30700, train loss = 0.325772, train accuracy = 0.914062\n",
      "[2018-07-16 19:24:44.240982] Iteration 30800, train loss = 0.318049, train accuracy = 0.890625\n",
      "[2018-07-16 19:24:50.305628] Iteration 30900, train loss = 0.328613, train accuracy = 0.890625\n",
      "[2018-07-16 19:24:56.449225] Iteration 31000, train loss = 0.377325, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.851500\n",
      "[2018-07-16 19:25:04.422145] Iteration 31100, train loss = 0.287978, train accuracy = 0.929688\n",
      "[2018-07-16 19:25:10.537697] Iteration 31200, train loss = 0.346471, train accuracy = 0.890625\n",
      "[2018-07-16 19:25:16.623381] Iteration 31300, train loss = 0.406752, train accuracy = 0.875000\n",
      "[2018-07-16 19:25:22.784590] Iteration 31400, train loss = 0.356354, train accuracy = 0.875000\n",
      "[2018-07-16 19:25:28.913749] Iteration 31500, train loss = 0.376360, train accuracy = 0.859375\n",
      "[2018-07-16 19:25:35.046673] Iteration 31600, train loss = 0.393127, train accuracy = 0.898438\n",
      "[2018-07-16 19:25:41.153693] Iteration 31700, train loss = 0.402554, train accuracy = 0.882812\n",
      "[2018-07-16 19:25:47.294515] Iteration 31800, train loss = 0.361994, train accuracy = 0.898438\n",
      "[2018-07-16 19:25:53.435854] Iteration 31900, train loss = 0.422620, train accuracy = 0.898438\n",
      "[2018-07-16 19:25:59.599312] Iteration 32000, train loss = 0.345293, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.830000\n",
      "[2018-07-16 19:26:07.500161] Iteration 32100, train loss = 0.293274, train accuracy = 0.914062\n",
      "[2018-07-16 19:26:13.627524] Iteration 32200, train loss = 0.359969, train accuracy = 0.882812\n",
      "[2018-07-16 19:26:19.743703] Iteration 32300, train loss = 0.409724, train accuracy = 0.882812\n",
      "[2018-07-16 19:26:25.906148] Iteration 32400, train loss = 0.380391, train accuracy = 0.859375\n",
      "[2018-07-16 19:26:32.065668] Iteration 32500, train loss = 0.278053, train accuracy = 0.953125\n",
      "[2018-07-16 19:26:38.222313] Iteration 32600, train loss = 0.371860, train accuracy = 0.890625\n",
      "[2018-07-16 19:26:44.336591] Iteration 32700, train loss = 0.300186, train accuracy = 0.929688\n",
      "[2018-07-16 19:26:50.502710] Iteration 32800, train loss = 0.337878, train accuracy = 0.890625\n",
      "[2018-07-16 19:26:56.651852] Iteration 32900, train loss = 0.301445, train accuracy = 0.906250\n",
      "[2018-07-16 19:27:02.759008] Iteration 33000, train loss = 0.398335, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.837300\n",
      "[2018-07-16 19:27:10.724367] Iteration 33100, train loss = 0.378189, train accuracy = 0.875000\n",
      "[2018-07-16 19:27:16.880739] Iteration 33200, train loss = 0.397928, train accuracy = 0.921875\n",
      "[2018-07-16 19:27:23.046455] Iteration 33300, train loss = 0.256690, train accuracy = 0.929688\n",
      "[2018-07-16 19:27:29.178036] Iteration 33400, train loss = 0.361450, train accuracy = 0.914062\n",
      "[2018-07-16 19:27:35.323202] Iteration 33500, train loss = 0.302926, train accuracy = 0.929688\n",
      "[2018-07-16 19:27:41.413885] Iteration 33600, train loss = 0.404379, train accuracy = 0.906250\n",
      "[2018-07-16 19:27:47.529883] Iteration 33700, train loss = 0.240585, train accuracy = 0.937500\n",
      "[2018-07-16 19:27:53.680153] Iteration 33800, train loss = 0.315753, train accuracy = 0.890625\n",
      "[2018-07-16 19:27:59.776064] Iteration 33900, train loss = 0.427646, train accuracy = 0.906250\n",
      "[2018-07-16 19:28:05.877057] Iteration 34000, train loss = 0.313629, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.833100\n",
      "[2018-07-16 19:28:13.880695] Iteration 34100, train loss = 0.289332, train accuracy = 0.937500\n",
      "[2018-07-16 19:28:20.020248] Iteration 34200, train loss = 0.296994, train accuracy = 0.937500\n",
      "[2018-07-16 19:28:26.200261] Iteration 34300, train loss = 0.322573, train accuracy = 0.937500\n",
      "[2018-07-16 19:28:32.349710] Iteration 34400, train loss = 0.515492, train accuracy = 0.835938\n",
      "[2018-07-16 19:28:38.513573] Iteration 34500, train loss = 0.400686, train accuracy = 0.890625\n",
      "[2018-07-16 19:28:44.652208] Iteration 34600, train loss = 0.368095, train accuracy = 0.859375\n",
      "[2018-07-16 19:28:50.799866] Iteration 34700, train loss = 0.355410, train accuracy = 0.921875\n",
      "[2018-07-16 19:28:56.947513] Iteration 34800, train loss = 0.316981, train accuracy = 0.906250\n",
      "[2018-07-16 19:29:03.083875] Iteration 34900, train loss = 0.290147, train accuracy = 0.921875\n",
      "[2018-07-16 19:29:09.147092] Iteration 35000, train loss = 0.436010, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.843400\n",
      "[2018-07-16 19:29:17.128510] Iteration 35100, train loss = 0.377899, train accuracy = 0.890625\n",
      "[2018-07-16 19:29:23.335375] Iteration 35200, train loss = 0.407536, train accuracy = 0.890625\n",
      "[2018-07-16 19:29:29.429849] Iteration 35300, train loss = 0.336606, train accuracy = 0.898438\n",
      "[2018-07-16 19:29:35.625183] Iteration 35400, train loss = 0.448247, train accuracy = 0.875000\n",
      "[2018-07-16 19:29:41.677808] Iteration 35500, train loss = 0.502419, train accuracy = 0.812500\n",
      "[2018-07-16 19:29:47.870561] Iteration 35600, train loss = 0.314955, train accuracy = 0.929688\n",
      "[2018-07-16 19:29:54.074489] Iteration 35700, train loss = 0.299470, train accuracy = 0.937500\n",
      "[2018-07-16 19:30:00.243089] Iteration 35800, train loss = 0.297151, train accuracy = 0.921875\n",
      "[2018-07-16 19:30:06.419257] Iteration 35900, train loss = 0.355856, train accuracy = 0.914062\n",
      "[2018-07-16 19:30:12.624490] Iteration 36000, train loss = 0.350183, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.836200\n",
      "[2018-07-16 19:30:20.633447] Iteration 36100, train loss = 0.380283, train accuracy = 0.921875\n",
      "[2018-07-16 19:30:26.738367] Iteration 36200, train loss = 0.444722, train accuracy = 0.859375\n",
      "[2018-07-16 19:30:32.916800] Iteration 36300, train loss = 0.300281, train accuracy = 0.914062\n",
      "[2018-07-16 19:30:39.089446] Iteration 36400, train loss = 0.428356, train accuracy = 0.867188\n",
      "[2018-07-16 19:30:45.206054] Iteration 36500, train loss = 0.349576, train accuracy = 0.906250\n",
      "[2018-07-16 19:30:51.360978] Iteration 36600, train loss = 0.318218, train accuracy = 0.898438\n",
      "[2018-07-16 19:30:57.544477] Iteration 36700, train loss = 0.273181, train accuracy = 0.945312\n",
      "[2018-07-16 19:31:03.723976] Iteration 36800, train loss = 0.302646, train accuracy = 0.914062\n",
      "[2018-07-16 19:31:09.909941] Iteration 36900, train loss = 0.295723, train accuracy = 0.921875\n",
      "[2018-07-16 19:31:16.100079] Iteration 37000, train loss = 0.336026, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.843900\n",
      "[2018-07-16 19:31:24.155650] Iteration 37100, train loss = 0.341010, train accuracy = 0.867188\n",
      "[2018-07-16 19:31:30.341091] Iteration 37200, train loss = 0.354643, train accuracy = 0.906250\n",
      "[2018-07-16 19:31:36.536510] Iteration 37300, train loss = 0.324392, train accuracy = 0.906250\n",
      "[2018-07-16 19:31:42.694240] Iteration 37400, train loss = 0.243711, train accuracy = 0.976562\n",
      "[2018-07-16 19:31:48.853041] Iteration 37500, train loss = 0.395225, train accuracy = 0.898438\n",
      "[2018-07-16 19:31:55.009625] Iteration 37600, train loss = 0.382367, train accuracy = 0.898438\n",
      "[2018-07-16 19:32:01.202100] Iteration 37700, train loss = 0.358948, train accuracy = 0.890625\n",
      "[2018-07-16 19:32:07.350326] Iteration 37800, train loss = 0.339055, train accuracy = 0.890625\n",
      "[2018-07-16 19:32:13.552332] Iteration 37900, train loss = 0.462484, train accuracy = 0.859375\n",
      "[2018-07-16 19:32:19.746207] Iteration 38000, train loss = 0.366005, train accuracy = 0.882812\n",
      "Evaluating...\n",
      "Test accuracy = 0.847600\n",
      "[2018-07-16 19:32:27.757707] Iteration 38100, train loss = 0.316971, train accuracy = 0.929688\n",
      "[2018-07-16 19:32:33.875972] Iteration 38200, train loss = 0.321031, train accuracy = 0.929688\n",
      "[2018-07-16 19:32:40.015437] Iteration 38300, train loss = 0.394114, train accuracy = 0.890625\n",
      "[2018-07-16 19:32:46.189770] Iteration 38400, train loss = 0.295536, train accuracy = 0.914062\n",
      "[2018-07-16 19:32:52.313298] Iteration 38500, train loss = 0.409454, train accuracy = 0.867188\n",
      "[2018-07-16 19:32:58.434153] Iteration 38600, train loss = 0.252751, train accuracy = 0.953125\n",
      "[2018-07-16 19:33:04.556479] Iteration 38700, train loss = 0.349933, train accuracy = 0.906250\n",
      "[2018-07-16 19:33:10.729512] Iteration 38800, train loss = 0.290223, train accuracy = 0.945312\n",
      "[2018-07-16 19:33:16.931642] Iteration 38900, train loss = 0.377374, train accuracy = 0.914062\n",
      "[2018-07-16 19:33:23.095353] Iteration 39000, train loss = 0.314988, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.835400\n",
      "[2018-07-16 19:33:31.138977] Iteration 39100, train loss = 0.328413, train accuracy = 0.937500\n",
      "[2018-07-16 19:33:37.301389] Iteration 39200, train loss = 0.290546, train accuracy = 0.914062\n",
      "[2018-07-16 19:33:43.472087] Iteration 39300, train loss = 0.309529, train accuracy = 0.953125\n",
      "[2018-07-16 19:33:49.610970] Iteration 39400, train loss = 0.284627, train accuracy = 0.929688\n",
      "[2018-07-16 19:33:55.751114] Iteration 39500, train loss = 0.412825, train accuracy = 0.882812\n",
      "[2018-07-16 19:34:01.943388] Iteration 39600, train loss = 0.343455, train accuracy = 0.898438\n",
      "[2018-07-16 19:34:08.133554] Iteration 39700, train loss = 0.367355, train accuracy = 0.906250\n",
      "[2018-07-16 19:34:14.316131] Iteration 39800, train loss = 0.439986, train accuracy = 0.898438\n",
      "[2018-07-16 19:34:20.476812] Iteration 39900, train loss = 0.370576, train accuracy = 0.914062\n",
      "[2018-07-16 19:34:26.655291] Iteration 40000, train loss = 0.295317, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.842100\n",
      "[2018-07-16 19:34:34.664193] Iteration 40100, train loss = 0.272286, train accuracy = 0.945312\n",
      "[2018-07-16 19:34:40.811959] Iteration 40200, train loss = 0.320138, train accuracy = 0.914062\n",
      "[2018-07-16 19:34:47.000674] Iteration 40300, train loss = 0.393055, train accuracy = 0.843750\n",
      "[2018-07-16 19:34:53.173286] Iteration 40400, train loss = 0.273773, train accuracy = 0.953125\n",
      "[2018-07-16 19:34:59.351538] Iteration 40500, train loss = 0.288110, train accuracy = 0.937500\n",
      "[2018-07-16 19:35:05.528483] Iteration 40600, train loss = 0.268544, train accuracy = 0.953125\n",
      "[2018-07-16 19:35:11.736811] Iteration 40700, train loss = 0.360123, train accuracy = 0.898438\n",
      "[2018-07-16 19:35:17.912199] Iteration 40800, train loss = 0.363799, train accuracy = 0.882812\n",
      "[2018-07-16 19:35:24.041076] Iteration 40900, train loss = 0.298215, train accuracy = 0.937500\n",
      "[2018-07-16 19:35:30.179782] Iteration 41000, train loss = 0.355746, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.830800\n",
      "[2018-07-16 19:35:38.198700] Iteration 41100, train loss = 0.432870, train accuracy = 0.875000\n",
      "[2018-07-16 19:35:44.341780] Iteration 41200, train loss = 0.362696, train accuracy = 0.914062\n",
      "[2018-07-16 19:35:50.500685] Iteration 41300, train loss = 0.394347, train accuracy = 0.882812\n",
      "[2018-07-16 19:35:56.667357] Iteration 41400, train loss = 0.335378, train accuracy = 0.921875\n",
      "[2018-07-16 19:36:02.832982] Iteration 41500, train loss = 0.365811, train accuracy = 0.914062\n",
      "[2018-07-16 19:36:08.988664] Iteration 41600, train loss = 0.386144, train accuracy = 0.906250\n",
      "[2018-07-16 19:36:15.052281] Iteration 41700, train loss = 0.328528, train accuracy = 0.906250\n",
      "[2018-07-16 19:36:21.054222] Iteration 41800, train loss = 0.385123, train accuracy = 0.906250\n",
      "[2018-07-16 19:36:27.241370] Iteration 41900, train loss = 0.303155, train accuracy = 0.921875\n",
      "[2018-07-16 19:36:33.403105] Iteration 42000, train loss = 0.337513, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.840300\n",
      "[2018-07-16 19:36:41.414556] Iteration 42100, train loss = 0.365314, train accuracy = 0.898438\n",
      "[2018-07-16 19:36:47.601783] Iteration 42200, train loss = 0.356582, train accuracy = 0.914062\n",
      "[2018-07-16 19:36:53.789771] Iteration 42300, train loss = 0.343019, train accuracy = 0.921875\n",
      "[2018-07-16 19:36:59.841756] Iteration 42400, train loss = 0.263901, train accuracy = 0.945312\n",
      "[2018-07-16 19:37:06.060364] Iteration 42500, train loss = 0.303834, train accuracy = 0.906250\n",
      "[2018-07-16 19:37:12.139152] Iteration 42600, train loss = 0.455065, train accuracy = 0.882812\n",
      "[2018-07-16 19:37:18.334997] Iteration 42700, train loss = 0.356175, train accuracy = 0.906250\n",
      "[2018-07-16 19:37:24.533997] Iteration 42800, train loss = 0.329035, train accuracy = 0.921875\n",
      "[2018-07-16 19:37:30.705142] Iteration 42900, train loss = 0.281055, train accuracy = 0.953125\n",
      "[2018-07-16 19:37:36.871663] Iteration 43000, train loss = 0.303658, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.851100\n",
      "[2018-07-16 19:37:44.854115] Iteration 43100, train loss = 0.377402, train accuracy = 0.867188\n",
      "[2018-07-16 19:37:50.974496] Iteration 43200, train loss = 0.202547, train accuracy = 0.968750\n",
      "[2018-07-16 19:37:57.117913] Iteration 43300, train loss = 0.480750, train accuracy = 0.843750\n",
      "[2018-07-16 19:38:03.255435] Iteration 43400, train loss = 0.365879, train accuracy = 0.906250\n",
      "[2018-07-16 19:38:09.439380] Iteration 43500, train loss = 0.401201, train accuracy = 0.906250\n",
      "[2018-07-16 19:38:15.617981] Iteration 43600, train loss = 0.420985, train accuracy = 0.890625\n",
      "[2018-07-16 19:38:21.779251] Iteration 43700, train loss = 0.258859, train accuracy = 0.929688\n",
      "[2018-07-16 19:38:27.940465] Iteration 43800, train loss = 0.226461, train accuracy = 0.968750\n",
      "[2018-07-16 19:38:34.089738] Iteration 43900, train loss = 0.313640, train accuracy = 0.929688\n",
      "[2018-07-16 19:38:40.251176] Iteration 44000, train loss = 0.427268, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.851500\n",
      "[2018-07-16 19:38:48.258209] Iteration 44100, train loss = 0.331344, train accuracy = 0.906250\n",
      "[2018-07-16 19:38:54.404080] Iteration 44200, train loss = 0.377486, train accuracy = 0.898438\n",
      "[2018-07-16 19:39:00.498606] Iteration 44300, train loss = 0.424147, train accuracy = 0.882812\n",
      "[2018-07-16 19:39:06.710738] Iteration 44400, train loss = 0.329471, train accuracy = 0.921875\n",
      "[2018-07-16 19:39:12.843882] Iteration 44500, train loss = 0.354841, train accuracy = 0.898438\n",
      "[2018-07-16 19:39:19.017387] Iteration 44600, train loss = 0.280264, train accuracy = 0.945312\n",
      "[2018-07-16 19:39:25.154318] Iteration 44700, train loss = 0.182498, train accuracy = 0.976562\n",
      "[2018-07-16 19:39:31.307948] Iteration 44800, train loss = 0.281589, train accuracy = 0.937500\n",
      "[2018-07-16 19:39:37.443384] Iteration 44900, train loss = 0.330660, train accuracy = 0.906250\n",
      "[2018-07-16 19:39:43.607481] Iteration 45000, train loss = 0.322720, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.845800\n",
      "[2018-07-16 19:39:51.580032] Iteration 45100, train loss = 0.443397, train accuracy = 0.882812\n",
      "[2018-07-16 19:39:57.666977] Iteration 45200, train loss = 0.361605, train accuracy = 0.906250\n",
      "[2018-07-16 19:40:03.859488] Iteration 45300, train loss = 0.387699, train accuracy = 0.898438\n",
      "[2018-07-16 19:40:09.976720] Iteration 45400, train loss = 0.307808, train accuracy = 0.921875\n",
      "[2018-07-16 19:40:16.068192] Iteration 45500, train loss = 0.250732, train accuracy = 0.929688\n",
      "[2018-07-16 19:40:22.240686] Iteration 45600, train loss = 0.377382, train accuracy = 0.906250\n",
      "[2018-07-16 19:40:28.301056] Iteration 45700, train loss = 0.342614, train accuracy = 0.914062\n",
      "[2018-07-16 19:40:34.456375] Iteration 45800, train loss = 0.491914, train accuracy = 0.851562\n",
      "[2018-07-16 19:40:40.571321] Iteration 45900, train loss = 0.552047, train accuracy = 0.835938\n",
      "[2018-07-16 19:40:46.713028] Iteration 46000, train loss = 0.241846, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.845500\n",
      "[2018-07-16 19:40:54.721930] Iteration 46100, train loss = 0.309594, train accuracy = 0.937500\n",
      "[2018-07-16 19:41:00.813298] Iteration 46200, train loss = 0.312526, train accuracy = 0.921875\n",
      "[2018-07-16 19:41:06.975635] Iteration 46300, train loss = 0.383992, train accuracy = 0.929688\n",
      "[2018-07-16 19:41:13.114278] Iteration 46400, train loss = 0.285507, train accuracy = 0.937500\n",
      "[2018-07-16 19:41:19.242029] Iteration 46500, train loss = 0.395999, train accuracy = 0.914062\n",
      "[2018-07-16 19:41:25.337182] Iteration 46600, train loss = 0.366213, train accuracy = 0.914062\n",
      "[2018-07-16 19:41:31.470007] Iteration 46700, train loss = 0.290251, train accuracy = 0.953125\n",
      "[2018-07-16 19:41:37.577200] Iteration 46800, train loss = 0.270685, train accuracy = 0.953125\n",
      "[2018-07-16 19:41:43.706434] Iteration 46900, train loss = 0.325375, train accuracy = 0.906250\n",
      "[2018-07-16 19:41:49.886180] Iteration 47000, train loss = 0.235883, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.838500\n",
      "[2018-07-16 19:41:57.976134] Iteration 47100, train loss = 0.282591, train accuracy = 0.906250\n",
      "[2018-07-16 19:42:04.137583] Iteration 47200, train loss = 0.407835, train accuracy = 0.875000\n",
      "[2018-07-16 19:42:10.183659] Iteration 47300, train loss = 0.392530, train accuracy = 0.890625\n",
      "[2018-07-16 19:42:16.308481] Iteration 47400, train loss = 0.384193, train accuracy = 0.914062\n",
      "[2018-07-16 19:42:22.451486] Iteration 47500, train loss = 0.425056, train accuracy = 0.898438\n",
      "[2018-07-16 19:42:28.533209] Iteration 47600, train loss = 0.308421, train accuracy = 0.921875\n",
      "[2018-07-16 19:42:34.623973] Iteration 47700, train loss = 0.384854, train accuracy = 0.906250\n",
      "[2018-07-16 19:42:40.786789] Iteration 47800, train loss = 0.329317, train accuracy = 0.929688\n",
      "[2018-07-16 19:42:46.929172] Iteration 47900, train loss = 0.362546, train accuracy = 0.890625\n",
      "[2018-07-16 19:42:53.042216] Iteration 48000, train loss = 0.258312, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.828300\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 19:43:01.057939] Iteration 48100, train loss = 0.333464, train accuracy = 0.914062\n",
      "[2018-07-16 19:43:07.219934] Iteration 48200, train loss = 0.293338, train accuracy = 0.914062\n",
      "[2018-07-16 19:43:13.305899] Iteration 48300, train loss = 0.283863, train accuracy = 0.937500\n",
      "[2018-07-16 19:43:19.463665] Iteration 48400, train loss = 0.271219, train accuracy = 0.937500\n",
      "[2018-07-16 19:43:25.589315] Iteration 48500, train loss = 0.251645, train accuracy = 0.945312\n",
      "[2018-07-16 19:43:31.753795] Iteration 48600, train loss = 0.259621, train accuracy = 0.921875\n",
      "[2018-07-16 19:43:37.924703] Iteration 48700, train loss = 0.250999, train accuracy = 0.960938\n",
      "[2018-07-16 19:43:44.009677] Iteration 48800, train loss = 0.333001, train accuracy = 0.898438\n",
      "[2018-07-16 19:43:50.164447] Iteration 48900, train loss = 0.305607, train accuracy = 0.921875\n",
      "[2018-07-16 19:43:56.334052] Iteration 49000, train loss = 0.294131, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.877100\n",
      "[2018-07-16 19:44:04.356539] Iteration 49100, train loss = 0.324796, train accuracy = 0.898438\n",
      "[2018-07-16 19:44:10.476958] Iteration 49200, train loss = 0.260169, train accuracy = 0.953125\n",
      "[2018-07-16 19:44:16.599632] Iteration 49300, train loss = 0.347124, train accuracy = 0.906250\n",
      "[2018-07-16 19:44:22.730131] Iteration 49400, train loss = 0.431196, train accuracy = 0.875000\n",
      "[2018-07-16 19:44:28.805439] Iteration 49500, train loss = 0.326669, train accuracy = 0.890625\n",
      "[2018-07-16 19:44:34.886207] Iteration 49600, train loss = 0.274622, train accuracy = 0.929688\n",
      "[2018-07-16 19:44:41.036342] Iteration 49700, train loss = 0.308778, train accuracy = 0.929688\n",
      "[2018-07-16 19:44:47.161116] Iteration 49800, train loss = 0.326864, train accuracy = 0.929688\n",
      "[2018-07-16 19:44:53.325360] Iteration 49900, train loss = 0.263131, train accuracy = 0.945312\n",
      "[2018-07-16 19:44:59.460958] Iteration 50000, train loss = 0.368472, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.878600\n",
      "[2018-07-16 19:45:07.449345] Iteration 50100, train loss = 0.315826, train accuracy = 0.906250\n",
      "[2018-07-16 19:45:13.552903] Iteration 50200, train loss = 0.313788, train accuracy = 0.898438\n",
      "[2018-07-16 19:45:19.719962] Iteration 50300, train loss = 0.396953, train accuracy = 0.890625\n",
      "[2018-07-16 19:45:25.818697] Iteration 50400, train loss = 0.266017, train accuracy = 0.929688\n",
      "[2018-07-16 19:45:31.932620] Iteration 50500, train loss = 0.311678, train accuracy = 0.937500\n",
      "[2018-07-16 19:45:38.026221] Iteration 50600, train loss = 0.302544, train accuracy = 0.921875\n",
      "[2018-07-16 19:45:44.085645] Iteration 50700, train loss = 0.302256, train accuracy = 0.914062\n",
      "[2018-07-16 19:45:50.246492] Iteration 50800, train loss = 0.247980, train accuracy = 0.945312\n",
      "[2018-07-16 19:45:56.387803] Iteration 50900, train loss = 0.342722, train accuracy = 0.914062\n",
      "[2018-07-16 19:46:02.542997] Iteration 51000, train loss = 0.254393, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.880300\n",
      "[2018-07-16 19:46:10.524767] Iteration 51100, train loss = 0.272982, train accuracy = 0.914062\n",
      "[2018-07-16 19:46:16.624895] Iteration 51200, train loss = 0.262112, train accuracy = 0.945312\n",
      "[2018-07-16 19:46:22.779846] Iteration 51300, train loss = 0.265191, train accuracy = 0.921875\n",
      "[2018-07-16 19:46:28.919946] Iteration 51400, train loss = 0.325358, train accuracy = 0.898438\n",
      "[2018-07-16 19:46:35.067632] Iteration 51500, train loss = 0.272072, train accuracy = 0.921875\n",
      "[2018-07-16 19:46:41.161662] Iteration 51600, train loss = 0.231995, train accuracy = 0.976562\n",
      "[2018-07-16 19:46:47.299767] Iteration 51700, train loss = 0.310048, train accuracy = 0.921875\n",
      "[2018-07-16 19:46:53.431062] Iteration 51800, train loss = 0.353717, train accuracy = 0.953125\n",
      "[2018-07-16 19:46:59.536573] Iteration 51900, train loss = 0.303394, train accuracy = 0.921875\n",
      "[2018-07-16 19:47:05.696607] Iteration 52000, train loss = 0.225676, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.878700\n",
      "[2018-07-16 19:47:13.663109] Iteration 52100, train loss = 0.378429, train accuracy = 0.906250\n",
      "[2018-07-16 19:47:19.730364] Iteration 52200, train loss = 0.308703, train accuracy = 0.914062\n",
      "[2018-07-16 19:47:25.869959] Iteration 52300, train loss = 0.214784, train accuracy = 0.960938\n",
      "[2018-07-16 19:47:32.010569] Iteration 52400, train loss = 0.335184, train accuracy = 0.945312\n",
      "[2018-07-16 19:47:38.117708] Iteration 52500, train loss = 0.238439, train accuracy = 0.968750\n",
      "[2018-07-16 19:47:44.296411] Iteration 52600, train loss = 0.254971, train accuracy = 0.945312\n",
      "[2018-07-16 19:47:50.480168] Iteration 52700, train loss = 0.255967, train accuracy = 0.945312\n",
      "[2018-07-16 19:47:56.663736] Iteration 52800, train loss = 0.231579, train accuracy = 0.953125\n",
      "[2018-07-16 19:48:02.818405] Iteration 52900, train loss = 0.307855, train accuracy = 0.937500\n",
      "[2018-07-16 19:48:08.950018] Iteration 53000, train loss = 0.321841, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.880300\n",
      "[2018-07-16 19:48:16.926468] Iteration 53100, train loss = 0.238946, train accuracy = 0.968750\n",
      "[2018-07-16 19:48:23.013472] Iteration 53200, train loss = 0.265091, train accuracy = 0.937500\n",
      "[2018-07-16 19:48:29.124135] Iteration 53300, train loss = 0.450051, train accuracy = 0.906250\n",
      "[2018-07-16 19:48:35.191878] Iteration 53400, train loss = 0.303722, train accuracy = 0.937500\n",
      "[2018-07-16 19:48:41.381114] Iteration 53500, train loss = 0.420224, train accuracy = 0.906250\n",
      "[2018-07-16 19:48:47.510939] Iteration 53600, train loss = 0.288249, train accuracy = 0.921875\n",
      "[2018-07-16 19:48:53.673779] Iteration 53700, train loss = 0.243594, train accuracy = 0.960938\n",
      "[2018-07-16 19:48:59.819799] Iteration 53800, train loss = 0.221872, train accuracy = 0.953125\n",
      "[2018-07-16 19:49:05.950368] Iteration 53900, train loss = 0.273478, train accuracy = 0.929688\n",
      "[2018-07-16 19:49:12.068650] Iteration 54000, train loss = 0.239050, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.876700\n",
      "[2018-07-16 19:49:20.031396] Iteration 54100, train loss = 0.235449, train accuracy = 0.945312\n",
      "[2018-07-16 19:49:26.172418] Iteration 54200, train loss = 0.261098, train accuracy = 0.937500\n",
      "[2018-07-16 19:49:32.299124] Iteration 54300, train loss = 0.301683, train accuracy = 0.945312\n",
      "[2018-07-16 19:49:38.456533] Iteration 54400, train loss = 0.263478, train accuracy = 0.921875\n",
      "[2018-07-16 19:49:44.525497] Iteration 54500, train loss = 0.354777, train accuracy = 0.914062\n",
      "[2018-07-16 19:49:50.663046] Iteration 54600, train loss = 0.310724, train accuracy = 0.929688\n",
      "[2018-07-16 19:49:56.859689] Iteration 54700, train loss = 0.304997, train accuracy = 0.921875\n",
      "[2018-07-16 19:50:03.028776] Iteration 54800, train loss = 0.329938, train accuracy = 0.898438\n",
      "[2018-07-16 19:50:09.149769] Iteration 54900, train loss = 0.273074, train accuracy = 0.937500\n",
      "[2018-07-16 19:50:15.219952] Iteration 55000, train loss = 0.362866, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.877500\n",
      "[2018-07-16 19:50:23.261501] Iteration 55100, train loss = 0.288117, train accuracy = 0.929688\n",
      "[2018-07-16 19:50:29.398552] Iteration 55200, train loss = 0.283138, train accuracy = 0.929688\n",
      "[2018-07-16 19:50:35.504818] Iteration 55300, train loss = 0.234443, train accuracy = 0.953125\n",
      "[2018-07-16 19:50:41.685217] Iteration 55400, train loss = 0.335098, train accuracy = 0.906250\n",
      "[2018-07-16 19:50:47.864182] Iteration 55500, train loss = 0.257673, train accuracy = 0.945312\n",
      "[2018-07-16 19:50:54.021608] Iteration 55600, train loss = 0.300314, train accuracy = 0.937500\n",
      "[2018-07-16 19:51:00.171645] Iteration 55700, train loss = 0.264442, train accuracy = 0.945312\n",
      "[2018-07-16 19:51:06.286478] Iteration 55800, train loss = 0.477006, train accuracy = 0.851562\n",
      "[2018-07-16 19:51:12.434772] Iteration 55900, train loss = 0.348931, train accuracy = 0.906250\n",
      "[2018-07-16 19:51:18.588090] Iteration 56000, train loss = 0.282652, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.877100\n",
      "[2018-07-16 19:51:26.509043] Iteration 56100, train loss = 0.325660, train accuracy = 0.890625\n",
      "[2018-07-16 19:51:32.590895] Iteration 56200, train loss = 0.296554, train accuracy = 0.921875\n",
      "[2018-07-16 19:51:38.749545] Iteration 56300, train loss = 0.314648, train accuracy = 0.906250\n",
      "[2018-07-16 19:51:44.901844] Iteration 56400, train loss = 0.291151, train accuracy = 0.945312\n",
      "[2018-07-16 19:51:51.031712] Iteration 56500, train loss = 0.282805, train accuracy = 0.937500\n",
      "[2018-07-16 19:51:57.151874] Iteration 56600, train loss = 0.237983, train accuracy = 0.953125\n",
      "[2018-07-16 19:52:03.280545] Iteration 56700, train loss = 0.266887, train accuracy = 0.921875\n",
      "[2018-07-16 19:52:09.371189] Iteration 56800, train loss = 0.259888, train accuracy = 0.968750\n",
      "[2018-07-16 19:52:15.520362] Iteration 56900, train loss = 0.213834, train accuracy = 0.960938\n",
      "[2018-07-16 19:52:21.682911] Iteration 57000, train loss = 0.250452, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.880500\n",
      "[2018-07-16 19:52:29.726575] Iteration 57100, train loss = 0.326238, train accuracy = 0.906250\n",
      "[2018-07-16 19:52:35.887040] Iteration 57200, train loss = 0.283879, train accuracy = 0.914062\n",
      "[2018-07-16 19:52:42.048889] Iteration 57300, train loss = 0.318266, train accuracy = 0.937500\n",
      "[2018-07-16 19:52:48.171655] Iteration 57400, train loss = 0.332311, train accuracy = 0.890625\n",
      "[2018-07-16 19:52:54.293012] Iteration 57500, train loss = 0.356250, train accuracy = 0.898438\n",
      "[2018-07-16 19:53:00.396474] Iteration 57600, train loss = 0.215066, train accuracy = 0.960938\n",
      "[2018-07-16 19:53:06.553545] Iteration 57700, train loss = 0.263618, train accuracy = 0.945312\n",
      "[2018-07-16 19:53:12.682424] Iteration 57800, train loss = 0.246709, train accuracy = 0.937500\n",
      "[2018-07-16 19:53:18.852479] Iteration 57900, train loss = 0.185573, train accuracy = 0.968750\n",
      "[2018-07-16 19:53:25.000628] Iteration 58000, train loss = 0.272345, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.876100\n",
      "[2018-07-16 19:53:32.938383] Iteration 58100, train loss = 0.272960, train accuracy = 0.929688\n",
      "[2018-07-16 19:53:39.067857] Iteration 58200, train loss = 0.317215, train accuracy = 0.929688\n",
      "[2018-07-16 19:53:45.204602] Iteration 58300, train loss = 0.266760, train accuracy = 0.929688\n",
      "[2018-07-16 19:53:51.323116] Iteration 58400, train loss = 0.372298, train accuracy = 0.906250\n",
      "[2018-07-16 19:53:57.435709] Iteration 58500, train loss = 0.263809, train accuracy = 0.929688\n",
      "[2018-07-16 19:54:03.600286] Iteration 58600, train loss = 0.190656, train accuracy = 0.984375\n",
      "[2018-07-16 19:54:09.702998] Iteration 58700, train loss = 0.358440, train accuracy = 0.921875\n",
      "[2018-07-16 19:54:15.842842] Iteration 58800, train loss = 0.255668, train accuracy = 0.953125\n",
      "[2018-07-16 19:54:21.974236] Iteration 58900, train loss = 0.206502, train accuracy = 0.960938\n",
      "[2018-07-16 19:54:28.091719] Iteration 59000, train loss = 0.301355, train accuracy = 0.914062\n",
      "Evaluating...\n",
      "Test accuracy = 0.877800\n",
      "[2018-07-16 19:54:36.090722] Iteration 59100, train loss = 0.222412, train accuracy = 0.953125\n",
      "[2018-07-16 19:54:42.272008] Iteration 59200, train loss = 0.226700, train accuracy = 0.976562\n",
      "[2018-07-16 19:54:48.423478] Iteration 59300, train loss = 0.311554, train accuracy = 0.890625\n",
      "[2018-07-16 19:54:54.575282] Iteration 59400, train loss = 0.255983, train accuracy = 0.921875\n",
      "[2018-07-16 19:55:00.737075] Iteration 59500, train loss = 0.296865, train accuracy = 0.929688\n",
      "[2018-07-16 19:55:06.842345] Iteration 59600, train loss = 0.222585, train accuracy = 0.953125\n",
      "[2018-07-16 19:55:13.021220] Iteration 59700, train loss = 0.245509, train accuracy = 0.945312\n",
      "[2018-07-16 19:55:19.099349] Iteration 59800, train loss = 0.247622, train accuracy = 0.960938\n",
      "[2018-07-16 19:55:25.279044] Iteration 59900, train loss = 0.278382, train accuracy = 0.945312\n",
      "[2018-07-16 19:55:31.474508] Iteration 60000, train loss = 0.275159, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.877800\n",
      "[2018-07-16 19:55:39.573066] Iteration 60100, train loss = 0.348652, train accuracy = 0.867188\n",
      "[2018-07-16 19:55:45.743457] Iteration 60200, train loss = 0.343970, train accuracy = 0.929688\n",
      "[2018-07-16 19:55:51.919390] Iteration 60300, train loss = 0.359012, train accuracy = 0.898438\n",
      "[2018-07-16 19:55:58.073312] Iteration 60400, train loss = 0.275704, train accuracy = 0.937500\n",
      "[2018-07-16 19:56:04.209394] Iteration 60500, train loss = 0.334137, train accuracy = 0.914062\n",
      "[2018-07-16 19:56:10.309640] Iteration 60600, train loss = 0.309160, train accuracy = 0.929688\n",
      "[2018-07-16 19:56:16.480501] Iteration 60700, train loss = 0.269609, train accuracy = 0.945312\n",
      "[2018-07-16 19:56:22.622166] Iteration 60800, train loss = 0.358076, train accuracy = 0.875000\n",
      "[2018-07-16 19:56:28.758062] Iteration 60900, train loss = 0.441088, train accuracy = 0.890625\n",
      "[2018-07-16 19:56:34.971527] Iteration 61000, train loss = 0.234339, train accuracy = 0.960938\n",
      "Evaluating...\n",
      "Test accuracy = 0.876200\n",
      "[2018-07-16 19:56:42.964933] Iteration 61100, train loss = 0.289789, train accuracy = 0.921875\n",
      "[2018-07-16 19:56:49.070343] Iteration 61200, train loss = 0.292199, train accuracy = 0.898438\n",
      "[2018-07-16 19:56:55.167264] Iteration 61300, train loss = 0.365657, train accuracy = 0.898438\n",
      "[2018-07-16 19:57:01.327660] Iteration 61400, train loss = 0.272516, train accuracy = 0.937500\n",
      "[2018-07-16 19:57:07.464841] Iteration 61500, train loss = 0.221267, train accuracy = 0.953125\n",
      "[2018-07-16 19:57:13.659184] Iteration 61600, train loss = 0.267334, train accuracy = 0.937500\n",
      "[2018-07-16 19:57:19.774900] Iteration 61700, train loss = 0.284900, train accuracy = 0.937500\n",
      "[2018-07-16 19:57:25.923725] Iteration 61800, train loss = 0.225171, train accuracy = 0.953125\n",
      "[2018-07-16 19:57:31.992361] Iteration 61900, train loss = 0.282381, train accuracy = 0.929688\n",
      "[2018-07-16 19:57:38.040143] Iteration 62000, train loss = 0.379828, train accuracy = 0.875000\n",
      "Evaluating...\n",
      "Test accuracy = 0.879000\n",
      "[2018-07-16 19:57:45.913284] Iteration 62100, train loss = 0.332010, train accuracy = 0.898438\n",
      "[2018-07-16 19:57:51.943017] Iteration 62200, train loss = 0.238789, train accuracy = 0.953125\n",
      "[2018-07-16 19:57:57.966297] Iteration 62300, train loss = 0.266871, train accuracy = 0.921875\n",
      "[2018-07-16 19:58:03.975323] Iteration 62400, train loss = 0.229807, train accuracy = 0.953125\n",
      "[2018-07-16 19:58:09.994039] Iteration 62500, train loss = 0.206165, train accuracy = 0.968750\n",
      "[2018-07-16 19:58:15.941422] Iteration 62600, train loss = 0.395551, train accuracy = 0.898438\n",
      "[2018-07-16 19:58:21.887854] Iteration 62700, train loss = 0.281891, train accuracy = 0.937500\n",
      "[2018-07-16 19:58:27.815454] Iteration 62800, train loss = 0.499185, train accuracy = 0.843750\n",
      "[2018-07-16 19:58:33.727038] Iteration 62900, train loss = 0.337470, train accuracy = 0.937500\n",
      "[2018-07-16 19:58:39.734098] Iteration 63000, train loss = 0.221185, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.876700\n",
      "[2018-07-16 19:58:47.765633] Iteration 63100, train loss = 0.275840, train accuracy = 0.945312\n",
      "[2018-07-16 19:58:53.699740] Iteration 63200, train loss = 0.181898, train accuracy = 0.984375\n",
      "[2018-07-16 19:58:59.608399] Iteration 63300, train loss = 0.290188, train accuracy = 0.921875\n",
      "[2018-07-16 19:59:05.608536] Iteration 63400, train loss = 0.273528, train accuracy = 0.906250\n",
      "[2018-07-16 19:59:11.637873] Iteration 63500, train loss = 0.245116, train accuracy = 0.953125\n",
      "[2018-07-16 19:59:17.674837] Iteration 63600, train loss = 0.278038, train accuracy = 0.937500\n",
      "[2018-07-16 19:59:23.641363] Iteration 63700, train loss = 0.309410, train accuracy = 0.921875\n",
      "[2018-07-16 19:59:29.477528] Iteration 63800, train loss = 0.401780, train accuracy = 0.882812\n",
      "[2018-07-16 19:59:35.472074] Iteration 63900, train loss = 0.168514, train accuracy = 0.976562\n",
      "[2018-07-16 19:59:41.430097] Iteration 64000, train loss = 0.250586, train accuracy = 0.937500\n",
      "Evaluating...\n",
      "Test accuracy = 0.877600\n",
      "[2018-07-16 19:59:49.102277] Iteration 64100, train loss = 0.285853, train accuracy = 0.929688\n",
      "[2018-07-16 19:59:55.107085] Iteration 64200, train loss = 0.283547, train accuracy = 0.921875\n",
      "[2018-07-16 20:00:00.995847] Iteration 64300, train loss = 0.267523, train accuracy = 0.929688\n",
      "[2018-07-16 20:00:06.979075] Iteration 64400, train loss = 0.332892, train accuracy = 0.929688\n",
      "[2018-07-16 20:00:13.009524] Iteration 64500, train loss = 0.219304, train accuracy = 0.937500\n",
      "[2018-07-16 20:00:18.920511] Iteration 64600, train loss = 0.258913, train accuracy = 0.937500\n",
      "[2018-07-16 20:00:24.862204] Iteration 64700, train loss = 0.302806, train accuracy = 0.921875\n",
      "[2018-07-16 20:00:30.819236] Iteration 64800, train loss = 0.339084, train accuracy = 0.914062\n",
      "[2018-07-16 20:00:36.797889] Iteration 64900, train loss = 0.306951, train accuracy = 0.921875\n",
      "[2018-07-16 20:00:42.829830] Iteration 65000, train loss = 0.295507, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.878600\n",
      "[2018-07-16 20:00:50.605990] Iteration 65100, train loss = 0.206032, train accuracy = 0.976562\n",
      "[2018-07-16 20:00:56.601517] Iteration 65200, train loss = 0.306659, train accuracy = 0.921875\n",
      "[2018-07-16 20:01:02.620141] Iteration 65300, train loss = 0.232839, train accuracy = 0.960938\n",
      "[2018-07-16 20:01:08.630495] Iteration 65400, train loss = 0.208421, train accuracy = 0.968750\n",
      "[2018-07-16 20:01:14.654806] Iteration 65500, train loss = 0.244983, train accuracy = 0.960938\n",
      "[2018-07-16 20:01:20.710456] Iteration 65600, train loss = 0.279452, train accuracy = 0.929688\n",
      "[2018-07-16 20:01:26.762382] Iteration 65700, train loss = 0.333846, train accuracy = 0.914062\n",
      "[2018-07-16 20:01:32.823712] Iteration 65800, train loss = 0.228513, train accuracy = 0.953125\n",
      "[2018-07-16 20:01:38.921626] Iteration 65900, train loss = 0.311963, train accuracy = 0.906250\n",
      "[2018-07-16 20:01:44.977222] Iteration 66000, train loss = 0.351565, train accuracy = 0.898438\n",
      "Evaluating...\n",
      "Test accuracy = 0.874200\n",
      "[2018-07-16 20:01:52.925793] Iteration 66100, train loss = 0.367524, train accuracy = 0.906250\n",
      "[2018-07-16 20:01:58.994176] Iteration 66200, train loss = 0.218757, train accuracy = 0.945312\n",
      "[2018-07-16 20:02:05.026094] Iteration 66300, train loss = 0.303450, train accuracy = 0.929688\n",
      "[2018-07-16 20:02:11.133275] Iteration 66400, train loss = 0.302196, train accuracy = 0.914062\n",
      "[2018-07-16 20:02:17.267596] Iteration 66500, train loss = 0.282686, train accuracy = 0.937500\n",
      "[2018-07-16 20:02:23.367339] Iteration 66600, train loss = 0.259659, train accuracy = 0.937500\n",
      "[2018-07-16 20:02:29.467927] Iteration 66700, train loss = 0.256155, train accuracy = 0.945312\n",
      "[2018-07-16 20:02:35.588340] Iteration 66800, train loss = 0.201000, train accuracy = 0.976562\n",
      "[2018-07-16 20:02:41.640520] Iteration 66900, train loss = 0.274972, train accuracy = 0.937500\n",
      "[2018-07-16 20:02:47.743270] Iteration 67000, train loss = 0.231869, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.876500\n",
      "[2018-07-16 20:02:55.730516] Iteration 67100, train loss = 0.230011, train accuracy = 0.968750\n",
      "[2018-07-16 20:03:01.815907] Iteration 67200, train loss = 0.234393, train accuracy = 0.953125\n",
      "[2018-07-16 20:03:07.900595] Iteration 67300, train loss = 0.272787, train accuracy = 0.937500\n",
      "[2018-07-16 20:03:13.983981] Iteration 67400, train loss = 0.278308, train accuracy = 0.937500\n",
      "[2018-07-16 20:03:20.030086] Iteration 67500, train loss = 0.258857, train accuracy = 0.953125\n",
      "[2018-07-16 20:03:26.160564] Iteration 67600, train loss = 0.268101, train accuracy = 0.945312\n",
      "[2018-07-16 20:03:32.272881] Iteration 67700, train loss = 0.258049, train accuracy = 0.929688\n",
      "[2018-07-16 20:03:38.409855] Iteration 67800, train loss = 0.321138, train accuracy = 0.906250\n",
      "[2018-07-16 20:03:44.459507] Iteration 67900, train loss = 0.320295, train accuracy = 0.945312\n",
      "[2018-07-16 20:03:50.562083] Iteration 68000, train loss = 0.342113, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.876400\n",
      "[2018-07-16 20:03:58.556695] Iteration 68100, train loss = 0.320283, train accuracy = 0.921875\n",
      "[2018-07-16 20:04:04.663400] Iteration 68200, train loss = 0.229143, train accuracy = 0.953125\n",
      "[2018-07-16 20:04:10.757276] Iteration 68300, train loss = 0.294139, train accuracy = 0.921875\n",
      "[2018-07-16 20:04:16.900341] Iteration 68400, train loss = 0.289915, train accuracy = 0.914062\n",
      "[2018-07-16 20:04:23.059803] Iteration 68500, train loss = 0.319574, train accuracy = 0.914062\n",
      "[2018-07-16 20:04:29.182530] Iteration 68600, train loss = 0.317796, train accuracy = 0.921875\n",
      "[2018-07-16 20:04:35.251507] Iteration 68700, train loss = 0.262578, train accuracy = 0.937500\n",
      "[2018-07-16 20:04:41.337640] Iteration 68800, train loss = 0.252791, train accuracy = 0.953125\n",
      "[2018-07-16 20:04:47.466228] Iteration 68900, train loss = 0.209963, train accuracy = 0.953125\n",
      "[2018-07-16 20:04:53.588980] Iteration 69000, train loss = 0.292947, train accuracy = 0.906250\n",
      "Evaluating...\n",
      "Test accuracy = 0.878100\n",
      "[2018-07-16 20:05:01.566027] Iteration 69100, train loss = 0.303108, train accuracy = 0.898438\n",
      "[2018-07-16 20:05:07.659247] Iteration 69200, train loss = 0.200530, train accuracy = 0.945312\n",
      "[2018-07-16 20:05:13.807683] Iteration 69300, train loss = 0.362749, train accuracy = 0.906250\n",
      "[2018-07-16 20:05:19.936749] Iteration 69400, train loss = 0.324951, train accuracy = 0.914062\n",
      "[2018-07-16 20:05:26.055729] Iteration 69500, train loss = 0.254682, train accuracy = 0.921875\n",
      "[2018-07-16 20:05:32.123255] Iteration 69600, train loss = 0.247058, train accuracy = 0.906250\n",
      "[2018-07-16 20:05:38.277522] Iteration 69700, train loss = 0.359257, train accuracy = 0.914062\n",
      "[2018-07-16 20:05:44.320488] Iteration 69800, train loss = 0.250950, train accuracy = 0.945312\n",
      "[2018-07-16 20:05:50.454778] Iteration 69900, train loss = 0.361199, train accuracy = 0.921875\n",
      "[2018-07-16 20:05:56.566773] Iteration 70000, train loss = 0.314685, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.875700\n",
      "[2018-07-16 20:06:04.606680] Iteration 70100, train loss = 0.275346, train accuracy = 0.937500\n",
      "[2018-07-16 20:06:10.695259] Iteration 70200, train loss = 0.197787, train accuracy = 0.976562\n",
      "[2018-07-16 20:06:16.744420] Iteration 70300, train loss = 0.277182, train accuracy = 0.937500\n",
      "[2018-07-16 20:06:22.811511] Iteration 70400, train loss = 0.281836, train accuracy = 0.929688\n",
      "[2018-07-16 20:06:28.908413] Iteration 70500, train loss = 0.242749, train accuracy = 0.953125\n",
      "[2018-07-16 20:06:35.085327] Iteration 70600, train loss = 0.278931, train accuracy = 0.921875\n",
      "[2018-07-16 20:06:41.269087] Iteration 70700, train loss = 0.450517, train accuracy = 0.859375\n",
      "[2018-07-16 20:06:47.470589] Iteration 70800, train loss = 0.320531, train accuracy = 0.906250\n",
      "[2018-07-16 20:06:53.635298] Iteration 70900, train loss = 0.281223, train accuracy = 0.937500\n",
      "[2018-07-16 20:06:59.799215] Iteration 71000, train loss = 0.252119, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.874800\n",
      "[2018-07-16 20:07:07.780834] Iteration 71100, train loss = 0.371576, train accuracy = 0.898438\n",
      "[2018-07-16 20:07:13.964161] Iteration 71200, train loss = 0.354502, train accuracy = 0.906250\n",
      "[2018-07-16 20:07:20.076064] Iteration 71300, train loss = 0.286624, train accuracy = 0.929688\n",
      "[2018-07-16 20:07:26.190338] Iteration 71400, train loss = 0.235108, train accuracy = 0.945312\n",
      "[2018-07-16 20:07:32.368401] Iteration 71500, train loss = 0.224158, train accuracy = 0.953125\n",
      "[2018-07-16 20:07:38.510960] Iteration 71600, train loss = 0.260065, train accuracy = 0.953125\n",
      "[2018-07-16 20:07:44.615230] Iteration 71700, train loss = 0.287046, train accuracy = 0.937500\n",
      "[2018-07-16 20:07:50.812913] Iteration 71800, train loss = 0.294101, train accuracy = 0.921875\n",
      "[2018-07-16 20:07:56.872862] Iteration 71900, train loss = 0.280764, train accuracy = 0.914062\n",
      "[2018-07-16 20:08:03.085658] Iteration 72000, train loss = 0.291023, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.875100\n",
      "[2018-07-16 20:08:11.056021] Iteration 72100, train loss = 0.305198, train accuracy = 0.929688\n",
      "[2018-07-16 20:08:17.209610] Iteration 72200, train loss = 0.273546, train accuracy = 0.945312\n",
      "[2018-07-16 20:08:23.340043] Iteration 72300, train loss = 0.233729, train accuracy = 0.937500\n",
      "[2018-07-16 20:08:29.463320] Iteration 72400, train loss = 0.241020, train accuracy = 0.968750\n",
      "[2018-07-16 20:08:35.606359] Iteration 72500, train loss = 0.252993, train accuracy = 0.937500\n",
      "[2018-07-16 20:08:41.736711] Iteration 72600, train loss = 0.290576, train accuracy = 0.937500\n",
      "[2018-07-16 20:08:47.812400] Iteration 72700, train loss = 0.293966, train accuracy = 0.929688\n",
      "[2018-07-16 20:08:53.984292] Iteration 72800, train loss = 0.332067, train accuracy = 0.914062\n",
      "[2018-07-16 20:09:00.150602] Iteration 72900, train loss = 0.392357, train accuracy = 0.898438\n",
      "[2018-07-16 20:09:06.274017] Iteration 73000, train loss = 0.408036, train accuracy = 0.890625\n",
      "Evaluating...\n",
      "Test accuracy = 0.877400\n",
      "[2018-07-16 20:09:14.217388] Iteration 73100, train loss = 0.274111, train accuracy = 0.921875\n",
      "[2018-07-16 20:09:20.279764] Iteration 73200, train loss = 0.274164, train accuracy = 0.914062\n",
      "[2018-07-16 20:09:26.389174] Iteration 73300, train loss = 0.268609, train accuracy = 0.929688\n",
      "[2018-07-16 20:09:32.521646] Iteration 73400, train loss = 0.304312, train accuracy = 0.937500\n",
      "[2018-07-16 20:09:38.637000] Iteration 73500, train loss = 0.234431, train accuracy = 0.953125\n",
      "[2018-07-16 20:09:44.783955] Iteration 73600, train loss = 0.250300, train accuracy = 0.953125\n",
      "[2018-07-16 20:09:50.888354] Iteration 73700, train loss = 0.259043, train accuracy = 0.945312\n",
      "[2018-07-16 20:09:57.038440] Iteration 73800, train loss = 0.283646, train accuracy = 0.914062\n",
      "[2018-07-16 20:10:03.168807] Iteration 73900, train loss = 0.222516, train accuracy = 0.953125\n",
      "[2018-07-16 20:10:09.268623] Iteration 74000, train loss = 0.274683, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.878500\n",
      "[2018-07-16 20:10:17.236100] Iteration 74100, train loss = 0.291637, train accuracy = 0.914062\n",
      "[2018-07-16 20:10:23.348042] Iteration 74200, train loss = 0.297250, train accuracy = 0.929688\n",
      "[2018-07-16 20:10:29.441819] Iteration 74300, train loss = 0.316503, train accuracy = 0.898438\n",
      "[2018-07-16 20:10:35.557735] Iteration 74400, train loss = 0.265172, train accuracy = 0.929688\n",
      "[2018-07-16 20:10:41.676441] Iteration 74500, train loss = 0.261953, train accuracy = 0.929688\n",
      "[2018-07-16 20:10:47.802504] Iteration 74600, train loss = 0.347682, train accuracy = 0.890625\n",
      "[2018-07-16 20:10:53.885226] Iteration 74700, train loss = 0.289330, train accuracy = 0.937500\n",
      "[2018-07-16 20:11:00.061784] Iteration 74800, train loss = 0.226356, train accuracy = 0.960938\n",
      "[2018-07-16 20:11:06.189332] Iteration 74900, train loss = 0.271396, train accuracy = 0.921875\n",
      "[2018-07-16 20:11:12.288869] Iteration 75000, train loss = 0.248242, train accuracy = 0.929688\n",
      "Evaluating...\n",
      "Test accuracy = 0.875500\n",
      "[2018-07-16 20:11:20.286747] Iteration 75100, train loss = 0.223583, train accuracy = 0.960938\n",
      "[2018-07-16 20:11:26.398756] Iteration 75200, train loss = 0.326130, train accuracy = 0.937500\n",
      "[2018-07-16 20:11:32.527097] Iteration 75300, train loss = 0.254993, train accuracy = 0.929688\n",
      "[2018-07-16 20:11:38.605666] Iteration 75400, train loss = 0.281163, train accuracy = 0.921875\n",
      "[2018-07-16 20:11:44.702468] Iteration 75500, train loss = 0.255448, train accuracy = 0.921875\n",
      "[2018-07-16 20:11:50.854674] Iteration 75600, train loss = 0.265230, train accuracy = 0.937500\n",
      "[2018-07-16 20:11:56.916821] Iteration 75700, train loss = 0.235746, train accuracy = 0.945312\n",
      "[2018-07-16 20:12:03.065256] Iteration 75800, train loss = 0.239866, train accuracy = 0.953125\n",
      "[2018-07-16 20:12:09.187138] Iteration 75900, train loss = 0.207976, train accuracy = 0.968750\n",
      "[2018-07-16 20:12:15.273559] Iteration 76000, train loss = 0.267679, train accuracy = 0.945312\n",
      "Evaluating...\n",
      "Test accuracy = 0.874400\n",
      "[2018-07-16 20:12:23.237533] Iteration 76100, train loss = 0.350042, train accuracy = 0.929688\n",
      "[2018-07-16 20:12:29.327918] Iteration 76200, train loss = 0.254927, train accuracy = 0.945312\n",
      "[2018-07-16 20:12:35.460333] Iteration 76300, train loss = 0.279583, train accuracy = 0.953125\n",
      "[2018-07-16 20:12:41.595847] Iteration 76400, train loss = 0.312828, train accuracy = 0.929688\n",
      "[2018-07-16 20:12:47.730213] Iteration 76500, train loss = 0.336860, train accuracy = 0.914062\n",
      "[2018-07-16 20:12:53.838953] Iteration 76600, train loss = 0.256045, train accuracy = 0.953125\n",
      "[2018-07-16 20:12:59.907020] Iteration 76700, train loss = 0.223433, train accuracy = 0.945312\n",
      "[2018-07-16 20:13:06.019659] Iteration 76800, train loss = 0.329423, train accuracy = 0.906250\n",
      "[2018-07-16 20:13:12.110086] Iteration 76900, train loss = 0.295189, train accuracy = 0.921875\n",
      "[2018-07-16 20:13:18.236068] Iteration 77000, train loss = 0.305714, train accuracy = 0.921875\n",
      "Evaluating...\n",
      "Test accuracy = 0.877400\n",
      "[2018-07-16 20:13:26.146071] Iteration 77100, train loss = 0.259553, train accuracy = 0.937500\n",
      "[2018-07-16 20:13:32.256522] Iteration 77200, train loss = 0.333984, train accuracy = 0.921875\n",
      "[2018-07-16 20:13:38.326008] Iteration 77300, train loss = 0.213613, train accuracy = 0.953125\n",
      "[2018-07-16 20:13:44.384966] Iteration 77400, train loss = 0.254379, train accuracy = 0.945312\n",
      "[2018-07-16 20:13:50.537493] Iteration 77500, train loss = 0.218553, train accuracy = 0.953125\n",
      "[2018-07-16 20:13:56.605086] Iteration 77600, train loss = 0.256942, train accuracy = 0.937500\n",
      "[2018-07-16 20:14:02.710436] Iteration 77700, train loss = 0.335117, train accuracy = 0.906250\n",
      "[2018-07-16 20:14:08.860220] Iteration 77800, train loss = 0.295229, train accuracy = 0.945312\n",
      "[2018-07-16 20:14:14.964350] Iteration 77900, train loss = 0.314374, train accuracy = 0.914062\n",
      "[2018-07-16 20:14:21.086812] Iteration 78000, train loss = 0.241624, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.875700\n",
      "[2018-07-16 20:14:29.000856] Iteration 78100, train loss = 0.293849, train accuracy = 0.929688\n",
      "[2018-07-16 20:14:35.155618] Iteration 78200, train loss = 0.255159, train accuracy = 0.929688\n",
      "[2018-07-16 20:14:41.230125] Iteration 78300, train loss = 0.220307, train accuracy = 0.945312\n",
      "[2018-07-16 20:14:47.313810] Iteration 78400, train loss = 0.218037, train accuracy = 0.960938\n",
      "[2018-07-16 20:14:53.396512] Iteration 78500, train loss = 0.270883, train accuracy = 0.945312\n",
      "[2018-07-16 20:14:59.453993] Iteration 78600, train loss = 0.252439, train accuracy = 0.953125\n",
      "[2018-07-16 20:15:05.538739] Iteration 78700, train loss = 0.176113, train accuracy = 0.984375\n",
      "[2018-07-16 20:15:11.591393] Iteration 78800, train loss = 0.306949, train accuracy = 0.914062\n",
      "[2018-07-16 20:15:17.670730] Iteration 78900, train loss = 0.361963, train accuracy = 0.890625\n",
      "[2018-07-16 20:15:23.725193] Iteration 79000, train loss = 0.229003, train accuracy = 0.953125\n",
      "Evaluating...\n",
      "Test accuracy = 0.876500\n",
      "[2018-07-16 20:15:31.720725] Iteration 79100, train loss = 0.238658, train accuracy = 0.945312\n",
      "[2018-07-16 20:15:37.767256] Iteration 79200, train loss = 0.258912, train accuracy = 0.945312\n",
      "[2018-07-16 20:15:43.833910] Iteration 79300, train loss = 0.334973, train accuracy = 0.890625\n",
      "[2018-07-16 20:15:49.897706] Iteration 79400, train loss = 0.249701, train accuracy = 0.929688\n",
      "[2018-07-16 20:15:55.983589] Iteration 79500, train loss = 0.317258, train accuracy = 0.921875\n",
      "[2018-07-16 20:16:02.038718] Iteration 79600, train loss = 0.218239, train accuracy = 0.960938\n",
      "[2018-07-16 20:16:08.131210] Iteration 79700, train loss = 0.260585, train accuracy = 0.937500\n",
      "[2018-07-16 20:16:14.238869] Iteration 79800, train loss = 0.328105, train accuracy = 0.914062\n",
      "[2018-07-16 20:16:20.371853] Iteration 79900, train loss = 0.321576, train accuracy = 0.921875\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第四轮  量化  \n",
    "prune_rate = 1.00"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune前的准确率\n",
      "Test accuracy = 0.876400\n"
     ]
    }
   ],
   "source": [
    "print('prune前的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625     -0.0625      0.0625      0.125       0.          0.\n",
      "  0.07715868 -0.1761247  -0.125      -0.125     ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =0.85时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "para_dict = {}\n",
    "one_dict = {}\n",
    "var_name = []\n",
    "for k in tf.trainable_variables():\n",
    "    para_dict[k.name] = k\n",
    "    one_dict[k.name] =tf.ones_like(k)\n",
    "    var_name.append(k.name)\n",
    "prune_dict=apply_inq(para_dict,one_dict,var_name,1)\n",
    "trainer = tf.train.GradientDescentOptimizer(learning_rate)\n",
    "grads_and_vars = trainer.compute_gradients(loss)\n",
    "grads_and_vars = apply_prune_on_grads(grads_and_vars, prune_dict)\n",
    "train_step = trainer.apply_gradients(grads_and_vars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate set to 0.010000\n",
      "[2018-07-16 20:18:16.320165] Iteration 100, train loss = 2.452042, train accuracy = 0.164062\n",
      "[2018-07-16 20:18:20.983089] Iteration 200, train loss = 2.648457, train accuracy = 0.140625\n",
      "[2018-07-16 20:18:25.638260] Iteration 300, train loss = 2.559318, train accuracy = 0.179688\n",
      "[2018-07-16 20:18:30.305785] Iteration 400, train loss = 2.480033, train accuracy = 0.164062\n",
      "[2018-07-16 20:18:34.994760] Iteration 500, train loss = 2.581846, train accuracy = 0.109375\n",
      "[2018-07-16 20:18:39.735957] Iteration 600, train loss = 2.420281, train accuracy = 0.164062\n",
      "[2018-07-16 20:18:44.540005] Iteration 700, train loss = 2.463500, train accuracy = 0.164062\n",
      "[2018-07-16 20:18:49.323701] Iteration 800, train loss = 2.412048, train accuracy = 0.171875\n",
      "[2018-07-16 20:18:54.123139] Iteration 900, train loss = 2.483890, train accuracy = 0.156250\n",
      "[2018-07-16 20:18:59.043586] Iteration 1000, train loss = 2.487085, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.165600\n",
      "[2018-07-16 20:19:05.674822] Iteration 1100, train loss = 2.698879, train accuracy = 0.156250\n",
      "[2018-07-16 20:19:10.606357] Iteration 1200, train loss = 2.562802, train accuracy = 0.195312\n",
      "[2018-07-16 20:19:15.608096] Iteration 1300, train loss = 2.543958, train accuracy = 0.148438\n",
      "[2018-07-16 20:19:20.664515] Iteration 1400, train loss = 2.499869, train accuracy = 0.132812\n",
      "[2018-07-16 20:19:25.680217] Iteration 1500, train loss = 2.580630, train accuracy = 0.132812\n",
      "[2018-07-16 20:19:30.763260] Iteration 1600, train loss = 2.572662, train accuracy = 0.203125\n",
      "[2018-07-16 20:19:35.874092] Iteration 1700, train loss = 2.529737, train accuracy = 0.148438\n",
      "[2018-07-16 20:19:40.992685] Iteration 1800, train loss = 2.533895, train accuracy = 0.179688\n",
      "[2018-07-16 20:19:46.122081] Iteration 1900, train loss = 2.612702, train accuracy = 0.140625\n",
      "[2018-07-16 20:19:51.237249] Iteration 2000, train loss = 2.520596, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 20:19:58.167085] Iteration 2100, train loss = 2.403735, train accuracy = 0.148438\n",
      "[2018-07-16 20:20:03.300684] Iteration 2200, train loss = 2.437322, train accuracy = 0.156250\n",
      "[2018-07-16 20:20:08.450960] Iteration 2300, train loss = 2.640756, train accuracy = 0.117188\n",
      "[2018-07-16 20:20:13.585166] Iteration 2400, train loss = 2.562256, train accuracy = 0.148438\n",
      "[2018-07-16 20:20:18.746129] Iteration 2500, train loss = 2.548799, train accuracy = 0.195312\n",
      "[2018-07-16 20:20:23.850970] Iteration 2600, train loss = 2.580121, train accuracy = 0.132812\n",
      "[2018-07-16 20:20:29.020186] Iteration 2700, train loss = 2.535193, train accuracy = 0.210938\n",
      "[2018-07-16 20:20:34.205778] Iteration 2800, train loss = 2.688169, train accuracy = 0.156250\n",
      "[2018-07-16 20:20:39.381657] Iteration 2900, train loss = 2.570756, train accuracy = 0.140625\n",
      "[2018-07-16 20:20:44.517885] Iteration 3000, train loss = 2.523340, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.166100\n",
      "[2018-07-16 20:20:51.535630] Iteration 3100, train loss = 2.538710, train accuracy = 0.140625\n",
      "[2018-07-16 20:20:56.678184] Iteration 3200, train loss = 2.619064, train accuracy = 0.148438\n",
      "[2018-07-16 20:21:01.773936] Iteration 3300, train loss = 2.473123, train accuracy = 0.140625\n",
      "[2018-07-16 20:21:06.853172] Iteration 3400, train loss = 2.433346, train accuracy = 0.171875\n",
      "[2018-07-16 20:21:12.052902] Iteration 3500, train loss = 2.591585, train accuracy = 0.140625\n",
      "[2018-07-16 20:21:17.210472] Iteration 3600, train loss = 2.548608, train accuracy = 0.203125\n",
      "[2018-07-16 20:21:22.318533] Iteration 3700, train loss = 2.553828, train accuracy = 0.164062\n",
      "[2018-07-16 20:21:27.412558] Iteration 3800, train loss = 2.630825, train accuracy = 0.140625\n",
      "[2018-07-16 20:21:32.328803] Iteration 3900, train loss = 2.442922, train accuracy = 0.117188\n",
      "[2018-07-16 20:21:37.413209] Iteration 4000, train loss = 2.519364, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.165600\n",
      "[2018-07-16 20:21:44.456099] Iteration 4100, train loss = 2.503637, train accuracy = 0.218750\n",
      "[2018-07-16 20:21:49.641472] Iteration 4200, train loss = 2.766423, train accuracy = 0.156250\n",
      "[2018-07-16 20:21:54.803217] Iteration 4300, train loss = 2.616427, train accuracy = 0.171875\n",
      "[2018-07-16 20:21:59.965816] Iteration 4400, train loss = 2.785287, train accuracy = 0.117188\n",
      "[2018-07-16 20:22:05.081802] Iteration 4500, train loss = 2.627187, train accuracy = 0.140625\n",
      "[2018-07-16 20:22:10.254878] Iteration 4600, train loss = 2.625319, train accuracy = 0.125000\n",
      "[2018-07-16 20:22:15.321293] Iteration 4700, train loss = 2.558671, train accuracy = 0.109375\n",
      "[2018-07-16 20:22:20.449211] Iteration 4800, train loss = 2.442134, train accuracy = 0.171875\n",
      "[2018-07-16 20:22:25.513484] Iteration 4900, train loss = 2.588368, train accuracy = 0.148438\n",
      "[2018-07-16 20:22:30.609672] Iteration 5000, train loss = 2.419741, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.164300\n",
      "[2018-07-16 20:22:37.656023] Iteration 5100, train loss = 2.458297, train accuracy = 0.140625\n",
      "[2018-07-16 20:22:42.847025] Iteration 5200, train loss = 2.506804, train accuracy = 0.140625\n",
      "[2018-07-16 20:22:47.997801] Iteration 5300, train loss = 2.556491, train accuracy = 0.117188\n",
      "[2018-07-16 20:22:53.136254] Iteration 5400, train loss = 2.536750, train accuracy = 0.109375\n",
      "[2018-07-16 20:22:58.228228] Iteration 5500, train loss = 2.477856, train accuracy = 0.156250\n",
      "[2018-07-16 20:23:03.347634] Iteration 5600, train loss = 2.549050, train accuracy = 0.164062\n",
      "[2018-07-16 20:23:08.532130] Iteration 5700, train loss = 2.579639, train accuracy = 0.171875\n",
      "[2018-07-16 20:23:13.614066] Iteration 5800, train loss = 2.556859, train accuracy = 0.187500\n",
      "[2018-07-16 20:23:18.696851] Iteration 5900, train loss = 2.573590, train accuracy = 0.148438\n",
      "[2018-07-16 20:23:23.870537] Iteration 6000, train loss = 2.565400, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.165200\n",
      "[2018-07-16 20:23:30.920707] Iteration 6100, train loss = 2.428892, train accuracy = 0.195312\n",
      "[2018-07-16 20:23:36.024830] Iteration 6200, train loss = 2.533452, train accuracy = 0.140625\n",
      "[2018-07-16 20:23:41.176282] Iteration 6300, train loss = 2.528910, train accuracy = 0.187500\n",
      "[2018-07-16 20:23:46.351990] Iteration 6400, train loss = 2.511283, train accuracy = 0.164062\n",
      "[2018-07-16 20:23:51.456453] Iteration 6500, train loss = 2.447893, train accuracy = 0.195312\n",
      "[2018-07-16 20:23:56.641901] Iteration 6600, train loss = 2.307395, train accuracy = 0.210938\n",
      "[2018-07-16 20:24:01.819480] Iteration 6700, train loss = 2.495311, train accuracy = 0.195312\n",
      "[2018-07-16 20:24:06.999613] Iteration 6800, train loss = 2.472204, train accuracy = 0.203125\n",
      "[2018-07-16 20:24:12.178192] Iteration 6900, train loss = 2.549408, train accuracy = 0.156250\n",
      "[2018-07-16 20:24:17.289551] Iteration 7000, train loss = 2.461278, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.165700\n",
      "[2018-07-16 20:24:24.349627] Iteration 7100, train loss = 2.581089, train accuracy = 0.125000\n",
      "[2018-07-16 20:24:29.573583] Iteration 7200, train loss = 2.468107, train accuracy = 0.132812\n",
      "[2018-07-16 20:24:34.751219] Iteration 7300, train loss = 2.580677, train accuracy = 0.132812\n",
      "[2018-07-16 20:24:39.884112] Iteration 7400, train loss = 2.421264, train accuracy = 0.132812\n",
      "[2018-07-16 20:24:44.990236] Iteration 7500, train loss = 2.530243, train accuracy = 0.156250\n",
      "[2018-07-16 20:24:50.168093] Iteration 7600, train loss = 2.360070, train accuracy = 0.171875\n",
      "[2018-07-16 20:24:55.351536] Iteration 7700, train loss = 2.494527, train accuracy = 0.187500\n",
      "[2018-07-16 20:25:00.529875] Iteration 7800, train loss = 2.518810, train accuracy = 0.195312\n",
      "[2018-07-16 20:25:05.711852] Iteration 7900, train loss = 2.732892, train accuracy = 0.117188\n",
      "[2018-07-16 20:25:10.838829] Iteration 8000, train loss = 2.507201, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 20:25:17.772053] Iteration 8100, train loss = 2.708134, train accuracy = 0.117188\n",
      "[2018-07-16 20:25:22.913217] Iteration 8200, train loss = 2.648457, train accuracy = 0.156250\n",
      "[2018-07-16 20:25:27.998845] Iteration 8300, train loss = 2.648508, train accuracy = 0.179688\n",
      "[2018-07-16 20:25:33.126504] Iteration 8400, train loss = 2.495716, train accuracy = 0.148438\n",
      "[2018-07-16 20:25:38.243452] Iteration 8500, train loss = 2.520421, train accuracy = 0.179688\n",
      "[2018-07-16 20:25:43.422538] Iteration 8600, train loss = 2.664677, train accuracy = 0.164062\n",
      "[2018-07-16 20:25:48.530418] Iteration 8700, train loss = 2.366220, train accuracy = 0.218750\n",
      "[2018-07-16 20:25:53.680711] Iteration 8800, train loss = 2.573064, train accuracy = 0.156250\n",
      "[2018-07-16 20:25:58.852600] Iteration 8900, train loss = 2.436380, train accuracy = 0.156250\n",
      "[2018-07-16 20:26:03.941214] Iteration 9000, train loss = 2.345721, train accuracy = 0.226562\n",
      "Evaluating...\n",
      "Test accuracy = 0.165800\n",
      "[2018-07-16 20:26:10.925206] Iteration 9100, train loss = 2.481138, train accuracy = 0.171875\n",
      "[2018-07-16 20:26:16.030839] Iteration 9200, train loss = 2.691220, train accuracy = 0.109375\n",
      "[2018-07-16 20:26:20.982845] Iteration 9300, train loss = 2.619091, train accuracy = 0.109375\n",
      "[2018-07-16 20:26:26.115606] Iteration 9400, train loss = 2.631176, train accuracy = 0.156250\n",
      "[2018-07-16 20:26:31.288865] Iteration 9500, train loss = 2.528117, train accuracy = 0.156250\n",
      "[2018-07-16 20:26:36.473358] Iteration 9600, train loss = 2.585625, train accuracy = 0.148438\n",
      "[2018-07-16 20:26:41.663750] Iteration 9700, train loss = 2.473163, train accuracy = 0.171875\n",
      "[2018-07-16 20:26:46.771287] Iteration 9800, train loss = 2.603076, train accuracy = 0.148438\n",
      "[2018-07-16 20:26:51.897110] Iteration 9900, train loss = 2.551269, train accuracy = 0.117188\n",
      "[2018-07-16 20:26:57.094985] Iteration 10000, train loss = 2.431544, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.166000\n",
      "[2018-07-16 20:27:04.085055] Iteration 10100, train loss = 2.490431, train accuracy = 0.132812\n",
      "[2018-07-16 20:27:09.239597] Iteration 10200, train loss = 2.590763, train accuracy = 0.117188\n",
      "[2018-07-16 20:27:14.356265] Iteration 10300, train loss = 2.411038, train accuracy = 0.132812\n",
      "[2018-07-16 20:27:19.518932] Iteration 10400, train loss = 2.409442, train accuracy = 0.179688\n",
      "[2018-07-16 20:27:24.617557] Iteration 10500, train loss = 2.478765, train accuracy = 0.210938\n",
      "[2018-07-16 20:27:29.664232] Iteration 10600, train loss = 2.654171, train accuracy = 0.140625\n",
      "[2018-07-16 20:27:34.840102] Iteration 10700, train loss = 2.411517, train accuracy = 0.179688\n",
      "[2018-07-16 20:27:39.928456] Iteration 10800, train loss = 2.464074, train accuracy = 0.164062\n",
      "[2018-07-16 20:27:45.086138] Iteration 10900, train loss = 2.531405, train accuracy = 0.171875\n",
      "[2018-07-16 20:27:50.272323] Iteration 11000, train loss = 2.466066, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164500\n",
      "[2018-07-16 20:27:57.304191] Iteration 11100, train loss = 2.510849, train accuracy = 0.164062\n",
      "[2018-07-16 20:28:02.451859] Iteration 11200, train loss = 2.387860, train accuracy = 0.171875\n",
      "[2018-07-16 20:28:07.572926] Iteration 11300, train loss = 2.422033, train accuracy = 0.148438\n",
      "[2018-07-16 20:28:12.754226] Iteration 11400, train loss = 2.663857, train accuracy = 0.187500\n",
      "[2018-07-16 20:28:17.930745] Iteration 11500, train loss = 2.635806, train accuracy = 0.109375\n",
      "[2018-07-16 20:28:23.018805] Iteration 11600, train loss = 2.620682, train accuracy = 0.148438\n",
      "[2018-07-16 20:28:28.153943] Iteration 11700, train loss = 2.577993, train accuracy = 0.179688\n",
      "[2018-07-16 20:28:33.336466] Iteration 11800, train loss = 2.586390, train accuracy = 0.203125\n",
      "[2018-07-16 20:28:38.575812] Iteration 11900, train loss = 2.492104, train accuracy = 0.179688\n",
      "[2018-07-16 20:28:43.647802] Iteration 12000, train loss = 2.464173, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.164600\n",
      "[2018-07-16 20:28:50.648830] Iteration 12100, train loss = 2.700794, train accuracy = 0.093750\n",
      "[2018-07-16 20:28:55.763338] Iteration 12200, train loss = 2.770718, train accuracy = 0.148438\n",
      "[2018-07-16 20:29:00.737758] Iteration 12300, train loss = 2.446055, train accuracy = 0.187500\n",
      "[2018-07-16 20:29:05.682980] Iteration 12400, train loss = 2.459772, train accuracy = 0.148438\n",
      "[2018-07-16 20:29:10.840996] Iteration 12500, train loss = 2.497283, train accuracy = 0.164062\n",
      "[2018-07-16 20:29:15.995794] Iteration 12600, train loss = 2.454184, train accuracy = 0.156250\n",
      "[2018-07-16 20:29:21.162733] Iteration 12700, train loss = 2.436810, train accuracy = 0.187500\n",
      "[2018-07-16 20:29:26.355538] Iteration 12800, train loss = 2.369433, train accuracy = 0.187500\n",
      "[2018-07-16 20:29:31.529698] Iteration 12900, train loss = 2.468725, train accuracy = 0.187500\n",
      "[2018-07-16 20:29:36.704793] Iteration 13000, train loss = 2.496710, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.166400\n",
      "[2018-07-16 20:29:43.756043] Iteration 13100, train loss = 2.500493, train accuracy = 0.187500\n",
      "[2018-07-16 20:29:48.911785] Iteration 13200, train loss = 2.499274, train accuracy = 0.140625\n",
      "[2018-07-16 20:29:54.078850] Iteration 13300, train loss = 2.416182, train accuracy = 0.210938\n",
      "[2018-07-16 20:29:59.258176] Iteration 13400, train loss = 2.585974, train accuracy = 0.140625\n",
      "[2018-07-16 20:30:04.433540] Iteration 13500, train loss = 2.527112, train accuracy = 0.148438\n",
      "[2018-07-16 20:30:09.611572] Iteration 13600, train loss = 2.461617, train accuracy = 0.148438\n",
      "[2018-07-16 20:30:14.777770] Iteration 13700, train loss = 2.535185, train accuracy = 0.156250\n",
      "[2018-07-16 20:30:19.764888] Iteration 13800, train loss = 2.364628, train accuracy = 0.218750\n",
      "[2018-07-16 20:30:24.758291] Iteration 13900, train loss = 2.527963, train accuracy = 0.187500\n",
      "[2018-07-16 20:30:29.867984] Iteration 14000, train loss = 2.460986, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.164300\n",
      "[2018-07-16 20:30:36.825340] Iteration 14100, train loss = 2.562188, train accuracy = 0.148438\n",
      "[2018-07-16 20:30:41.899933] Iteration 14200, train loss = 2.591210, train accuracy = 0.156250\n",
      "[2018-07-16 20:30:47.006465] Iteration 14300, train loss = 2.640419, train accuracy = 0.164062\n",
      "[2018-07-16 20:30:52.146400] Iteration 14400, train loss = 2.491184, train accuracy = 0.195312\n",
      "[2018-07-16 20:30:57.318224] Iteration 14500, train loss = 2.678975, train accuracy = 0.125000\n",
      "[2018-07-16 20:31:02.484705] Iteration 14600, train loss = 2.496713, train accuracy = 0.179688\n",
      "[2018-07-16 20:31:07.682235] Iteration 14700, train loss = 2.481513, train accuracy = 0.156250\n",
      "[2018-07-16 20:31:12.863699] Iteration 14800, train loss = 2.638776, train accuracy = 0.070312\n",
      "[2018-07-16 20:31:18.032892] Iteration 14900, train loss = 2.514574, train accuracy = 0.148438\n",
      "[2018-07-16 20:31:23.219177] Iteration 15000, train loss = 2.592648, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164500\n",
      "[2018-07-16 20:31:30.254905] Iteration 15100, train loss = 2.451809, train accuracy = 0.187500\n",
      "[2018-07-16 20:31:35.429253] Iteration 15200, train loss = 2.573190, train accuracy = 0.132812\n",
      "[2018-07-16 20:31:40.409519] Iteration 15300, train loss = 2.438068, train accuracy = 0.171875\n",
      "[2018-07-16 20:31:45.443237] Iteration 15400, train loss = 2.512509, train accuracy = 0.187500\n",
      "[2018-07-16 20:31:50.511989] Iteration 15500, train loss = 2.415089, train accuracy = 0.187500\n",
      "[2018-07-16 20:31:55.709945] Iteration 15600, train loss = 2.461154, train accuracy = 0.187500\n",
      "[2018-07-16 20:32:00.896258] Iteration 15700, train loss = 2.494585, train accuracy = 0.132812\n",
      "[2018-07-16 20:32:05.970340] Iteration 15800, train loss = 2.580036, train accuracy = 0.179688\n",
      "[2018-07-16 20:32:11.040599] Iteration 15900, train loss = 2.515747, train accuracy = 0.179688\n",
      "[2018-07-16 20:32:16.222531] Iteration 16000, train loss = 2.606041, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 20:32:23.265190] Iteration 16100, train loss = 2.420323, train accuracy = 0.210938\n",
      "[2018-07-16 20:32:28.424327] Iteration 16200, train loss = 2.437853, train accuracy = 0.164062\n",
      "[2018-07-16 20:32:33.560229] Iteration 16300, train loss = 2.544280, train accuracy = 0.187500\n",
      "[2018-07-16 20:32:38.565379] Iteration 16400, train loss = 2.525506, train accuracy = 0.187500\n",
      "[2018-07-16 20:32:43.711250] Iteration 16500, train loss = 2.532854, train accuracy = 0.117188\n",
      "[2018-07-16 20:32:48.788257] Iteration 16600, train loss = 2.494136, train accuracy = 0.148438\n",
      "[2018-07-16 20:32:53.894284] Iteration 16700, train loss = 2.486157, train accuracy = 0.156250\n",
      "[2018-07-16 20:32:59.060523] Iteration 16800, train loss = 2.614687, train accuracy = 0.171875\n",
      "[2018-07-16 20:33:04.220771] Iteration 16900, train loss = 2.693731, train accuracy = 0.117188\n",
      "[2018-07-16 20:33:09.385632] Iteration 17000, train loss = 2.635538, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165000\n",
      "[2018-07-16 20:33:16.371303] Iteration 17100, train loss = 2.504003, train accuracy = 0.148438\n",
      "[2018-07-16 20:33:21.539870] Iteration 17200, train loss = 2.500728, train accuracy = 0.164062\n",
      "[2018-07-16 20:33:26.710406] Iteration 17300, train loss = 2.541308, train accuracy = 0.125000\n",
      "[2018-07-16 20:33:31.894407] Iteration 17400, train loss = 2.524378, train accuracy = 0.171875\n",
      "[2018-07-16 20:33:37.097248] Iteration 17500, train loss = 2.502333, train accuracy = 0.164062\n",
      "[2018-07-16 20:33:42.260419] Iteration 17600, train loss = 2.691814, train accuracy = 0.132812\n",
      "[2018-07-16 20:33:47.412979] Iteration 17700, train loss = 2.516826, train accuracy = 0.117188\n",
      "[2018-07-16 20:33:52.408072] Iteration 17800, train loss = 2.448631, train accuracy = 0.234375\n",
      "[2018-07-16 20:33:57.581128] Iteration 17900, train loss = 2.557782, train accuracy = 0.195312\n",
      "[2018-07-16 20:34:02.742620] Iteration 18000, train loss = 2.455128, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164100\n",
      "[2018-07-16 20:34:09.780488] Iteration 18100, train loss = 2.745700, train accuracy = 0.117188\n",
      "[2018-07-16 20:34:14.902085] Iteration 18200, train loss = 2.508192, train accuracy = 0.117188\n",
      "[2018-07-16 20:34:20.089310] Iteration 18300, train loss = 2.450733, train accuracy = 0.171875\n",
      "[2018-07-16 20:34:25.267597] Iteration 18400, train loss = 2.473527, train accuracy = 0.148438\n",
      "[2018-07-16 20:34:30.407027] Iteration 18500, train loss = 2.481109, train accuracy = 0.179688\n",
      "[2018-07-16 20:34:35.556895] Iteration 18600, train loss = 2.385078, train accuracy = 0.218750\n",
      "[2018-07-16 20:34:40.684195] Iteration 18700, train loss = 2.625205, train accuracy = 0.117188\n",
      "[2018-07-16 20:34:45.769175] Iteration 18800, train loss = 2.636065, train accuracy = 0.171875\n",
      "[2018-07-16 20:34:50.874670] Iteration 18900, train loss = 2.496516, train accuracy = 0.117188\n",
      "[2018-07-16 20:34:56.039443] Iteration 19000, train loss = 2.458617, train accuracy = 0.195312\n",
      "Evaluating...\n",
      "Test accuracy = 0.165400\n",
      "[2018-07-16 20:35:03.111565] Iteration 19100, train loss = 2.469456, train accuracy = 0.210938\n",
      "[2018-07-16 20:35:08.246845] Iteration 19200, train loss = 2.585526, train accuracy = 0.171875\n",
      "[2018-07-16 20:35:13.437432] Iteration 19300, train loss = 2.566467, train accuracy = 0.218750\n",
      "[2018-07-16 20:35:18.605955] Iteration 19400, train loss = 2.411524, train accuracy = 0.195312\n",
      "[2018-07-16 20:35:23.767351] Iteration 19500, train loss = 2.612485, train accuracy = 0.187500\n",
      "[2018-07-16 20:35:28.938749] Iteration 19600, train loss = 2.373528, train accuracy = 0.195312\n",
      "[2018-07-16 20:35:34.111292] Iteration 19700, train loss = 2.342348, train accuracy = 0.273438\n",
      "[2018-07-16 20:35:39.294960] Iteration 19800, train loss = 2.656528, train accuracy = 0.117188\n",
      "[2018-07-16 20:35:44.468346] Iteration 19900, train loss = 2.520391, train accuracy = 0.218750\n",
      "[2018-07-16 20:35:49.624831] Iteration 20000, train loss = 2.502127, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.164200\n",
      "[2018-07-16 20:35:56.632523] Iteration 20100, train loss = 2.523477, train accuracy = 0.140625\n",
      "[2018-07-16 20:36:01.700075] Iteration 20200, train loss = 2.668738, train accuracy = 0.109375\n",
      "[2018-07-16 20:36:06.782317] Iteration 20300, train loss = 2.406687, train accuracy = 0.140625\n",
      "[2018-07-16 20:36:11.952018] Iteration 20400, train loss = 2.492218, train accuracy = 0.171875\n",
      "[2018-07-16 20:36:17.123492] Iteration 20500, train loss = 2.399632, train accuracy = 0.179688\n",
      "[2018-07-16 20:36:22.298758] Iteration 20600, train loss = 2.431910, train accuracy = 0.171875\n",
      "[2018-07-16 20:36:27.474461] Iteration 20700, train loss = 2.493066, train accuracy = 0.218750\n",
      "[2018-07-16 20:36:32.574562] Iteration 20800, train loss = 2.521535, train accuracy = 0.179688\n",
      "[2018-07-16 20:36:37.747388] Iteration 20900, train loss = 2.498931, train accuracy = 0.179688\n",
      "[2018-07-16 20:36:42.861842] Iteration 21000, train loss = 2.488116, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.164600\n",
      "[2018-07-16 20:36:49.688741] Iteration 21100, train loss = 2.533832, train accuracy = 0.156250\n",
      "[2018-07-16 20:36:54.760287] Iteration 21200, train loss = 2.575799, train accuracy = 0.117188\n",
      "[2018-07-16 20:36:59.855091] Iteration 21300, train loss = 2.496264, train accuracy = 0.187500\n",
      "[2018-07-16 20:37:05.005780] Iteration 21400, train loss = 2.532185, train accuracy = 0.171875\n",
      "[2018-07-16 20:37:10.169189] Iteration 21500, train loss = 2.519788, train accuracy = 0.132812\n",
      "[2018-07-16 20:37:15.352479] Iteration 21600, train loss = 2.437373, train accuracy = 0.187500\n",
      "[2018-07-16 20:37:20.493740] Iteration 21700, train loss = 2.498828, train accuracy = 0.187500\n",
      "[2018-07-16 20:37:25.676895] Iteration 21800, train loss = 2.462985, train accuracy = 0.156250\n",
      "[2018-07-16 20:37:30.757851] Iteration 21900, train loss = 2.560936, train accuracy = 0.164062\n",
      "[2018-07-16 20:37:35.918834] Iteration 22000, train loss = 2.463110, train accuracy = 0.234375\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 20:37:42.987736] Iteration 22100, train loss = 2.549074, train accuracy = 0.156250\n",
      "[2018-07-16 20:37:48.177597] Iteration 22200, train loss = 2.546081, train accuracy = 0.148438\n",
      "[2018-07-16 20:37:53.362817] Iteration 22300, train loss = 2.609121, train accuracy = 0.148438\n",
      "[2018-07-16 20:37:58.501728] Iteration 22400, train loss = 2.366020, train accuracy = 0.218750\n",
      "[2018-07-16 20:38:03.619802] Iteration 22500, train loss = 2.613106, train accuracy = 0.109375\n",
      "[2018-07-16 20:38:08.789768] Iteration 22600, train loss = 2.760391, train accuracy = 0.164062\n",
      "[2018-07-16 20:38:13.957818] Iteration 22700, train loss = 2.540031, train accuracy = 0.164062\n",
      "[2018-07-16 20:38:19.129360] Iteration 22800, train loss = 2.559810, train accuracy = 0.156250\n",
      "[2018-07-16 20:38:24.243860] Iteration 22900, train loss = 2.615974, train accuracy = 0.117188\n",
      "[2018-07-16 20:38:29.378608] Iteration 23000, train loss = 2.443742, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.165700\n",
      "[2018-07-16 20:38:36.379597] Iteration 23100, train loss = 2.521065, train accuracy = 0.171875\n",
      "[2018-07-16 20:38:41.508288] Iteration 23200, train loss = 2.450824, train accuracy = 0.195312\n",
      "[2018-07-16 20:38:46.627701] Iteration 23300, train loss = 2.478994, train accuracy = 0.164062\n",
      "[2018-07-16 20:38:51.681465] Iteration 23400, train loss = 2.725630, train accuracy = 0.117188\n",
      "[2018-07-16 20:38:56.858913] Iteration 23500, train loss = 2.464561, train accuracy = 0.156250\n",
      "[2018-07-16 20:39:02.023942] Iteration 23600, train loss = 2.553216, train accuracy = 0.171875\n",
      "[2018-07-16 20:39:07.151753] Iteration 23700, train loss = 2.580301, train accuracy = 0.164062\n",
      "[2018-07-16 20:39:12.314931] Iteration 23800, train loss = 2.421908, train accuracy = 0.171875\n",
      "[2018-07-16 20:39:17.498387] Iteration 23900, train loss = 2.582565, train accuracy = 0.140625\n",
      "[2018-07-16 20:39:22.664392] Iteration 24000, train loss = 2.794102, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.164600\n",
      "[2018-07-16 20:39:29.695131] Iteration 24100, train loss = 2.467938, train accuracy = 0.210938\n",
      "[2018-07-16 20:39:34.830644] Iteration 24200, train loss = 2.399250, train accuracy = 0.132812\n",
      "[2018-07-16 20:39:39.876278] Iteration 24300, train loss = 2.575304, train accuracy = 0.164062\n",
      "[2018-07-16 20:39:44.866757] Iteration 24400, train loss = 2.550131, train accuracy = 0.125000\n",
      "[2018-07-16 20:39:50.055656] Iteration 24500, train loss = 2.742234, train accuracy = 0.109375\n",
      "[2018-07-16 20:39:55.237072] Iteration 24600, train loss = 2.534297, train accuracy = 0.132812\n",
      "[2018-07-16 20:40:00.413830] Iteration 24700, train loss = 2.515492, train accuracy = 0.140625\n",
      "[2018-07-16 20:40:05.552134] Iteration 24800, train loss = 2.563283, train accuracy = 0.164062\n",
      "[2018-07-16 20:40:10.525311] Iteration 24900, train loss = 2.643055, train accuracy = 0.132812\n",
      "[2018-07-16 20:40:15.664065] Iteration 25000, train loss = 2.495335, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 20:40:22.643885] Iteration 25100, train loss = 2.558098, train accuracy = 0.109375\n",
      "[2018-07-16 20:40:27.818267] Iteration 25200, train loss = 2.353406, train accuracy = 0.187500\n",
      "[2018-07-16 20:40:32.970349] Iteration 25300, train loss = 2.550349, train accuracy = 0.140625\n",
      "[2018-07-16 20:40:38.075208] Iteration 25400, train loss = 2.674500, train accuracy = 0.140625\n",
      "[2018-07-16 20:40:43.252865] Iteration 25500, train loss = 2.613129, train accuracy = 0.117188\n",
      "[2018-07-16 20:40:48.372114] Iteration 25600, train loss = 2.666667, train accuracy = 0.117188\n",
      "[2018-07-16 20:40:53.518622] Iteration 25700, train loss = 2.690808, train accuracy = 0.140625\n",
      "[2018-07-16 20:40:58.677847] Iteration 25800, train loss = 2.470881, train accuracy = 0.210938\n",
      "[2018-07-16 20:41:03.840578] Iteration 25900, train loss = 2.557896, train accuracy = 0.187500\n",
      "[2018-07-16 20:41:09.021505] Iteration 26000, train loss = 2.610797, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 20:41:15.996149] Iteration 26100, train loss = 2.572103, train accuracy = 0.109375\n",
      "[2018-07-16 20:41:21.042354] Iteration 26200, train loss = 2.507475, train accuracy = 0.171875\n",
      "[2018-07-16 20:41:26.166057] Iteration 26300, train loss = 2.543649, train accuracy = 0.125000\n",
      "[2018-07-16 20:41:31.334327] Iteration 26400, train loss = 2.416215, train accuracy = 0.210938\n",
      "[2018-07-16 20:41:36.465892] Iteration 26500, train loss = 2.394063, train accuracy = 0.179688\n",
      "[2018-07-16 20:41:41.587098] Iteration 26600, train loss = 2.616775, train accuracy = 0.117188\n",
      "[2018-07-16 20:41:46.688838] Iteration 26700, train loss = 2.411561, train accuracy = 0.179688\n",
      "[2018-07-16 20:41:51.834078] Iteration 26800, train loss = 2.495054, train accuracy = 0.250000\n",
      "[2018-07-16 20:41:56.983628] Iteration 26900, train loss = 2.420512, train accuracy = 0.179688\n",
      "[2018-07-16 20:42:02.167313] Iteration 27000, train loss = 2.612180, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 20:42:09.134236] Iteration 27100, train loss = 2.459483, train accuracy = 0.164062\n",
      "[2018-07-16 20:42:14.259409] Iteration 27200, train loss = 2.506141, train accuracy = 0.218750\n",
      "[2018-07-16 20:42:19.426780] Iteration 27300, train loss = 2.756934, train accuracy = 0.140625\n",
      "[2018-07-16 20:42:24.611012] Iteration 27400, train loss = 2.492656, train accuracy = 0.140625\n",
      "[2018-07-16 20:42:29.788187] Iteration 27500, train loss = 2.409900, train accuracy = 0.179688\n",
      "[2018-07-16 20:42:35.005119] Iteration 27600, train loss = 2.479733, train accuracy = 0.218750\n",
      "[2018-07-16 20:42:40.120347] Iteration 27700, train loss = 2.506439, train accuracy = 0.203125\n",
      "[2018-07-16 20:42:45.122723] Iteration 27800, train loss = 2.591991, train accuracy = 0.164062\n",
      "[2018-07-16 20:42:50.231709] Iteration 27900, train loss = 2.521313, train accuracy = 0.132812\n",
      "[2018-07-16 20:42:55.372283] Iteration 28000, train loss = 2.663645, train accuracy = 0.257812\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 20:43:02.358449] Iteration 28100, train loss = 2.564157, train accuracy = 0.132812\n",
      "[2018-07-16 20:43:07.514580] Iteration 28200, train loss = 2.500721, train accuracy = 0.171875\n",
      "[2018-07-16 20:43:12.663626] Iteration 28300, train loss = 2.506635, train accuracy = 0.164062\n",
      "[2018-07-16 20:43:17.779213] Iteration 28400, train loss = 2.501870, train accuracy = 0.218750\n",
      "[2018-07-16 20:43:22.945729] Iteration 28500, train loss = 2.553933, train accuracy = 0.187500\n",
      "[2018-07-16 20:43:28.141698] Iteration 28600, train loss = 2.578734, train accuracy = 0.148438\n",
      "[2018-07-16 20:43:33.304586] Iteration 28700, train loss = 2.720192, train accuracy = 0.109375\n",
      "[2018-07-16 20:43:38.422257] Iteration 28800, train loss = 2.528913, train accuracy = 0.164062\n",
      "[2018-07-16 20:43:43.536267] Iteration 28900, train loss = 2.557305, train accuracy = 0.140625\n",
      "[2018-07-16 20:43:48.701957] Iteration 29000, train loss = 2.571108, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.165200\n",
      "[2018-07-16 20:43:55.735915] Iteration 29100, train loss = 2.503857, train accuracy = 0.164062\n",
      "[2018-07-16 20:44:00.842915] Iteration 29200, train loss = 2.391462, train accuracy = 0.203125\n",
      "[2018-07-16 20:44:05.932942] Iteration 29300, train loss = 2.595078, train accuracy = 0.140625\n",
      "[2018-07-16 20:44:11.074622] Iteration 29400, train loss = 2.489269, train accuracy = 0.210938\n",
      "[2018-07-16 20:44:16.258759] Iteration 29500, train loss = 2.443444, train accuracy = 0.164062\n",
      "[2018-07-16 20:44:21.434683] Iteration 29600, train loss = 2.422277, train accuracy = 0.187500\n",
      "[2018-07-16 20:44:26.626934] Iteration 29700, train loss = 2.582767, train accuracy = 0.148438\n",
      "[2018-07-16 20:44:31.783133] Iteration 29800, train loss = 2.467320, train accuracy = 0.148438\n",
      "[2018-07-16 20:44:36.939620] Iteration 29900, train loss = 2.635193, train accuracy = 0.109375\n",
      "[2018-07-16 20:44:42.122869] Iteration 30000, train loss = 2.431309, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.164000\n",
      "[2018-07-16 20:44:49.180203] Iteration 30100, train loss = 2.520944, train accuracy = 0.140625\n",
      "[2018-07-16 20:44:54.347664] Iteration 30200, train loss = 2.443590, train accuracy = 0.195312\n",
      "[2018-07-16 20:44:59.497978] Iteration 30300, train loss = 2.568859, train accuracy = 0.140625\n",
      "[2018-07-16 20:45:04.681616] Iteration 30400, train loss = 2.571777, train accuracy = 0.187500\n",
      "[2018-07-16 20:45:09.860782] Iteration 30500, train loss = 2.410929, train accuracy = 0.187500\n",
      "[2018-07-16 20:45:15.027815] Iteration 30600, train loss = 2.559565, train accuracy = 0.156250\n",
      "[2018-07-16 20:45:20.214107] Iteration 30700, train loss = 2.703302, train accuracy = 0.132812\n",
      "[2018-07-16 20:45:25.374704] Iteration 30800, train loss = 2.515177, train accuracy = 0.156250\n",
      "[2018-07-16 20:45:30.538396] Iteration 30900, train loss = 2.368347, train accuracy = 0.195312\n",
      "[2018-07-16 20:45:35.674555] Iteration 31000, train loss = 2.532928, train accuracy = 0.226562\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 20:45:42.698385] Iteration 31100, train loss = 2.506275, train accuracy = 0.125000\n",
      "[2018-07-16 20:45:47.820087] Iteration 31200, train loss = 2.377579, train accuracy = 0.148438\n",
      "[2018-07-16 20:45:53.016328] Iteration 31300, train loss = 2.492821, train accuracy = 0.164062\n",
      "[2018-07-16 20:45:58.143186] Iteration 31400, train loss = 2.600197, train accuracy = 0.085938\n",
      "[2018-07-16 20:46:03.147260] Iteration 31500, train loss = 2.561313, train accuracy = 0.117188\n",
      "[2018-07-16 20:46:08.271280] Iteration 31600, train loss = 2.389802, train accuracy = 0.171875\n",
      "[2018-07-16 20:46:13.442477] Iteration 31700, train loss = 2.435333, train accuracy = 0.187500\n",
      "[2018-07-16 20:46:18.548238] Iteration 31800, train loss = 2.490535, train accuracy = 0.156250\n",
      "[2018-07-16 20:46:23.583681] Iteration 31900, train loss = 2.626002, train accuracy = 0.125000\n",
      "[2018-07-16 20:46:28.586017] Iteration 32000, train loss = 2.482317, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.164700\n",
      "[2018-07-16 20:46:35.622628] Iteration 32100, train loss = 2.523725, train accuracy = 0.156250\n",
      "[2018-07-16 20:46:40.751387] Iteration 32200, train loss = 2.568653, train accuracy = 0.164062\n",
      "[2018-07-16 20:46:45.934770] Iteration 32300, train loss = 2.444836, train accuracy = 0.148438\n",
      "[2018-07-16 20:46:51.109818] Iteration 32400, train loss = 2.523797, train accuracy = 0.156250\n",
      "[2018-07-16 20:46:56.246170] Iteration 32500, train loss = 2.486762, train accuracy = 0.156250\n",
      "[2018-07-16 20:47:01.212658] Iteration 32600, train loss = 2.460483, train accuracy = 0.164062\n",
      "[2018-07-16 20:47:06.357511] Iteration 32700, train loss = 2.606644, train accuracy = 0.148438\n",
      "[2018-07-16 20:47:11.513298] Iteration 32800, train loss = 2.574580, train accuracy = 0.132812\n",
      "[2018-07-16 20:47:16.559506] Iteration 32900, train loss = 2.529389, train accuracy = 0.140625\n",
      "[2018-07-16 20:47:21.711033] Iteration 33000, train loss = 2.406729, train accuracy = 0.203125\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 20:47:28.723685] Iteration 33100, train loss = 2.549397, train accuracy = 0.148438\n",
      "[2018-07-16 20:47:33.818825] Iteration 33200, train loss = 2.377892, train accuracy = 0.187500\n",
      "[2018-07-16 20:47:38.990707] Iteration 33300, train loss = 2.516418, train accuracy = 0.140625\n",
      "[2018-07-16 20:47:44.176358] Iteration 33400, train loss = 2.435277, train accuracy = 0.234375\n",
      "[2018-07-16 20:47:49.336425] Iteration 33500, train loss = 2.578196, train accuracy = 0.156250\n",
      "[2018-07-16 20:47:54.485417] Iteration 33600, train loss = 2.627162, train accuracy = 0.179688\n",
      "[2018-07-16 20:47:59.661961] Iteration 33700, train loss = 2.698959, train accuracy = 0.140625\n",
      "[2018-07-16 20:48:04.825061] Iteration 33800, train loss = 2.596503, train accuracy = 0.218750\n",
      "[2018-07-16 20:48:09.999095] Iteration 33900, train loss = 2.446558, train accuracy = 0.148438\n",
      "[2018-07-16 20:48:15.018374] Iteration 34000, train loss = 2.489409, train accuracy = 0.195312\n",
      "Evaluating...\n",
      "Test accuracy = 0.164400\n",
      "[2018-07-16 20:48:22.053223] Iteration 34100, train loss = 2.634953, train accuracy = 0.132812\n",
      "[2018-07-16 20:48:27.178684] Iteration 34200, train loss = 2.464716, train accuracy = 0.125000\n",
      "[2018-07-16 20:48:32.363866] Iteration 34300, train loss = 2.599694, train accuracy = 0.117188\n",
      "[2018-07-16 20:48:37.521738] Iteration 34400, train loss = 2.690997, train accuracy = 0.109375\n",
      "[2018-07-16 20:48:42.702831] Iteration 34500, train loss = 2.486959, train accuracy = 0.218750\n",
      "[2018-07-16 20:48:47.871514] Iteration 34600, train loss = 2.661575, train accuracy = 0.171875\n",
      "[2018-07-16 20:48:53.051356] Iteration 34700, train loss = 2.605930, train accuracy = 0.132812\n",
      "[2018-07-16 20:48:58.210541] Iteration 34800, train loss = 2.503417, train accuracy = 0.171875\n",
      "[2018-07-16 20:49:03.363199] Iteration 34900, train loss = 2.419472, train accuracy = 0.179688\n",
      "[2018-07-16 20:49:08.539846] Iteration 35000, train loss = 2.463238, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165400\n",
      "[2018-07-16 20:49:15.650000] Iteration 35100, train loss = 2.494787, train accuracy = 0.195312\n",
      "[2018-07-16 20:49:20.835000] Iteration 35200, train loss = 2.563228, train accuracy = 0.164062\n",
      "[2018-07-16 20:49:26.031366] Iteration 35300, train loss = 2.483439, train accuracy = 0.218750\n",
      "[2018-07-16 20:49:31.199308] Iteration 35400, train loss = 2.578885, train accuracy = 0.093750\n",
      "[2018-07-16 20:49:36.287496] Iteration 35500, train loss = 2.367217, train accuracy = 0.179688\n",
      "[2018-07-16 20:49:41.471128] Iteration 35600, train loss = 2.445173, train accuracy = 0.171875\n",
      "[2018-07-16 20:49:46.654435] Iteration 35700, train loss = 2.446903, train accuracy = 0.164062\n",
      "[2018-07-16 20:49:51.829849] Iteration 35800, train loss = 2.479747, train accuracy = 0.218750\n",
      "[2018-07-16 20:49:57.000149] Iteration 35900, train loss = 2.454777, train accuracy = 0.210938\n",
      "[2018-07-16 20:50:01.996937] Iteration 36000, train loss = 2.591916, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.164700\n",
      "[2018-07-16 20:50:08.795188] Iteration 36100, train loss = 2.434405, train accuracy = 0.156250\n",
      "[2018-07-16 20:50:13.933733] Iteration 36200, train loss = 2.447472, train accuracy = 0.179688\n",
      "[2018-07-16 20:50:18.983752] Iteration 36300, train loss = 2.559036, train accuracy = 0.117188\n",
      "[2018-07-16 20:50:24.168441] Iteration 36400, train loss = 2.427463, train accuracy = 0.164062\n",
      "[2018-07-16 20:50:29.284650] Iteration 36500, train loss = 2.507992, train accuracy = 0.171875\n",
      "[2018-07-16 20:50:34.456971] Iteration 36600, train loss = 2.592754, train accuracy = 0.156250\n",
      "[2018-07-16 20:50:39.639605] Iteration 36700, train loss = 2.665956, train accuracy = 0.156250\n",
      "[2018-07-16 20:50:44.685938] Iteration 36800, train loss = 2.492074, train accuracy = 0.164062\n",
      "[2018-07-16 20:50:49.816732] Iteration 36900, train loss = 2.544432, train accuracy = 0.210938\n",
      "[2018-07-16 20:50:54.964463] Iteration 37000, train loss = 2.464132, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165200\n",
      "[2018-07-16 20:51:02.008221] Iteration 37100, train loss = 2.463499, train accuracy = 0.179688\n",
      "[2018-07-16 20:51:07.170249] Iteration 37200, train loss = 2.608151, train accuracy = 0.132812\n",
      "[2018-07-16 20:51:12.334873] Iteration 37300, train loss = 2.433496, train accuracy = 0.203125\n",
      "[2018-07-16 20:51:17.497467] Iteration 37400, train loss = 2.571954, train accuracy = 0.179688\n",
      "[2018-07-16 20:51:22.670313] Iteration 37500, train loss = 2.513798, train accuracy = 0.164062\n",
      "[2018-07-16 20:51:27.829812] Iteration 37600, train loss = 2.325382, train accuracy = 0.226562\n",
      "[2018-07-16 20:51:32.890642] Iteration 37700, train loss = 2.508424, train accuracy = 0.195312\n",
      "[2018-07-16 20:51:38.077750] Iteration 37800, train loss = 2.576366, train accuracy = 0.218750\n",
      "[2018-07-16 20:51:42.957756] Iteration 37900, train loss = 2.431896, train accuracy = 0.171875\n",
      "[2018-07-16 20:51:47.997917] Iteration 38000, train loss = 2.643542, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.165500\n",
      "[2018-07-16 20:51:55.050281] Iteration 38100, train loss = 2.505479, train accuracy = 0.164062\n",
      "[2018-07-16 20:52:00.221199] Iteration 38200, train loss = 2.501703, train accuracy = 0.171875\n",
      "[2018-07-16 20:52:05.410398] Iteration 38300, train loss = 2.491858, train accuracy = 0.171875\n",
      "[2018-07-16 20:52:10.530213] Iteration 38400, train loss = 2.580435, train accuracy = 0.132812\n",
      "[2018-07-16 20:52:15.713529] Iteration 38500, train loss = 2.477298, train accuracy = 0.195312\n",
      "[2018-07-16 20:52:20.881233] Iteration 38600, train loss = 2.523649, train accuracy = 0.179688\n",
      "[2018-07-16 20:52:26.059912] Iteration 38700, train loss = 2.497330, train accuracy = 0.203125\n",
      "[2018-07-16 20:52:31.270317] Iteration 38800, train loss = 2.496930, train accuracy = 0.132812\n",
      "[2018-07-16 20:52:36.423427] Iteration 38900, train loss = 2.530891, train accuracy = 0.148438\n",
      "[2018-07-16 20:52:41.524445] Iteration 39000, train loss = 2.556306, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.165700\n",
      "[2018-07-16 20:52:48.555950] Iteration 39100, train loss = 2.611964, train accuracy = 0.117188\n",
      "[2018-07-16 20:52:53.697588] Iteration 39200, train loss = 2.497161, train accuracy = 0.132812\n",
      "[2018-07-16 20:52:58.879681] Iteration 39300, train loss = 2.474582, train accuracy = 0.187500\n",
      "[2018-07-16 20:53:04.033496] Iteration 39400, train loss = 2.570652, train accuracy = 0.164062\n",
      "[2018-07-16 20:53:09.060040] Iteration 39500, train loss = 2.438146, train accuracy = 0.148438\n",
      "[2018-07-16 20:53:14.117845] Iteration 39600, train loss = 2.456125, train accuracy = 0.164062\n",
      "[2018-07-16 20:53:19.292651] Iteration 39700, train loss = 2.393616, train accuracy = 0.179688\n",
      "[2018-07-16 20:53:24.326383] Iteration 39800, train loss = 2.542929, train accuracy = 0.117188\n",
      "[2018-07-16 20:53:29.432630] Iteration 39900, train loss = 2.477620, train accuracy = 0.179688\n",
      "[2018-07-16 20:53:34.455070] Iteration 40000, train loss = 2.479217, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.166200\n",
      "[2018-07-16 20:53:41.477952] Iteration 40100, train loss = 2.411282, train accuracy = 0.132812\n",
      "[2018-07-16 20:53:46.648745] Iteration 40200, train loss = 2.473065, train accuracy = 0.156250\n",
      "[2018-07-16 20:53:51.826180] Iteration 40300, train loss = 2.521170, train accuracy = 0.171875\n",
      "[2018-07-16 20:53:56.995483] Iteration 40400, train loss = 2.495330, train accuracy = 0.171875\n",
      "[2018-07-16 20:54:02.181772] Iteration 40500, train loss = 2.650334, train accuracy = 0.109375\n",
      "[2018-07-16 20:54:07.371963] Iteration 40600, train loss = 2.519343, train accuracy = 0.187500\n",
      "[2018-07-16 20:54:12.541319] Iteration 40700, train loss = 2.524080, train accuracy = 0.140625\n",
      "[2018-07-16 20:54:17.714537] Iteration 40800, train loss = 2.559763, train accuracy = 0.140625\n",
      "[2018-07-16 20:54:22.887133] Iteration 40900, train loss = 2.492670, train accuracy = 0.203125\n",
      "[2018-07-16 20:54:28.034574] Iteration 41000, train loss = 2.655914, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.166200\n",
      "[2018-07-16 20:54:35.014325] Iteration 41100, train loss = 2.363848, train accuracy = 0.234375\n",
      "[2018-07-16 20:54:40.069994] Iteration 41200, train loss = 2.608324, train accuracy = 0.156250\n",
      "[2018-07-16 20:54:45.147609] Iteration 41300, train loss = 2.514116, train accuracy = 0.140625\n",
      "[2018-07-16 20:54:50.315628] Iteration 41400, train loss = 2.544565, train accuracy = 0.109375\n",
      "[2018-07-16 20:54:55.425720] Iteration 41500, train loss = 2.650359, train accuracy = 0.132812\n",
      "[2018-07-16 20:55:00.570114] Iteration 41600, train loss = 2.444330, train accuracy = 0.140625\n",
      "[2018-07-16 20:55:05.736296] Iteration 41700, train loss = 2.458128, train accuracy = 0.195312\n",
      "[2018-07-16 20:55:10.905754] Iteration 41800, train loss = 2.609615, train accuracy = 0.164062\n",
      "[2018-07-16 20:55:16.074298] Iteration 41900, train loss = 2.570956, train accuracy = 0.132812\n",
      "[2018-07-16 20:55:21.025486] Iteration 42000, train loss = 2.599130, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 20:55:28.064620] Iteration 42100, train loss = 2.427819, train accuracy = 0.179688\n",
      "[2018-07-16 20:55:33.215362] Iteration 42200, train loss = 2.508441, train accuracy = 0.125000\n",
      "[2018-07-16 20:55:38.397515] Iteration 42300, train loss = 2.379618, train accuracy = 0.179688\n",
      "[2018-07-16 20:55:43.583848] Iteration 42400, train loss = 2.415024, train accuracy = 0.195312\n",
      "[2018-07-16 20:55:48.675974] Iteration 42500, train loss = 2.578923, train accuracy = 0.171875\n",
      "[2018-07-16 20:55:53.659812] Iteration 42600, train loss = 2.589658, train accuracy = 0.156250\n",
      "[2018-07-16 20:55:58.680868] Iteration 42700, train loss = 2.588244, train accuracy = 0.164062\n",
      "[2018-07-16 20:56:03.852974] Iteration 42800, train loss = 2.532657, train accuracy = 0.140625\n",
      "[2018-07-16 20:56:08.981732] Iteration 42900, train loss = 2.541052, train accuracy = 0.156250\n",
      "[2018-07-16 20:56:14.148665] Iteration 43000, train loss = 2.454180, train accuracy = 0.203125\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 20:56:21.169703] Iteration 43100, train loss = 2.374483, train accuracy = 0.210938\n",
      "[2018-07-16 20:56:26.261898] Iteration 43200, train loss = 2.577547, train accuracy = 0.132812\n",
      "[2018-07-16 20:56:31.414630] Iteration 43300, train loss = 2.498434, train accuracy = 0.148438\n",
      "[2018-07-16 20:56:36.604800] Iteration 43400, train loss = 2.761023, train accuracy = 0.132812\n",
      "[2018-07-16 20:56:41.655445] Iteration 43500, train loss = 2.528885, train accuracy = 0.179688\n",
      "[2018-07-16 20:56:46.828398] Iteration 43600, train loss = 2.579692, train accuracy = 0.117188\n",
      "[2018-07-16 20:56:51.981289] Iteration 43700, train loss = 2.495155, train accuracy = 0.164062\n",
      "[2018-07-16 20:56:57.137419] Iteration 43800, train loss = 2.627844, train accuracy = 0.140625\n",
      "[2018-07-16 20:57:02.219018] Iteration 43900, train loss = 2.558414, train accuracy = 0.156250\n",
      "[2018-07-16 20:57:07.235453] Iteration 44000, train loss = 2.620642, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 20:57:14.271203] Iteration 44100, train loss = 2.664240, train accuracy = 0.140625\n",
      "[2018-07-16 20:57:19.441772] Iteration 44200, train loss = 2.605543, train accuracy = 0.140625\n",
      "[2018-07-16 20:57:24.648594] Iteration 44300, train loss = 2.595951, train accuracy = 0.171875\n",
      "[2018-07-16 20:57:29.812654] Iteration 44400, train loss = 2.543366, train accuracy = 0.148438\n",
      "[2018-07-16 20:57:34.974828] Iteration 44500, train loss = 2.570369, train accuracy = 0.179688\n",
      "[2018-07-16 20:57:40.095780] Iteration 44600, train loss = 2.448340, train accuracy = 0.257812\n",
      "[2018-07-16 20:57:45.273133] Iteration 44700, train loss = 2.570388, train accuracy = 0.148438\n",
      "[2018-07-16 20:57:50.438824] Iteration 44800, train loss = 2.415532, train accuracy = 0.226562\n",
      "[2018-07-16 20:57:55.484148] Iteration 44900, train loss = 2.455028, train accuracy = 0.164062\n",
      "[2018-07-16 20:58:00.440827] Iteration 45000, train loss = 2.630830, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 20:58:07.476930] Iteration 45100, train loss = 2.696927, train accuracy = 0.140625\n",
      "[2018-07-16 20:58:12.624233] Iteration 45200, train loss = 2.600263, train accuracy = 0.179688\n",
      "[2018-07-16 20:58:17.784251] Iteration 45300, train loss = 2.548196, train accuracy = 0.179688\n",
      "[2018-07-16 20:58:22.924185] Iteration 45400, train loss = 2.634012, train accuracy = 0.117188\n",
      "[2018-07-16 20:58:28.105234] Iteration 45500, train loss = 2.435818, train accuracy = 0.203125\n",
      "[2018-07-16 20:58:33.265858] Iteration 45600, train loss = 2.619321, train accuracy = 0.125000\n",
      "[2018-07-16 20:58:38.441410] Iteration 45700, train loss = 2.456025, train accuracy = 0.218750\n",
      "[2018-07-16 20:58:43.597450] Iteration 45800, train loss = 2.463208, train accuracy = 0.132812\n",
      "[2018-07-16 20:58:48.702031] Iteration 45900, train loss = 2.519896, train accuracy = 0.179688\n",
      "[2018-07-16 20:58:53.783042] Iteration 46000, train loss = 2.497298, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.163600\n",
      "[2018-07-16 20:59:00.819953] Iteration 46100, train loss = 2.553540, train accuracy = 0.148438\n",
      "[2018-07-16 20:59:05.986778] Iteration 46200, train loss = 2.461761, train accuracy = 0.148438\n",
      "[2018-07-16 20:59:11.103126] Iteration 46300, train loss = 2.355824, train accuracy = 0.218750\n",
      "[2018-07-16 20:59:16.200168] Iteration 46400, train loss = 2.447753, train accuracy = 0.164062\n",
      "[2018-07-16 20:59:21.360501] Iteration 46500, train loss = 2.558742, train accuracy = 0.171875\n",
      "[2018-07-16 20:59:26.455809] Iteration 46600, train loss = 2.392917, train accuracy = 0.179688\n",
      "[2018-07-16 20:59:31.577286] Iteration 46700, train loss = 2.522213, train accuracy = 0.148438\n",
      "[2018-07-16 20:59:36.761962] Iteration 46800, train loss = 2.441167, train accuracy = 0.140625\n",
      "[2018-07-16 20:59:41.951433] Iteration 46900, train loss = 2.657424, train accuracy = 0.125000\n",
      "[2018-07-16 20:59:47.053744] Iteration 47000, train loss = 2.502878, train accuracy = 0.187500\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 20:59:54.115947] Iteration 47100, train loss = 2.568942, train accuracy = 0.117188\n",
      "[2018-07-16 20:59:59.294974] Iteration 47200, train loss = 2.486220, train accuracy = 0.226562\n",
      "[2018-07-16 21:00:04.470575] Iteration 47300, train loss = 2.486795, train accuracy = 0.140625\n",
      "[2018-07-16 21:00:09.609007] Iteration 47400, train loss = 2.554368, train accuracy = 0.140625\n",
      "[2018-07-16 21:00:14.790321] Iteration 47500, train loss = 2.455116, train accuracy = 0.179688\n",
      "[2018-07-16 21:00:19.983544] Iteration 47600, train loss = 2.643160, train accuracy = 0.093750\n",
      "[2018-07-16 21:00:25.160812] Iteration 47700, train loss = 2.386720, train accuracy = 0.187500\n",
      "[2018-07-16 21:00:30.331596] Iteration 47800, train loss = 2.589702, train accuracy = 0.156250\n",
      "[2018-07-16 21:00:35.486737] Iteration 47900, train loss = 2.565498, train accuracy = 0.164062\n",
      "[2018-07-16 21:00:40.685631] Iteration 48000, train loss = 2.397856, train accuracy = 0.195312\n",
      "Evaluating...\n",
      "Test accuracy = 0.164600\n",
      "Learning rate set to 0.001000\n",
      "[2018-07-16 21:00:47.700883] Iteration 48100, train loss = 2.553816, train accuracy = 0.125000\n",
      "[2018-07-16 21:00:52.756136] Iteration 48200, train loss = 2.533373, train accuracy = 0.164062\n",
      "[2018-07-16 21:00:57.938241] Iteration 48300, train loss = 2.564313, train accuracy = 0.171875\n",
      "[2018-07-16 21:01:03.061066] Iteration 48400, train loss = 2.479791, train accuracy = 0.187500\n",
      "[2018-07-16 21:01:08.205187] Iteration 48500, train loss = 2.461092, train accuracy = 0.171875\n",
      "[2018-07-16 21:01:13.316095] Iteration 48600, train loss = 2.575824, train accuracy = 0.171875\n",
      "[2018-07-16 21:01:18.315252] Iteration 48700, train loss = 2.436046, train accuracy = 0.210938\n",
      "[2018-07-16 21:01:23.294619] Iteration 48800, train loss = 2.474171, train accuracy = 0.164062\n",
      "[2018-07-16 21:01:28.397780] Iteration 48900, train loss = 2.462166, train accuracy = 0.140625\n",
      "[2018-07-16 21:01:33.603316] Iteration 49000, train loss = 2.560948, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.164200\n",
      "[2018-07-16 21:01:40.646813] Iteration 49100, train loss = 2.463913, train accuracy = 0.218750\n",
      "[2018-07-16 21:01:45.768066] Iteration 49200, train loss = 2.543164, train accuracy = 0.140625\n",
      "[2018-07-16 21:01:50.937911] Iteration 49300, train loss = 2.525803, train accuracy = 0.140625\n",
      "[2018-07-16 21:01:56.031020] Iteration 49400, train loss = 2.541316, train accuracy = 0.156250\n",
      "[2018-07-16 21:02:01.202236] Iteration 49500, train loss = 2.489831, train accuracy = 0.156250\n",
      "[2018-07-16 21:02:06.343628] Iteration 49600, train loss = 2.668721, train accuracy = 0.156250\n",
      "[2018-07-16 21:02:11.449955] Iteration 49700, train loss = 2.664345, train accuracy = 0.101562\n",
      "[2018-07-16 21:02:16.584189] Iteration 49800, train loss = 2.554327, train accuracy = 0.093750\n",
      "[2018-07-16 21:02:21.786515] Iteration 49900, train loss = 2.523064, train accuracy = 0.179688\n",
      "[2018-07-16 21:02:26.932861] Iteration 50000, train loss = 2.535830, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 21:02:33.977605] Iteration 50100, train loss = 2.324378, train accuracy = 0.210938\n",
      "[2018-07-16 21:02:39.146510] Iteration 50200, train loss = 2.513640, train accuracy = 0.156250\n",
      "[2018-07-16 21:02:44.155050] Iteration 50300, train loss = 2.357791, train accuracy = 0.179688\n",
      "[2018-07-16 21:02:49.316895] Iteration 50400, train loss = 2.590106, train accuracy = 0.125000\n",
      "[2018-07-16 21:02:54.474342] Iteration 50500, train loss = 2.539643, train accuracy = 0.117188\n",
      "[2018-07-16 21:02:59.585665] Iteration 50600, train loss = 2.494105, train accuracy = 0.203125\n",
      "[2018-07-16 21:03:04.759727] Iteration 50700, train loss = 2.612233, train accuracy = 0.140625\n",
      "[2018-07-16 21:03:09.975947] Iteration 50800, train loss = 2.485156, train accuracy = 0.164062\n",
      "[2018-07-16 21:03:15.125026] Iteration 50900, train loss = 2.534012, train accuracy = 0.140625\n",
      "[2018-07-16 21:03:20.286386] Iteration 51000, train loss = 2.436882, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165200\n",
      "[2018-07-16 21:03:27.339047] Iteration 51100, train loss = 2.390430, train accuracy = 0.210938\n",
      "[2018-07-16 21:03:32.442874] Iteration 51200, train loss = 2.510324, train accuracy = 0.164062\n",
      "[2018-07-16 21:03:37.602296] Iteration 51300, train loss = 2.625835, train accuracy = 0.148438\n",
      "[2018-07-16 21:03:42.781911] Iteration 51400, train loss = 2.647764, train accuracy = 0.156250\n",
      "[2018-07-16 21:03:47.962490] Iteration 51500, train loss = 2.467475, train accuracy = 0.156250\n",
      "[2018-07-16 21:03:53.105058] Iteration 51600, train loss = 2.593269, train accuracy = 0.101562\n",
      "[2018-07-16 21:03:58.185435] Iteration 51700, train loss = 2.454540, train accuracy = 0.132812\n",
      "[2018-07-16 21:04:03.339899] Iteration 51800, train loss = 2.576428, train accuracy = 0.187500\n",
      "[2018-07-16 21:04:08.450676] Iteration 51900, train loss = 2.641979, train accuracy = 0.125000\n",
      "[2018-07-16 21:04:13.495647] Iteration 52000, train loss = 2.462352, train accuracy = 0.210938\n",
      "Evaluating...\n",
      "Test accuracy = 0.164600\n",
      "[2018-07-16 21:04:20.448654] Iteration 52100, train loss = 2.610099, train accuracy = 0.125000\n",
      "[2018-07-16 21:04:25.537872] Iteration 52200, train loss = 2.317576, train accuracy = 0.234375\n",
      "[2018-07-16 21:04:30.657567] Iteration 52300, train loss = 2.465720, train accuracy = 0.164062\n",
      "[2018-07-16 21:04:35.612717] Iteration 52400, train loss = 2.396872, train accuracy = 0.218750\n",
      "[2018-07-16 21:04:40.609162] Iteration 52500, train loss = 2.443977, train accuracy = 0.195312\n",
      "[2018-07-16 21:04:45.684547] Iteration 52600, train loss = 2.558210, train accuracy = 0.203125\n",
      "[2018-07-16 21:04:50.822626] Iteration 52700, train loss = 2.580165, train accuracy = 0.187500\n",
      "[2018-07-16 21:04:55.961283] Iteration 52800, train loss = 2.579495, train accuracy = 0.195312\n",
      "[2018-07-16 21:05:01.089333] Iteration 52900, train loss = 2.432137, train accuracy = 0.203125\n",
      "[2018-07-16 21:05:06.211412] Iteration 53000, train loss = 2.488797, train accuracy = 0.187500\n",
      "Evaluating...\n",
      "Test accuracy = 0.165500\n",
      "[2018-07-16 21:05:13.086568] Iteration 53100, train loss = 2.578683, train accuracy = 0.140625\n",
      "[2018-07-16 21:05:18.105935] Iteration 53200, train loss = 2.529884, train accuracy = 0.156250\n",
      "[2018-07-16 21:05:23.140841] Iteration 53300, train loss = 2.567784, train accuracy = 0.132812\n",
      "[2018-07-16 21:05:28.102837] Iteration 53400, train loss = 2.537380, train accuracy = 0.171875\n",
      "[2018-07-16 21:05:33.099649] Iteration 53500, train loss = 2.498927, train accuracy = 0.203125\n",
      "[2018-07-16 21:05:38.074558] Iteration 53600, train loss = 2.531828, train accuracy = 0.164062\n",
      "[2018-07-16 21:05:43.015142] Iteration 53700, train loss = 2.539750, train accuracy = 0.164062\n",
      "[2018-07-16 21:05:47.962644] Iteration 53800, train loss = 2.606870, train accuracy = 0.125000\n",
      "[2018-07-16 21:05:52.906178] Iteration 53900, train loss = 2.539823, train accuracy = 0.125000\n",
      "[2018-07-16 21:05:57.850364] Iteration 54000, train loss = 2.530025, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.164200\n",
      "[2018-07-16 21:06:04.483175] Iteration 54100, train loss = 2.492788, train accuracy = 0.132812\n",
      "[2018-07-16 21:06:09.373063] Iteration 54200, train loss = 2.456992, train accuracy = 0.164062\n",
      "[2018-07-16 21:06:14.247184] Iteration 54300, train loss = 2.462776, train accuracy = 0.125000\n",
      "[2018-07-16 21:06:19.100763] Iteration 54400, train loss = 2.396679, train accuracy = 0.195312\n",
      "[2018-07-16 21:06:23.945864] Iteration 54500, train loss = 2.585031, train accuracy = 0.156250\n",
      "[2018-07-16 21:06:28.821092] Iteration 54600, train loss = 2.559502, train accuracy = 0.148438\n",
      "[2018-07-16 21:06:33.657477] Iteration 54700, train loss = 2.550549, train accuracy = 0.156250\n",
      "[2018-07-16 21:06:38.490717] Iteration 54800, train loss = 2.558811, train accuracy = 0.203125\n",
      "[2018-07-16 21:06:43.293701] Iteration 54900, train loss = 2.424476, train accuracy = 0.171875\n",
      "[2018-07-16 21:06:48.101400] Iteration 55000, train loss = 2.499225, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.164400\n",
      "[2018-07-16 21:06:54.568629] Iteration 55100, train loss = 2.560507, train accuracy = 0.132812\n",
      "[2018-07-16 21:06:59.352375] Iteration 55200, train loss = 2.524276, train accuracy = 0.164062\n",
      "[2018-07-16 21:07:04.136729] Iteration 55300, train loss = 2.475980, train accuracy = 0.156250\n",
      "[2018-07-16 21:07:08.919993] Iteration 55400, train loss = 2.493796, train accuracy = 0.156250\n",
      "[2018-07-16 21:07:13.698013] Iteration 55500, train loss = 2.526234, train accuracy = 0.171875\n",
      "[2018-07-16 21:07:18.470920] Iteration 55600, train loss = 2.334703, train accuracy = 0.242188\n",
      "[2018-07-16 21:07:23.245497] Iteration 55700, train loss = 2.474282, train accuracy = 0.132812\n",
      "[2018-07-16 21:07:27.999686] Iteration 55800, train loss = 2.619911, train accuracy = 0.125000\n",
      "[2018-07-16 21:07:32.742205] Iteration 55900, train loss = 2.496826, train accuracy = 0.148438\n",
      "[2018-07-16 21:07:37.491869] Iteration 56000, train loss = 2.625285, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 21:07:43.887202] Iteration 56100, train loss = 2.594514, train accuracy = 0.171875\n",
      "[2018-07-16 21:07:48.628570] Iteration 56200, train loss = 2.541001, train accuracy = 0.140625\n",
      "[2018-07-16 21:07:53.358125] Iteration 56300, train loss = 2.386546, train accuracy = 0.179688\n",
      "[2018-07-16 21:07:58.105824] Iteration 56400, train loss = 2.471342, train accuracy = 0.140625\n",
      "[2018-07-16 21:08:02.846547] Iteration 56500, train loss = 2.589807, train accuracy = 0.179688\n",
      "[2018-07-16 21:08:07.579306] Iteration 56600, train loss = 2.409552, train accuracy = 0.195312\n",
      "[2018-07-16 21:08:12.318131] Iteration 56700, train loss = 2.433425, train accuracy = 0.148438\n",
      "[2018-07-16 21:08:17.048609] Iteration 56800, train loss = 2.552647, train accuracy = 0.171875\n",
      "[2018-07-16 21:08:21.763904] Iteration 56900, train loss = 2.597624, train accuracy = 0.148438\n",
      "[2018-07-16 21:08:26.483928] Iteration 57000, train loss = 2.600362, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.164200\n",
      "[2018-07-16 21:08:32.843058] Iteration 57100, train loss = 2.500486, train accuracy = 0.148438\n",
      "[2018-07-16 21:08:37.559042] Iteration 57200, train loss = 2.389736, train accuracy = 0.218750\n",
      "[2018-07-16 21:08:42.265069] Iteration 57300, train loss = 2.635521, train accuracy = 0.117188\n",
      "[2018-07-16 21:08:46.987715] Iteration 57400, train loss = 2.541475, train accuracy = 0.109375\n",
      "[2018-07-16 21:08:51.740356] Iteration 57500, train loss = 2.449442, train accuracy = 0.187500\n",
      "[2018-07-16 21:08:56.471788] Iteration 57600, train loss = 2.571209, train accuracy = 0.164062\n",
      "[2018-07-16 21:09:01.186160] Iteration 57700, train loss = 2.477639, train accuracy = 0.156250\n",
      "[2018-07-16 21:09:05.912890] Iteration 57800, train loss = 2.588109, train accuracy = 0.156250\n",
      "[2018-07-16 21:09:10.630405] Iteration 57900, train loss = 2.558072, train accuracy = 0.093750\n",
      "[2018-07-16 21:09:15.359790] Iteration 58000, train loss = 2.640383, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.164800\n",
      "[2018-07-16 21:09:21.710131] Iteration 58100, train loss = 2.559577, train accuracy = 0.148438\n",
      "[2018-07-16 21:09:26.416297] Iteration 58200, train loss = 2.497209, train accuracy = 0.187500\n",
      "[2018-07-16 21:09:31.121788] Iteration 58300, train loss = 2.455484, train accuracy = 0.179688\n",
      "[2018-07-16 21:09:35.817196] Iteration 58400, train loss = 2.520508, train accuracy = 0.171875\n",
      "[2018-07-16 21:09:40.506581] Iteration 58500, train loss = 2.559326, train accuracy = 0.187500\n",
      "[2018-07-16 21:09:45.221648] Iteration 58600, train loss = 2.541834, train accuracy = 0.140625\n",
      "[2018-07-16 21:09:49.936649] Iteration 58700, train loss = 2.509526, train accuracy = 0.164062\n",
      "[2018-07-16 21:09:54.652634] Iteration 58800, train loss = 2.495553, train accuracy = 0.179688\n",
      "[2018-07-16 21:09:59.355441] Iteration 58900, train loss = 2.549131, train accuracy = 0.187500\n",
      "[2018-07-16 21:10:04.062799] Iteration 59000, train loss = 2.595420, train accuracy = 0.140625\n",
      "Evaluating...\n",
      "Test accuracy = 0.166000\n",
      "[2018-07-16 21:10:10.391703] Iteration 59100, train loss = 2.482237, train accuracy = 0.148438\n",
      "[2018-07-16 21:10:15.091807] Iteration 59200, train loss = 2.592589, train accuracy = 0.070312\n",
      "[2018-07-16 21:10:19.808650] Iteration 59300, train loss = 2.489892, train accuracy = 0.164062\n",
      "[2018-07-16 21:10:24.516964] Iteration 59400, train loss = 2.492690, train accuracy = 0.101562\n",
      "[2018-07-16 21:10:29.234741] Iteration 59500, train loss = 2.372177, train accuracy = 0.195312\n",
      "[2018-07-16 21:10:33.938434] Iteration 59600, train loss = 2.615898, train accuracy = 0.117188\n",
      "[2018-07-16 21:10:38.642598] Iteration 59700, train loss = 2.529852, train accuracy = 0.179688\n",
      "[2018-07-16 21:10:43.337036] Iteration 59800, train loss = 2.602457, train accuracy = 0.171875\n",
      "[2018-07-16 21:10:48.036663] Iteration 59900, train loss = 2.583610, train accuracy = 0.187500\n",
      "[2018-07-16 21:10:52.750452] Iteration 60000, train loss = 2.524975, train accuracy = 0.117188\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 21:10:59.070895] Iteration 60100, train loss = 2.497736, train accuracy = 0.179688\n",
      "[2018-07-16 21:11:03.789817] Iteration 60200, train loss = 2.767028, train accuracy = 0.117188\n",
      "[2018-07-16 21:11:08.489459] Iteration 60300, train loss = 2.528395, train accuracy = 0.132812\n",
      "[2018-07-16 21:11:13.182469] Iteration 60400, train loss = 2.549355, train accuracy = 0.125000\n",
      "[2018-07-16 21:11:17.899858] Iteration 60500, train loss = 2.435620, train accuracy = 0.164062\n",
      "[2018-07-16 21:11:22.572863] Iteration 60600, train loss = 2.587365, train accuracy = 0.140625\n",
      "[2018-07-16 21:11:27.283405] Iteration 60700, train loss = 2.433817, train accuracy = 0.171875\n",
      "[2018-07-16 21:11:32.003003] Iteration 60800, train loss = 2.510450, train accuracy = 0.132812\n",
      "[2018-07-16 21:11:36.722489] Iteration 60900, train loss = 2.475045, train accuracy = 0.148438\n",
      "[2018-07-16 21:11:41.426826] Iteration 61000, train loss = 2.471768, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.165300\n",
      "[2018-07-16 21:11:47.746996] Iteration 61100, train loss = 2.532435, train accuracy = 0.101562\n",
      "[2018-07-16 21:11:52.451129] Iteration 61200, train loss = 2.558501, train accuracy = 0.210938\n",
      "[2018-07-16 21:11:57.124982] Iteration 61300, train loss = 2.739495, train accuracy = 0.101562\n",
      "[2018-07-16 21:12:01.831545] Iteration 61400, train loss = 2.554523, train accuracy = 0.164062\n",
      "[2018-07-16 21:12:06.548924] Iteration 61500, train loss = 2.656097, train accuracy = 0.140625\n",
      "[2018-07-16 21:12:11.248948] Iteration 61600, train loss = 2.429300, train accuracy = 0.218750\n",
      "[2018-07-16 21:12:15.952836] Iteration 61700, train loss = 2.568235, train accuracy = 0.148438\n",
      "[2018-07-16 21:12:20.659157] Iteration 61800, train loss = 2.625284, train accuracy = 0.140625\n",
      "[2018-07-16 21:12:25.372746] Iteration 61900, train loss = 2.688074, train accuracy = 0.117188\n",
      "[2018-07-16 21:12:30.072766] Iteration 62000, train loss = 2.505089, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 21:12:36.394985] Iteration 62100, train loss = 2.557056, train accuracy = 0.148438\n",
      "[2018-07-16 21:12:41.087814] Iteration 62200, train loss = 2.619509, train accuracy = 0.179688\n",
      "[2018-07-16 21:12:45.793234] Iteration 62300, train loss = 2.456272, train accuracy = 0.171875\n",
      "[2018-07-16 21:12:50.480383] Iteration 62400, train loss = 2.606437, train accuracy = 0.117188\n",
      "[2018-07-16 21:12:55.171719] Iteration 62500, train loss = 2.522532, train accuracy = 0.140625\n",
      "[2018-07-16 21:12:59.868581] Iteration 62600, train loss = 2.598012, train accuracy = 0.195312\n",
      "[2018-07-16 21:13:04.561235] Iteration 62700, train loss = 2.497039, train accuracy = 0.156250\n",
      "[2018-07-16 21:13:09.235263] Iteration 62800, train loss = 2.577940, train accuracy = 0.156250\n",
      "[2018-07-16 21:13:13.937995] Iteration 62900, train loss = 2.564521, train accuracy = 0.085938\n",
      "[2018-07-16 21:13:18.623448] Iteration 63000, train loss = 2.515894, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165900\n",
      "[2018-07-16 21:13:24.937951] Iteration 63100, train loss = 2.421167, train accuracy = 0.148438\n",
      "[2018-07-16 21:13:29.638046] Iteration 63200, train loss = 2.468995, train accuracy = 0.257812\n",
      "[2018-07-16 21:13:34.338168] Iteration 63300, train loss = 2.499105, train accuracy = 0.164062\n",
      "[2018-07-16 21:13:39.042578] Iteration 63400, train loss = 2.465648, train accuracy = 0.203125\n",
      "[2018-07-16 21:13:43.744801] Iteration 63500, train loss = 2.522625, train accuracy = 0.156250\n",
      "[2018-07-16 21:13:48.430472] Iteration 63600, train loss = 2.578827, train accuracy = 0.148438\n",
      "[2018-07-16 21:13:53.122410] Iteration 63700, train loss = 2.416204, train accuracy = 0.234375\n",
      "[2018-07-16 21:13:57.807435] Iteration 63800, train loss = 2.442257, train accuracy = 0.132812\n",
      "[2018-07-16 21:14:02.513521] Iteration 63900, train loss = 2.545386, train accuracy = 0.164062\n",
      "[2018-07-16 21:14:07.182684] Iteration 64000, train loss = 2.449085, train accuracy = 0.148438\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 21:14:13.477065] Iteration 64100, train loss = 2.515787, train accuracy = 0.164062\n",
      "[2018-07-16 21:14:18.167150] Iteration 64200, train loss = 2.514333, train accuracy = 0.148438\n",
      "[2018-07-16 21:14:22.879738] Iteration 64300, train loss = 2.486565, train accuracy = 0.164062\n",
      "[2018-07-16 21:14:27.572234] Iteration 64400, train loss = 2.618169, train accuracy = 0.109375\n",
      "[2018-07-16 21:14:32.253802] Iteration 64500, train loss = 2.574525, train accuracy = 0.187500\n",
      "[2018-07-16 21:14:36.950613] Iteration 64600, train loss = 2.327720, train accuracy = 0.218750\n",
      "[2018-07-16 21:14:41.650240] Iteration 64700, train loss = 2.454286, train accuracy = 0.171875\n",
      "[2018-07-16 21:14:46.339109] Iteration 64800, train loss = 2.463604, train accuracy = 0.148438\n",
      "[2018-07-16 21:14:51.053790] Iteration 64900, train loss = 2.537963, train accuracy = 0.140625\n",
      "[2018-07-16 21:14:55.743811] Iteration 65000, train loss = 2.516132, train accuracy = 0.218750\n",
      "Evaluating...\n",
      "Test accuracy = 0.166600\n",
      "[2018-07-16 21:15:02.070492] Iteration 65100, train loss = 2.448900, train accuracy = 0.117188\n",
      "[2018-07-16 21:15:06.771332] Iteration 65200, train loss = 2.568002, train accuracy = 0.140625\n",
      "[2018-07-16 21:15:11.440621] Iteration 65300, train loss = 2.526212, train accuracy = 0.179688\n",
      "[2018-07-16 21:15:16.145634] Iteration 65400, train loss = 2.668463, train accuracy = 0.148438\n",
      "[2018-07-16 21:15:20.840206] Iteration 65500, train loss = 2.531577, train accuracy = 0.132812\n",
      "[2018-07-16 21:15:25.542380] Iteration 65600, train loss = 2.558330, train accuracy = 0.203125\n",
      "[2018-07-16 21:15:30.235990] Iteration 65700, train loss = 2.434657, train accuracy = 0.156250\n",
      "[2018-07-16 21:15:34.934542] Iteration 65800, train loss = 2.628113, train accuracy = 0.109375\n",
      "[2018-07-16 21:15:39.623549] Iteration 65900, train loss = 2.508344, train accuracy = 0.195312\n",
      "[2018-07-16 21:15:44.329039] Iteration 66000, train loss = 2.409522, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 21:15:50.646068] Iteration 66100, train loss = 2.589577, train accuracy = 0.125000\n",
      "[2018-07-16 21:15:55.340692] Iteration 66200, train loss = 2.628911, train accuracy = 0.140625\n",
      "[2018-07-16 21:16:00.033402] Iteration 66300, train loss = 2.461322, train accuracy = 0.156250\n",
      "[2018-07-16 21:16:04.736031] Iteration 66400, train loss = 2.540962, train accuracy = 0.218750\n",
      "[2018-07-16 21:16:09.420399] Iteration 66500, train loss = 2.553393, train accuracy = 0.132812\n",
      "[2018-07-16 21:16:14.111648] Iteration 66600, train loss = 2.393749, train accuracy = 0.156250\n",
      "[2018-07-16 21:16:18.784328] Iteration 66700, train loss = 2.555961, train accuracy = 0.148438\n",
      "[2018-07-16 21:16:23.476206] Iteration 66800, train loss = 2.610387, train accuracy = 0.148438\n",
      "[2018-07-16 21:16:28.177217] Iteration 66900, train loss = 2.479115, train accuracy = 0.195312\n",
      "[2018-07-16 21:16:32.883102] Iteration 67000, train loss = 2.499291, train accuracy = 0.210938\n",
      "Evaluating...\n",
      "Test accuracy = 0.164000\n",
      "[2018-07-16 21:16:39.196451] Iteration 67100, train loss = 2.633378, train accuracy = 0.179688\n",
      "[2018-07-16 21:16:43.887054] Iteration 67200, train loss = 2.530584, train accuracy = 0.226562\n",
      "[2018-07-16 21:16:48.559274] Iteration 67300, train loss = 2.400530, train accuracy = 0.140625\n",
      "[2018-07-16 21:16:53.261307] Iteration 67400, train loss = 2.570217, train accuracy = 0.164062\n",
      "[2018-07-16 21:16:57.957586] Iteration 67500, train loss = 2.563553, train accuracy = 0.117188\n",
      "[2018-07-16 21:17:02.663571] Iteration 67600, train loss = 2.438892, train accuracy = 0.164062\n",
      "[2018-07-16 21:17:07.353832] Iteration 67700, train loss = 2.505983, train accuracy = 0.171875\n",
      "[2018-07-16 21:17:12.045780] Iteration 67800, train loss = 2.396351, train accuracy = 0.195312\n",
      "[2018-07-16 21:17:16.746893] Iteration 67900, train loss = 2.433203, train accuracy = 0.148438\n",
      "[2018-07-16 21:17:21.453378] Iteration 68000, train loss = 2.555277, train accuracy = 0.203125\n",
      "Evaluating...\n",
      "Test accuracy = 0.164500\n",
      "[2018-07-16 21:17:27.746614] Iteration 68100, train loss = 2.467368, train accuracy = 0.187500\n",
      "[2018-07-16 21:17:32.455878] Iteration 68200, train loss = 2.481049, train accuracy = 0.171875\n",
      "[2018-07-16 21:17:37.145342] Iteration 68300, train loss = 2.621972, train accuracy = 0.140625\n",
      "[2018-07-16 21:17:41.848226] Iteration 68400, train loss = 2.499587, train accuracy = 0.187500\n",
      "[2018-07-16 21:17:46.545302] Iteration 68500, train loss = 2.445721, train accuracy = 0.132812\n",
      "[2018-07-16 21:17:51.267912] Iteration 68600, train loss = 2.511963, train accuracy = 0.164062\n",
      "[2018-07-16 21:17:55.958590] Iteration 68700, train loss = 2.730760, train accuracy = 0.164062\n",
      "[2018-07-16 21:18:00.651979] Iteration 68800, train loss = 2.466249, train accuracy = 0.187500\n",
      "[2018-07-16 21:18:05.357635] Iteration 68900, train loss = 2.484406, train accuracy = 0.117188\n",
      "[2018-07-16 21:18:10.044273] Iteration 69000, train loss = 2.612767, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.164400\n",
      "[2018-07-16 21:18:16.336789] Iteration 69100, train loss = 2.542243, train accuracy = 0.179688\n",
      "[2018-07-16 21:18:21.027451] Iteration 69200, train loss = 2.603009, train accuracy = 0.117188\n",
      "[2018-07-16 21:18:25.712934] Iteration 69300, train loss = 2.700279, train accuracy = 0.132812\n",
      "[2018-07-16 21:18:30.399052] Iteration 69400, train loss = 2.531872, train accuracy = 0.140625\n",
      "[2018-07-16 21:18:35.087627] Iteration 69500, train loss = 2.563050, train accuracy = 0.132812\n",
      "[2018-07-16 21:18:39.782997] Iteration 69600, train loss = 2.687522, train accuracy = 0.164062\n",
      "[2018-07-16 21:18:44.456485] Iteration 69700, train loss = 2.371214, train accuracy = 0.187500\n",
      "[2018-07-16 21:18:49.154874] Iteration 69800, train loss = 2.524802, train accuracy = 0.179688\n",
      "[2018-07-16 21:18:53.846716] Iteration 69900, train loss = 2.555570, train accuracy = 0.148438\n",
      "[2018-07-16 21:18:58.524276] Iteration 70000, train loss = 2.537392, train accuracy = 0.226562\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 21:19:04.821957] Iteration 70100, train loss = 2.574469, train accuracy = 0.156250\n",
      "[2018-07-16 21:19:09.514363] Iteration 70200, train loss = 2.649201, train accuracy = 0.156250\n",
      "[2018-07-16 21:19:14.215548] Iteration 70300, train loss = 2.524974, train accuracy = 0.164062\n",
      "[2018-07-16 21:19:18.907432] Iteration 70400, train loss = 2.499820, train accuracy = 0.125000\n",
      "[2018-07-16 21:19:23.582124] Iteration 70500, train loss = 2.383398, train accuracy = 0.156250\n",
      "[2018-07-16 21:19:28.268697] Iteration 70600, train loss = 2.661412, train accuracy = 0.164062\n",
      "[2018-07-16 21:19:32.967395] Iteration 70700, train loss = 2.567528, train accuracy = 0.117188\n",
      "[2018-07-16 21:19:37.643294] Iteration 70800, train loss = 2.521032, train accuracy = 0.140625\n",
      "[2018-07-16 21:19:42.334185] Iteration 70900, train loss = 2.488451, train accuracy = 0.164062\n",
      "[2018-07-16 21:19:47.044261] Iteration 71000, train loss = 2.681746, train accuracy = 0.093750\n",
      "Evaluating...\n",
      "Test accuracy = 0.166100\n",
      "[2018-07-16 21:19:53.344024] Iteration 71100, train loss = 2.536539, train accuracy = 0.140625\n",
      "[2018-07-16 21:19:58.037334] Iteration 71200, train loss = 2.581859, train accuracy = 0.117188\n",
      "[2018-07-16 21:20:02.718322] Iteration 71300, train loss = 2.461265, train accuracy = 0.203125\n",
      "[2018-07-16 21:20:07.414435] Iteration 71400, train loss = 2.502875, train accuracy = 0.101562\n",
      "[2018-07-16 21:20:12.085869] Iteration 71500, train loss = 2.594813, train accuracy = 0.117188\n",
      "[2018-07-16 21:20:16.777512] Iteration 71600, train loss = 2.475750, train accuracy = 0.195312\n",
      "[2018-07-16 21:20:21.470868] Iteration 71700, train loss = 2.479617, train accuracy = 0.179688\n",
      "[2018-07-16 21:20:26.149907] Iteration 71800, train loss = 2.531980, train accuracy = 0.164062\n",
      "[2018-07-16 21:20:30.825906] Iteration 71900, train loss = 2.420112, train accuracy = 0.156250\n",
      "[2018-07-16 21:20:35.517525] Iteration 72000, train loss = 2.731952, train accuracy = 0.132812\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 21:20:41.817164] Iteration 72100, train loss = 2.535199, train accuracy = 0.203125\n",
      "[2018-07-16 21:20:46.510152] Iteration 72200, train loss = 2.581324, train accuracy = 0.140625\n",
      "[2018-07-16 21:20:51.196829] Iteration 72300, train loss = 2.572898, train accuracy = 0.187500\n",
      "[2018-07-16 21:20:55.893395] Iteration 72400, train loss = 2.622498, train accuracy = 0.093750\n",
      "[2018-07-16 21:21:00.580071] Iteration 72500, train loss = 2.491888, train accuracy = 0.140625\n",
      "[2018-07-16 21:21:05.288891] Iteration 72600, train loss = 2.501674, train accuracy = 0.132812\n",
      "[2018-07-16 21:21:09.972812] Iteration 72700, train loss = 2.552809, train accuracy = 0.164062\n",
      "[2018-07-16 21:21:14.650373] Iteration 72800, train loss = 2.474494, train accuracy = 0.164062\n",
      "[2018-07-16 21:21:19.328464] Iteration 72900, train loss = 2.438583, train accuracy = 0.125000\n",
      "[2018-07-16 21:21:24.017044] Iteration 73000, train loss = 2.444466, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.164900\n",
      "[2018-07-16 21:21:30.331944] Iteration 73100, train loss = 2.607056, train accuracy = 0.117188\n",
      "[2018-07-16 21:21:35.013184] Iteration 73200, train loss = 2.426128, train accuracy = 0.140625\n",
      "[2018-07-16 21:21:39.688775] Iteration 73300, train loss = 2.499945, train accuracy = 0.179688\n",
      "[2018-07-16 21:21:44.372068] Iteration 73400, train loss = 2.442122, train accuracy = 0.125000\n",
      "[2018-07-16 21:21:49.067877] Iteration 73500, train loss = 2.492294, train accuracy = 0.125000\n",
      "[2018-07-16 21:21:53.750495] Iteration 73600, train loss = 2.484040, train accuracy = 0.164062\n",
      "[2018-07-16 21:21:58.434700] Iteration 73700, train loss = 2.413397, train accuracy = 0.140625\n",
      "[2018-07-16 21:22:03.122857] Iteration 73800, train loss = 2.613763, train accuracy = 0.218750\n",
      "[2018-07-16 21:22:07.805785] Iteration 73900, train loss = 2.537393, train accuracy = 0.218750\n",
      "[2018-07-16 21:22:12.490135] Iteration 74000, train loss = 2.493652, train accuracy = 0.195312\n",
      "Evaluating...\n",
      "Test accuracy = 0.165900\n",
      "[2018-07-16 21:22:18.783892] Iteration 74100, train loss = 2.584605, train accuracy = 0.132812\n",
      "[2018-07-16 21:22:23.473541] Iteration 74200, train loss = 2.484684, train accuracy = 0.218750\n",
      "[2018-07-16 21:22:28.165625] Iteration 74300, train loss = 2.464694, train accuracy = 0.156250\n",
      "[2018-07-16 21:22:32.847344] Iteration 74400, train loss = 2.539507, train accuracy = 0.187500\n",
      "[2018-07-16 21:22:37.545856] Iteration 74500, train loss = 2.438936, train accuracy = 0.203125\n",
      "[2018-07-16 21:22:42.237832] Iteration 74600, train loss = 2.487971, train accuracy = 0.179688\n",
      "[2018-07-16 21:22:46.914800] Iteration 74700, train loss = 2.562271, train accuracy = 0.187500\n",
      "[2018-07-16 21:22:51.615463] Iteration 74800, train loss = 2.413550, train accuracy = 0.203125\n",
      "[2018-07-16 21:22:56.297558] Iteration 74900, train loss = 2.354042, train accuracy = 0.195312\n",
      "[2018-07-16 21:23:00.974233] Iteration 75000, train loss = 2.571157, train accuracy = 0.156250\n",
      "Evaluating...\n",
      "Test accuracy = 0.164500\n",
      "[2018-07-16 21:23:07.283550] Iteration 75100, train loss = 2.581274, train accuracy = 0.101562\n",
      "[2018-07-16 21:23:11.970955] Iteration 75200, train loss = 2.738034, train accuracy = 0.117188\n",
      "[2018-07-16 21:23:16.659086] Iteration 75300, train loss = 2.513801, train accuracy = 0.132812\n",
      "[2018-07-16 21:23:21.341640] Iteration 75400, train loss = 2.574782, train accuracy = 0.156250\n",
      "[2018-07-16 21:23:26.066096] Iteration 75500, train loss = 2.518006, train accuracy = 0.171875\n",
      "[2018-07-16 21:23:30.769119] Iteration 75600, train loss = 2.501657, train accuracy = 0.187500\n",
      "[2018-07-16 21:23:35.439706] Iteration 75700, train loss = 2.604698, train accuracy = 0.148438\n",
      "[2018-07-16 21:23:40.137834] Iteration 75800, train loss = 2.561702, train accuracy = 0.148438\n",
      "[2018-07-16 21:23:44.815230] Iteration 75900, train loss = 2.568627, train accuracy = 0.132812\n",
      "[2018-07-16 21:23:49.502072] Iteration 76000, train loss = 2.429811, train accuracy = 0.179688\n",
      "Evaluating...\n",
      "Test accuracy = 0.164200\n",
      "[2018-07-16 21:23:55.806195] Iteration 76100, train loss = 2.494759, train accuracy = 0.210938\n",
      "[2018-07-16 21:24:00.497167] Iteration 76200, train loss = 2.550135, train accuracy = 0.164062\n",
      "[2018-07-16 21:24:05.169739] Iteration 76300, train loss = 2.437787, train accuracy = 0.210938\n",
      "[2018-07-16 21:24:09.834038] Iteration 76400, train loss = 2.671335, train accuracy = 0.132812\n",
      "[2018-07-16 21:24:14.534036] Iteration 76500, train loss = 2.541612, train accuracy = 0.132812\n",
      "[2018-07-16 21:24:19.229209] Iteration 76600, train loss = 2.520096, train accuracy = 0.148438\n",
      "[2018-07-16 21:24:23.927807] Iteration 76700, train loss = 2.436712, train accuracy = 0.164062\n",
      "[2018-07-16 21:24:28.610709] Iteration 76800, train loss = 2.508291, train accuracy = 0.164062\n",
      "[2018-07-16 21:24:33.299531] Iteration 76900, train loss = 2.489932, train accuracy = 0.132812\n",
      "[2018-07-16 21:24:37.993886] Iteration 77000, train loss = 2.565651, train accuracy = 0.164062\n",
      "Evaluating...\n",
      "Test accuracy = 0.165000\n",
      "[2018-07-16 21:24:44.302502] Iteration 77100, train loss = 2.558554, train accuracy = 0.164062\n",
      "[2018-07-16 21:24:48.990888] Iteration 77200, train loss = 2.464309, train accuracy = 0.187500\n",
      "[2018-07-16 21:24:53.666506] Iteration 77300, train loss = 2.644698, train accuracy = 0.195312\n",
      "[2018-07-16 21:24:58.353734] Iteration 77400, train loss = 2.719120, train accuracy = 0.117188\n",
      "[2018-07-16 21:25:03.043364] Iteration 77500, train loss = 2.579762, train accuracy = 0.156250\n",
      "[2018-07-16 21:25:07.739615] Iteration 77600, train loss = 2.596902, train accuracy = 0.132812\n",
      "[2018-07-16 21:25:12.436191] Iteration 77700, train loss = 2.577360, train accuracy = 0.148438\n",
      "[2018-07-16 21:25:17.111095] Iteration 77800, train loss = 2.553526, train accuracy = 0.179688\n",
      "[2018-07-16 21:25:21.795465] Iteration 77900, train loss = 2.419002, train accuracy = 0.187500\n",
      "[2018-07-16 21:25:26.501008] Iteration 78000, train loss = 2.681688, train accuracy = 0.125000\n",
      "Evaluating...\n",
      "Test accuracy = 0.166000\n",
      "[2018-07-16 21:25:32.791882] Iteration 78100, train loss = 2.509948, train accuracy = 0.203125\n",
      "[2018-07-16 21:25:37.481186] Iteration 78200, train loss = 2.418088, train accuracy = 0.093750\n",
      "[2018-07-16 21:25:42.159329] Iteration 78300, train loss = 2.537330, train accuracy = 0.140625\n",
      "[2018-07-16 21:25:46.850377] Iteration 78400, train loss = 2.496245, train accuracy = 0.210938\n",
      "[2018-07-16 21:25:51.539406] Iteration 78500, train loss = 2.771848, train accuracy = 0.140625\n",
      "[2018-07-16 21:25:56.228689] Iteration 78600, train loss = 2.506020, train accuracy = 0.125000\n",
      "[2018-07-16 21:26:00.924730] Iteration 78700, train loss = 2.487378, train accuracy = 0.164062\n",
      "[2018-07-16 21:26:05.616760] Iteration 78800, train loss = 2.473157, train accuracy = 0.187500\n",
      "[2018-07-16 21:26:10.316063] Iteration 78900, train loss = 2.365805, train accuracy = 0.179688\n",
      "[2018-07-16 21:26:15.004659] Iteration 79000, train loss = 2.449932, train accuracy = 0.171875\n",
      "Evaluating...\n",
      "Test accuracy = 0.165100\n",
      "[2018-07-16 21:26:21.300048] Iteration 79100, train loss = 2.475858, train accuracy = 0.203125\n",
      "[2018-07-16 21:26:25.988961] Iteration 79200, train loss = 2.614331, train accuracy = 0.148438\n",
      "[2018-07-16 21:26:30.662742] Iteration 79300, train loss = 2.453090, train accuracy = 0.195312\n",
      "[2018-07-16 21:26:35.351643] Iteration 79400, train loss = 2.404178, train accuracy = 0.140625\n",
      "[2018-07-16 21:26:40.015157] Iteration 79500, train loss = 2.547760, train accuracy = 0.179688\n",
      "[2018-07-16 21:26:44.714663] Iteration 79600, train loss = 2.499726, train accuracy = 0.148438\n",
      "[2018-07-16 21:26:49.390402] Iteration 79700, train loss = 2.584667, train accuracy = 0.132812\n",
      "[2018-07-16 21:26:54.086317] Iteration 79800, train loss = 2.402687, train accuracy = 0.195312\n",
      "[2018-07-16 21:26:58.770662] Iteration 79900, train loss = 2.556030, train accuracy = 0.164062\n"
     ]
    }
   ],
   "source": [
    "for step in range(FLAGS.max_steps):\n",
    "  if step <= 48000:\n",
    "    _lr = 1e-2\n",
    "  else:\n",
    "    _lr = 1e-3\n",
    "  if curr_lr != _lr:\n",
    "    curr_lr = _lr\n",
    "    print('Learning rate set to %f' % curr_lr)\n",
    "\n",
    "  # train\n",
    "  fetches = [train_step, loss]\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    fetches += [accuracy]\n",
    "  sess_outputs = sess.run(fetches, {phase_train.name: True, learning_rate.name: curr_lr})\n",
    "\n",
    "\n",
    "  if step > 0 and step % FLAGS.summary_interval == 0:\n",
    "    train_loss_value, train_acc_value= sess_outputs[1:]\n",
    "    print('[%s] Iteration %d, train loss = %f, train accuracy = %f' %\n",
    "        (datetime.now(), step, train_loss_value, train_acc_value))\n",
    "\n",
    "  # validation\n",
    "  if step > 0 and step % FLAGS.val_interval == 0:\n",
    "    print('Evaluating...')\n",
    "    n_val_samples = 10000\n",
    "    val_batch_size = FLAGS.val_batch_size\n",
    "    n_val_batch = int(n_val_samples / val_batch_size)\n",
    "    val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "    val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "    val_losses = []\n",
    "    for i in range(n_val_batch):\n",
    "      fetches = [logits, label_batch, loss]\n",
    "      session_outputs = sess.run(\n",
    "        fetches, {phase_train.name: False})\n",
    "      val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "      val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "      val_losses.append(session_outputs[2])\n",
    "    pred_labels = np.argmax(val_logits, axis=1)\n",
    "    val_accuracy = np.count_nonzero(\n",
    "      pred_labels == val_labels) / n_val_samples\n",
    "    val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "    print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune后的准确率\n",
      "Test accuracy = 0.165200\n"
     ]
    }
   ],
   "source": [
    "print('prune后的准确率')\n",
    "n_val_samples = 10000\n",
    "val_batch_size = FLAGS.val_batch_size\n",
    "n_val_batch = int(n_val_samples / val_batch_size)\n",
    "val_logits = np.zeros((n_val_samples, 10), dtype=np.float32)\n",
    "val_labels = np.zeros((n_val_samples), dtype=np.int64)\n",
    "val_losses = []\n",
    "for i in range(n_val_batch):\n",
    "    fetches = [logits, label_batch, loss]\n",
    "    session_outputs = sess.run(fetches, {phase_train.name: False})\n",
    "    val_logits[i*val_batch_size:(i+1)*val_batch_size, :] = session_outputs[0]\n",
    "    val_labels[i*val_batch_size:(i+1)*val_batch_size] = session_outputs[1]\n",
    "val_losses.append(session_outputs[2])\n",
    "pred_labels = np.argmax(val_logits, axis=1)\n",
    "val_accuracy = np.count_nonzero(pred_labels == val_labels) / n_val_samples\n",
    "val_loss = float(np.mean(np.asarray(val_losses)))\n",
    "print('Test accuracy = %f' % val_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0\n",
      "[ 0.0625 -0.0625  0.0625  0.125   0.      0.      0.0625 -0.125  -0.125\n",
      " -0.125 ]\n"
     ]
    }
   ],
   "source": [
    "print('prune_rate =1.00时，可视化部分参数：res_net/conv_last/bias:0')\n",
    "print(sess.run(tf.get_collection('last_biases')[0]))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
